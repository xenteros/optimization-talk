{"traceEvents": [{"ph": "M", "pid": 29736, "tid": 29736, "name": "process_name", "args": {"name": "MainProcess"}}, {"ph": "M", "pid": 29736, "tid": 14104, "name": "thread_name", "args": {"name": "MainThread"}}, {"pid": 29736, "tid": 14104, "ts": 2292897569458.0, "dur": 0.4, "name": "type.__new__", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897569456.5, "dur": 2.1, "name": "__new__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:149)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897569460.8, "dur": 0.1, "name": "is_scripting (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\_jit_internal.py:1120)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897569459.5, "dur": 3.2, "name": "__init__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\autograd\\grad_mode.py:74)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897569464.5, "dur": 1.7, "name": "torch.is_grad_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897569467.8, "dur": 0.1, "name": "torch.is_grad_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897569469.1, "dur": 3.0, "name": "torch._C._set_grad_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897569467.6, "dur": 4.9, "name": "__init__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\autograd\\grad_mode.py:183)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897569463.7, "dur": 9.2, "name": "__enter__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\autograd\\grad_mode.py:79)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897569478.5, "dur": 8.0, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897569495.2, "dur": 2.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897569501.1, "dur": 1.5, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897569506.8, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897569508.6, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897569512.1, "dur": 181313.1, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897569509.6, "dur": 181316.9, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897569504.6, "dur": 181323.9, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897569500.8, "dur": 181329.4, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897569498.5, "dur": 181332.8, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897750844.2, "dur": 3.8, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897750856.0, "dur": 8.0, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897750877.1, "dur": 1.7, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897750874.9, "dur": 4.8, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897750883.6, "dur": 1.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897750887.1, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897750888.7, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897750890.7, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897750892.3, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897750896.9, "dur": 0.3, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897750910.0, "dur": 1.2, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897750908.6, "dur": 2.7, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897750911.7, "dur": 1686.8, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897750895.9, "dur": 1703.2, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897750872.0, "dur": 1728.1, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897750853.8, "dur": 1747.2, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897750850.3, "dur": 1751.6, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897752610.6, "dur": 2.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897752616.7, "dur": 3.7, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897752628.3, "dur": 0.8, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897752630.4, "dur": 1424.0, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897752627.8, "dur": 1428.2, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897752623.8, "dur": 1434.0, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897752615.8, "dur": 1443.6, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897752614.3, "dur": 1447.4, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897754099.5, "dur": 4.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897754111.2, "dur": 8.1, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897754144.3, "dur": 0.9, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897754146.7, "dur": 2497.9, "name": "torch.max_pool2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897754143.6, "dur": 2501.5, "name": "_max_pool2d (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:774)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897754139.3, "dur": 2507.8, "name": "fn (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\_jit_internal.py:489)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897754130.4, "dur": 2517.6, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:163)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897754109.1, "dur": 2539.6, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897754106.5, "dur": 2542.8, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897756661.6, "dur": 2.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897756669.7, "dur": 5.5, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897756684.0, "dur": 0.5, "name": "collections.OrderedDict.values", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897756685.1, "dur": 0.6, "name": "builtins.iter", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897756682.4, "dur": 3.6, "name": "__iter__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:207)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897756689.4, "dur": 1.3, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897756700.6, "dur": 1.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897756704.4, "dur": 1.2, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897756710.0, "dur": 0.8, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897756711.9, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897756716.1, "dur": 17817.6, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897756713.0, "dur": 17821.7, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897756707.6, "dur": 17829.2, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897756704.1, "dur": 17834.3, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897756702.9, "dur": 17836.8, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774553.4, "dur": 3.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774563.1, "dur": 7.4, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774576.7, "dur": 1.9, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774575.1, "dur": 4.3, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774582.0, "dur": 1.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774585.3, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774587.0, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774588.7, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774590.1, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774592.3, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774598.1, "dur": 0.9, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774597.3, "dur": 1.72, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774599.5, "dur": 121.3, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774591.7, "dur": 129.3, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774573.4, "dur": 148.0, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774560.7, "dur": 161.2, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774558.5, "dur": 164.0, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774728.0, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774731.8, "dur": 2.5, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774738.0, "dur": 0.3, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774739.2, "dur": 64.7, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774737.6, "dur": 66.5, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774735.9, "dur": 68.5, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774731.0, "dur": 73.7, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774730.1, "dur": 74.9, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774817.0, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774820.2, "dur": 2.1, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774826.0, "dur": 0.9, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774827.9, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774830.7, "dur": 161499.4, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774828.8, "dur": 161502.7, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774824.0, "dur": 161509.4, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774819.9, "dur": 161515.4, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897774818.7, "dur": 161517.8, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897936353.9, "dur": 4.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897936363.3, "dur": 7.2, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897936377.1, "dur": 1.9, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897936375.6, "dur": 4.3, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897936387.2, "dur": 1.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897936391.1, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897936393.1, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897936395.1, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897936396.7, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897936398.9, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897936405.0, "dur": 0.8, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897936404.4, "dur": 1.5, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897936406.3, "dur": 331.9, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897936398.3, "dur": 340.6, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897936373.9, "dur": 366.0, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897936361.2, "dur": 380.1, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897936359.4, "dur": 383.2, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897936844.1, "dur": 46.8, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897936897.5, "dur": 6.9, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897936910.1, "dur": 0.5, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897936911.6, "dur": 171.3, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897936909.6, "dur": 173.7, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897936907.3, "dur": 176.4, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897936895.8, "dur": 188.3, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897936893.8, "dur": 190.8, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897937090.4, "dur": 1.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897937094.8, "dur": 2.7, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897937102.4, "dur": 0.8, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897937104.5, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897937107.7, "dur": 69144.8, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897937105.6, "dur": 69148.3, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897937100.2, "dur": 69155.2, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897937094.1, "dur": 69163.4, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897937092.8, "dur": 69165.9, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006277.6, "dur": 4.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006287.0, "dur": 7.6, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006301.3, "dur": 1.5, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006299.6, "dur": 4.1, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006311.8, "dur": 1.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006315.2, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006316.9, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006318.6, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006320.0, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006322.2, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006328.7, "dur": 0.8, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006327.8, "dur": 1.8, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006330.1, "dur": 320.1, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006321.6, "dur": 328.9, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006298.0, "dur": 352.8, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006285.1, "dur": 366.2, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006283.1, "dur": 368.7, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006656.6, "dur": 1.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006659.2, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006662.7, "dur": 2.3, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006669.1, "dur": 0.8, "name": "collections.OrderedDict.values", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006670.1, "dur": 0.8, "name": "builtins.iter", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006667.9, "dur": 3.2, "name": "__iter__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:207)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006674.4, "dur": 1.2, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006679.4, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006681.2, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006684.4, "dur": 69043.9, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006682.1, "dur": 69048.0, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006677.2, "dur": 69055.0, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006674.1, "dur": 69059.8, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006673.0, "dur": 69062.1, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898075746.9, "dur": 8.3, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898075763.0, "dur": 1.4, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898075760.8, "dur": 4.4, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898075772.7, "dur": 2.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898075777.8, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898075779.5, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898075781.3, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898075782.7, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898075784.8, "dur": 0.3, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898075790.6, "dur": 0.9, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898075789.7, "dur": 1.82, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898075792.0, "dur": 510.0, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898075784.2, "dur": 518.3, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898075758.7, "dur": 544.2, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898075744.2, "dur": 559.4, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898075741.3, "dur": 563.0, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006667.0, "dur": 69640.6, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006662.1, "dur": 69646.4, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898006660.7, "dur": 69648.6, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898077151.8, "dur": 2.9, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898077160.4, "dur": 5.7, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898077170.8, "dur": 0.8, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898077172.3, "dur": 196.2, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898077170.4, "dur": 198.4, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898077168.5, "dur": 200.6, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898077158.6, "dur": 211.0, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898077156.7, "dur": 213.4, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897756697.6, "dur": 320672.9, "name": "forward (C:\\Users\\adam/.cache\\torch\\hub\\pytorch_vision_v0.10.0\\torchvision\\models\\resnet.py:121)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897756689.1, "dur": 320686.9, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897756687.7, "dur": 320689.1, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898077381.1, "dur": 2.3, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898077387.1, "dur": 1.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898077390.5, "dur": 1.3, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898077395.1, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898077396.9, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898077400.2, "dur": 67844.3, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898077397.9, "dur": 67848.1, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898077393.3, "dur": 67855.0, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898077390.3, "dur": 67859.7, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898077389.1, "dur": 67862.3, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145264.3, "dur": 4.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145273.9, "dur": 10.5, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145290.9, "dur": 1.4, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145289.4, "dur": 3.8, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145295.9, "dur": 1.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145299.4, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145301.3, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145303.1, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145304.7, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145307.1, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145313.4, "dur": 0.8, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145312.7, "dur": 1.6, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145314.8, "dur": 121.7, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145306.4, "dur": 130.3, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145287.3, "dur": 150.1, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145271.7, "dur": 166.1, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145269.7, "dur": 168.5, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145443.1, "dur": 1.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145447.0, "dur": 2.5, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145454.0, "dur": 0.3, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145454.9, "dur": 64.2, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145453.7, "dur": 65.7, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145451.6, "dur": 67.9, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145446.4, "dur": 73.4, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145445.4, "dur": 74.8, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145522.3, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145532.0, "dur": 2.0, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145537.7, "dur": 0.9, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145539.6, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145542.2, "dur": 160383.3, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145540.4, "dur": 160386.2, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145535.6, "dur": 160392.7, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145531.6, "dur": 160399.0, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898145530.4, "dur": 160401.6, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898305947.8, "dur": 3.8, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898305957.1, "dur": 7.9, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898305972.0, "dur": 4.9, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898305970.2, "dur": 7.8, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898305980.8, "dur": 1.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898305984.2, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898305986.0, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898305987.7, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898305989.2, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898305991.6, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898305997.6, "dur": 0.8, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898305997.0, "dur": 1.5, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898305999.1, "dur": 144.5, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898305990.7, "dur": 153.1, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898305968.5, "dur": 175.7, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898305955.0, "dur": 189.7, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898305953.2, "dur": 192.0, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898306149.2, "dur": 1.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898306153.0, "dur": 2.4, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898306159.5, "dur": 0.2, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898306160.4, "dur": 62.4, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898306159.1, "dur": 64.0, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898306157.4, "dur": 65.9, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898306152.4, "dur": 71.1, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898306151.4, "dur": 72.4, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898306226.1, "dur": 1.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898306229.0, "dur": 1.5, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898306233.8, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898306235.7, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898306238.6, "dur": 68440.6, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898306236.6, "dur": 68444.1, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898306232.0, "dur": 68450.6, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898306228.7, "dur": 68456.2, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898306227.8, "dur": 68458.9, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898374704.5, "dur": 3.9, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898374713.9, "dur": 7.7, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898374728.0, "dur": 1.5, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898374726.5, "dur": 3.9, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898374739.9, "dur": 1.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898374743.5, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898374745.1, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898374746.8, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898374748.2, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898374750.3, "dur": 0.5, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898374756.4, "dur": 0.8, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898374755.8, "dur": 1.5, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898374757.8, "dur": 518.1, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898374749.7, "dur": 527.0, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898374724.6, "dur": 552.9, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898374711.8, "dur": 566.9, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898374709.8, "dur": 569.7, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898375759.0, "dur": 2.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898375765.4, "dur": 4.7, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898375774.7, "dur": 0.7, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898375776.2, "dur": 192.5, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898375774.3, "dur": 194.6, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898375772.4, "dur": 196.8, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898375764.2, "dur": 205.4, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898375762.8, "dur": 207.4, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898077384.9, "dur": 298585.6, "name": "forward (C:\\Users\\adam/.cache\\torch\\hub\\pytorch_vision_v0.10.0\\torchvision\\models\\resnet.py:121)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898077380.7, "dur": 298590.3, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898077378.9, "dur": 298593.0, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898375979.7, "dur": 2.0, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898375985.3, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898375988.7, "dur": 1.1, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898375993.1, "dur": 0.8, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898375994.9, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898375997.8, "dur": 68074.8, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898375995.8, "dur": 68078.1, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898375991.3, "dur": 68084.2, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898375988.4, "dur": 68088.8, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898375987.1, "dur": 68091.7, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444091.6, "dur": 4.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444101.8, "dur": 8.2, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444116.4, "dur": 1.8, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444114.9, "dur": 4.4, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444121.9, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444125.4, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444127.2, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444129.0, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444130.6, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444133.1, "dur": 0.3, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444141.6, "dur": 1.2, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444141.0, "dur": 1.82, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444143.4, "dur": 120.0, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444132.2, "dur": 137.6, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444113.2, "dur": 156.9, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444099.6, "dur": 170.9, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444097.6, "dur": 173.5, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444276.2, "dur": 1.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444280.5, "dur": 2.3, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444286.6, "dur": 0.3, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444287.5, "dur": 72.7, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444286.2, "dur": 74.2, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444284.6, "dur": 76.0, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444279.8, "dur": 81.0, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444278.7, "dur": 82.3, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444363.1, "dur": 1.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444365.9, "dur": 1.7, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444371.2, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444372.9, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444375.9, "dur": 167488.7, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444373.8, "dur": 167492.1, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444369.5, "dur": 167498.1, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444365.6, "dur": 167504.0, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898444364.6, "dur": 167506.6, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898611886.7, "dur": 3.9, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898611896.3, "dur": 7.5, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898611910.2, "dur": 1.5, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898611908.7, "dur": 3.8, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898611915.2, "dur": 1.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898611918.3, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898611920.0, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898611921.6, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898611923.1, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898611925.1, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898611930.8, "dur": 1.1, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898611930.2, "dur": 1.8, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898611932.5, "dur": 121.1, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898611924.5, "dur": 129.3, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898611906.7, "dur": 147.5, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898611894.0, "dur": 160.7, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898611892.0, "dur": 163.1, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898612059.5, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898612063.3, "dur": 2.4, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898612069.6, "dur": 0.3, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898612070.6, "dur": 65.6, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898612069.3, "dur": 67.1, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898612067.7, "dur": 68.9, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898612062.7, "dur": 74.1, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898612061.5, "dur": 75.6, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898612144.8, "dur": 1.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898612147.8, "dur": 1.6, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898612152.8, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898612154.5, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898612157.2, "dur": 69686.6, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898612155.3, "dur": 69689.5, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898612151.0, "dur": 69695.3, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898612147.4, "dur": 69701.0, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898612146.4, "dur": 69703.3, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898681867.1, "dur": 3.9, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898681876.7, "dur": 7.7, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898681890.7, "dur": 1.5, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898681889.4, "dur": 3.6, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898681895.8, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898681899.0, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898681900.6, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898681902.2, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898681903.6, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898681905.9, "dur": 0.3, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898681911.8, "dur": 1.1, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898681911.2, "dur": 1.8, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898681913.5, "dur": 474.5, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898681905.2, "dur": 483.1, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898681887.5, "dur": 501.2, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898681874.7, "dur": 514.6, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898681872.6, "dur": 517.2, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898682844.9, "dur": 1.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898682849.5, "dur": 3.1, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898682857.0, "dur": 0.4, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898682858.0, "dur": 190.9, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898682856.6, "dur": 192.6, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898682854.7, "dur": 194.7, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898682848.7, "dur": 201.0, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898682847.4, "dur": 202.6, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898375983.1, "dur": 307067.2, "name": "forward (C:\\Users\\adam/.cache\\torch\\hub\\pytorch_vision_v0.10.0\\torchvision\\models\\resnet.py:121)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898375979.2, "dur": 307072.0, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898375977.7, "dur": 307074.3, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897756680.9, "dur": 926375.0, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897756668.1, "dur": 926388.6, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897756666.1, "dur": 926391.3, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898683061.2, "dur": 1.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898683065.0, "dur": 1.7, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898683070.1, "dur": 0.5, "name": "collections.OrderedDict.values", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898683070.8, "dur": 0.6, "name": "builtins.iter", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898683069.0, "dur": 2.6, "name": "__iter__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:207)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898683074.7, "dur": 1.2, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898683078.3, "dur": 0.9, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898683084.5, "dur": 1.1, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898683088.9, "dur": 0.9, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898683090.8, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898683093.6, "dur": 136170.6, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898683091.8, "dur": 136174.1, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898683086.9, "dur": 136180.8, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898683084.3, "dur": 136185.2, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898683083.4, "dur": 136187.3, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819283.6, "dur": 4.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819293.3, "dur": 8.1, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819308.6, "dur": 1.6, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819306.9, "dur": 4.1, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819313.8, "dur": 1.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819316.9, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819318.5, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819320.1, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819321.5, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819323.7, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819329.7, "dur": 1.0, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819329.1, "dur": 1.62, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819331.3, "dur": 210.8, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819323.0, "dur": 219.4, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819304.8, "dur": 238.0, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819291.1, "dur": 252.3, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819289.5, "dur": 254.5, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819549.3, "dur": 1.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819553.4, "dur": 2.8, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819560.3, "dur": 0.3, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819561.6, "dur": 220.6, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819560.0, "dur": 222.9, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819558.2, "dur": 228.4, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819552.7, "dur": 235.2, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819551.6, "dur": 237.1, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819795.7, "dur": 2.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819801.7, "dur": 4.5, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819810.7, "dur": 0.8, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819812.8, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819816.1, "dur": 165233.8, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819813.8, "dur": 165237.4, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819808.2, "dur": 165244.7, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819800.6, "dur": 165254.4, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898819799.0, "dur": 165257.5, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985071.9, "dur": 4.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985081.1, "dur": 10.8, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985098.5, "dur": 1.8, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985096.9, "dur": 4.5, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985110.4, "dur": 1.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985113.7, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985115.4, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985117.3, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985118.7, "dur": 0.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985120.7, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985126.7, "dur": 0.9, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985126.1, "dur": 1.6, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985128.2, "dur": 111.6, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985120.1, "dur": 119.9, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985095.2, "dur": 145.1, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985079.1, "dur": 161.7, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985077.4, "dur": 163.9, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985245.2, "dur": 1.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985248.6, "dur": 2.8, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985255.6, "dur": 0.2, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985256.4, "dur": 52.3, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985255.2, "dur": 53.8, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985253.2, "dur": 56.0, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985248.1, "dur": 61.4, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985247.1, "dur": 62.6, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985312.1, "dur": 1.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985315.0, "dur": 1.7, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985320.5, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985322.1, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985325.3, "dur": 63161.0, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985323.0, "dur": 63164.8, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985318.5, "dur": 63171.2, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985314.7, "dur": 63177.0, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898985313.7, "dur": 63179.6, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048511.0, "dur": 4.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048521.1, "dur": 7.5, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048535.2, "dur": 1.5, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048533.5, "dur": 4.1, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048540.4, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048543.8, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048545.6, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048547.6, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048549.1, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048551.4, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048557.5, "dur": 0.9, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048556.9, "dur": 1.52, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048558.9, "dur": 194.7, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048550.8, "dur": 203.2, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048531.8, "dur": 222.7, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048519.1, "dur": 236.2, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048517.1, "dur": 238.7, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048766.5, "dur": 1.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048769.0, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048772.6, "dur": 2.8, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048779.6, "dur": 0.4, "name": "collections.OrderedDict.values", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048780.2, "dur": 0.8, "name": "builtins.iter", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048778.4, "dur": 2.8, "name": "__iter__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:207)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048784.1, "dur": 1.2, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048788.6, "dur": 0.8, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048790.6, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048793.4, "dur": 131741.1, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048791.4, "dur": 131744.8, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048786.8, "dur": 131751.5, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048783.8, "dur": 131756.2, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048782.9, "dur": 131758.3, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899180554.9, "dur": 8.2, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899180570.6, "dur": 1.7, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899180568.8, "dur": 4.4, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899180581.5, "dur": 3.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899180587.4, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899180589.4, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899180591.4, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899180593.0, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899180595.2, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899180601.3, "dur": 0.8, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899180600.7, "dur": 1.42, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899180602.6, "dur": 194.0, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899180594.6, "dur": 202.2, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899180566.8, "dur": 230.4, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899180552.6, "dur": 245.1, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899180547.2, "dur": 251.1, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048777.6, "dur": 132023.4, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048772.0, "dur": 132029.6, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899048770.7, "dur": 132031.9, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899181042.8, "dur": 2.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899181048.4, "dur": 4.0, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899181058.3, "dur": 0.3, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899181059.3, "dur": 101.9, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899181057.9, "dur": 103.5, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899181056.1, "dur": 105.6, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899181047.7, "dur": 114.3, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899181046.4, "dur": 115.8, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898683077.0, "dur": 498085.5, "name": "forward (C:\\Users\\adam/.cache\\torch\\hub\\pytorch_vision_v0.10.0\\torchvision\\models\\resnet.py:121)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898683074.5, "dur": 498089.9, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898683073.4, "dur": 498091.8, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899181168.9, "dur": 1.7, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899181176.5, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899181185.6, "dur": 1.4, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899181190.7, "dur": 0.8, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899181192.5, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899181195.3, "dur": 66779.0, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899181193.5, "dur": 66782.3, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899181188.8, "dur": 66788.9, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899181185.3, "dur": 66794.1, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899181184.0, "dur": 66797.1, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899247994.4, "dur": 4.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248004.7, "dur": 7.8, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248019.1, "dur": 2.0, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248017.2, "dur": 5.1, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248028.3, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248031.8, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248033.7, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248035.5, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248037.0, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248039.2, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248045.3, "dur": 1.0, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248044.6, "dur": 1.8, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248046.9, "dur": 95.0, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248038.6, "dur": 103.5, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248015.6, "dur": 127.0, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248002.2, "dur": 140.9, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248000.4, "dur": 143.2, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248148.8, "dur": 1.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248153.0, "dur": 2.3, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248159.3, "dur": 0.3, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248160.3, "dur": 50.0, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248158.9, "dur": 51.6, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248157.1, "dur": 53.7, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248152.5, "dur": 58.5, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248151.1, "dur": 60.2, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248213.3, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248216.6, "dur": 1.8, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248222.0, "dur": 1.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248224.1, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248227.2, "dur": 163242.9, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248225.2, "dur": 163246.6, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248220.2, "dur": 163253.8, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248216.3, "dur": 163260.2, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899248215.2, "dur": 163263.0, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411569.0, "dur": 5.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411580.0, "dur": 8.6, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411595.5, "dur": 2.1, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411593.7, "dur": 4.9, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411601.6, "dur": 1.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411612.2, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411614.0, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411615.7, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411617.2, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411619.7, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411626.2, "dur": 0.8, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411625.2, "dur": 1.9, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411627.6, "dur": 130.8, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411619.1, "dur": 139.5, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411591.7, "dur": 167.2, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411577.8, "dur": 181.6, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411575.7, "dur": 184.2, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411763.9, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411767.5, "dur": 2.5, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411774.6, "dur": 0.3, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411775.5, "dur": 54.5, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411774.3, "dur": 56.0, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411772.3, "dur": 58.2, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411767.0, "dur": 63.7, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411765.9, "dur": 65.1, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411833.6, "dur": 1.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411836.6, "dur": 1.7, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411841.6, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411843.2, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411846.5, "dur": 63290.3, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411844.3, "dur": 63294.0, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411839.8, "dur": 63300.9, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411836.4, "dur": 63306.2, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899411835.3, "dur": 63308.6, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475161.3, "dur": 4.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475171.8, "dur": 7.9, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475186.2, "dur": 1.8, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475184.6, "dur": 4.6, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475191.9, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475195.6, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475197.6, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475199.5, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475201.1, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475203.7, "dur": 0.3, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475209.3, "dur": 1.0, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475208.7, "dur": 1.7, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475210.9, "dur": 195.9, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475203.0, "dur": 204.0, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475182.9, "dur": 224.5, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475169.6, "dur": 238.3, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475167.6, "dur": 240.9, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475648.2, "dur": 1.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475657.4, "dur": 3.0, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475665.2, "dur": 0.3, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475666.2, "dur": 102.7, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475664.8, "dur": 104.3, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475662.6, "dur": 106.8, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475656.6, "dur": 113.1, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475655.4, "dur": 114.5, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899181172.0, "dur": 294598.3, "name": "forward (C:\\Users\\adam/.cache\\torch\\hub\\pytorch_vision_v0.10.0\\torchvision\\models\\resnet.py:121)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899181168.5, "dur": 294602.4, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899181167.0, "dur": 294604.9, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475777.5, "dur": 1.6, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475782.3, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475785.4, "dur": 1.1, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475789.5, "dur": 0.8, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475791.4, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475794.3, "dur": 66400.7, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475792.3, "dur": 66404.6, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475787.8, "dur": 66411.5, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475785.2, "dur": 66415.8, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475784.2, "dur": 66418.1, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542214.8, "dur": 4.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542224.5, "dur": 7.7, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542239.0, "dur": 1.5, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542237.5, "dur": 3.9, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542244.2, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542247.6, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542249.5, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542251.2, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542252.9, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542255.1, "dur": 0.3, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542261.3, "dur": 0.9, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542260.7, "dur": 1.6, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542262.8, "dur": 90.1, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542254.4, "dur": 98.6, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542235.4, "dur": 118.0, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542222.4, "dur": 131.4, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542220.7, "dur": 133.6, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542358.8, "dur": 1.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542362.9, "dur": 2.4, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542369.8, "dur": 0.3, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542370.7, "dur": 49.5, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542369.4, "dur": 51.1, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542367.4, "dur": 53.3, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542362.4, "dur": 58.5, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542361.2, "dur": 60.0, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542423.2, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542431.7, "dur": 1.9, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542437.1, "dur": 0.8, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542438.9, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542441.7, "dur": 162980.7, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542439.8, "dur": 162984.1, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542435.2, "dur": 162991.0, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542431.4, "dur": 162996.5, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899542430.3, "dur": 162999.0, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705444.5, "dur": 4.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705454.6, "dur": 7.7, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705469.0, "dur": 1.5, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705467.4, "dur": 4.0, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705474.4, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705478.0, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705479.9, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705481.6, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705483.2, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705485.5, "dur": 0.3, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705491.5, "dur": 0.8, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705490.9, "dur": 1.5, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705492.9, "dur": 85.5, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705484.8, "dur": 93.8, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705465.5, "dur": 113.4, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705452.6, "dur": 126.8, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705450.5, "dur": 129.3, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705583.7, "dur": 1.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705587.6, "dur": 2.5, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705594.0, "dur": 0.3, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705594.9, "dur": 50.0, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705593.7, "dur": 51.5, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705592.0, "dur": 53.4, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705587.0, "dur": 58.7, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705585.9, "dur": 60.1, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705648.5, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705653.6, "dur": 2.0, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705659.3, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705661.1, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705664.0, "dur": 63348.2, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705662.0, "dur": 63352.0, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705657.4, "dur": 63358.4, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705653.2, "dur": 63364.3, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899705652.0, "dur": 63367.0, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899769035.8, "dur": 3.8, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899769045.4, "dur": 7.1, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899769059.3, "dur": 1.6, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899769057.2, "dur": 4.6, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899769064.5, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899769073.1, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899769074.9, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899769076.9, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899769078.3, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899769080.3, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899769086.2, "dur": 0.9, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899769085.6, "dur": 1.6, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899769087.6, "dur": 311.2, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899769079.6, "dur": 319.7, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899769055.6, "dur": 344.2, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899769042.8, "dur": 357.7, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899769041.0, "dur": 360.3, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899769936.3, "dur": 2.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899769943.5, "dur": 5.3, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899769954.2, "dur": 0.4, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899769955.6, "dur": 479.2, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899769953.7, "dur": 482.3, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899769951.5, "dur": 485.6, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899769942.4, "dur": 495.5, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899769940.6, "dur": 498.6, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475780.4, "dur": 294659.6, "name": "forward (C:\\Users\\adam/.cache\\torch\\hub\\pytorch_vision_v0.10.0\\torchvision\\models\\resnet.py:121)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475777.1, "dur": 294664.0, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899475775.7, "dur": 294666.2, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899770455.2, "dur": 5.5, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899770468.6, "dur": 2.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899770473.8, "dur": 1.4, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899770481.7, "dur": 1.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899770484.0, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899770488.0, "dur": 68580.7, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899770485.1, "dur": 68585.3, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899770479.2, "dur": 68593.6, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899770473.4, "dur": 68601.2, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899770472.3, "dur": 68603.7, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839089.1, "dur": 4.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839098.9, "dur": 7.9, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839113.6, "dur": 1.7, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839112.1, "dur": 4.0, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839119.0, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839122.5, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839124.6, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839126.4, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839127.9, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839130.2, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839136.1, "dur": 0.9, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839135.7, "dur": 1.32, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839137.6, "dur": 88.5, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839129.5, "dur": 102.3, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839110.3, "dur": 121.8, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839096.8, "dur": 136.0, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839094.8, "dur": 138.5, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839238.8, "dur": 1.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839242.7, "dur": 2.4, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839249.2, "dur": 0.3, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839250.1, "dur": 71.9, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839248.8, "dur": 73.5, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839246.8, "dur": 75.7, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839242.2, "dur": 80.6, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839241.1, "dur": 81.9, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839325.1, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839328.2, "dur": 1.7, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839333.2, "dur": 0.9, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839335.2, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839338.0, "dur": 162241.5, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839336.1, "dur": 162244.9, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839331.5, "dur": 162251.1, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839327.8, "dur": 162256.8, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899839326.9, "dur": 162259.0, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001601.7, "dur": 4.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001611.3, "dur": 7.4, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001625.7, "dur": 1.7, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001623.9, "dur": 4.3, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001631.1, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001634.6, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001636.5, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001638.4, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001639.9, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001642.1, "dur": 0.3, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001648.1, "dur": 0.9, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001647.6, "dur": 1.5, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001649.6, "dur": 89.3, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001641.5, "dur": 97.6, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001621.8, "dur": 117.7, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001609.2, "dur": 130.9, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001607.5, "dur": 133.0, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001744.7, "dur": 1.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001748.7, "dur": 2.5, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001755.7, "dur": 0.2, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001756.5, "dur": 52.6, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001755.3, "dur": 54.0, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001753.4, "dur": 56.1, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001748.2, "dur": 61.6, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001746.9, "dur": 63.2, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001812.6, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001824.3, "dur": 2.0, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001829.9, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001831.7, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001834.8, "dur": 64394.0, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001832.5, "dur": 64397.4, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001827.9, "dur": 64404.0, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001821.0, "dur": 64412.8, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900001819.9, "dur": 64415.3, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900066254.5, "dur": 4.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900066265.2, "dur": 8.7, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900066281.2, "dur": 2.1, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900066279.4, "dur": 4.7, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900066287.4, "dur": 1.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900066290.8, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900066292.7, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900066294.5, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900066296.1, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900066298.4, "dur": 0.5, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900066304.9, "dur": 0.9, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900066304.3, "dur": 1.6, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900066306.4, "dur": 337.1, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900066297.7, "dur": 346.2, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900066277.5, "dur": 367.0, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900066263.0, "dur": 382.0, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900066260.6, "dur": 385.0, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067123.3, "dur": 2.9, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067132.0, "dur": 7.2, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067144.8, "dur": 0.6, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067146.1, "dur": 307.7, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067144.4, "dur": 310.0, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067141.8, "dur": 313.1, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067130.4, "dur": 325.2, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067128.6, "dur": 327.6, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899770463.1, "dur": 296993.9, "name": "forward (C:\\Users\\adam/.cache\\torch\\hub\\pytorch_vision_v0.10.0\\torchvision\\models\\resnet.py:121)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899770453.5, "dur": 297004.1, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292899770450.8, "dur": 297007.5, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898683068.2, "dur": 1384395.8, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898683064.6, "dur": 1384400.0, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292898683063.6, "dur": 1384401.8, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067472.4, "dur": 2.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067477.9, "dur": 3.1, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067486.1, "dur": 0.7, "name": "collections.OrderedDict.values", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067487.0, "dur": 0.7, "name": "builtins.iter", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067484.1, "dur": 3.8, "name": "__iter__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:207)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067490.6, "dur": 1.2, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067494.7, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067502.2, "dur": 1.3, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067507.4, "dur": 0.9, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067509.8, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067513.4, "dur": 135615.3, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067510.9, "dur": 135619.0, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067505.4, "dur": 135626.7, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067502.0, "dur": 135632.4, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067500.8, "dur": 135635.0, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203149.4, "dur": 4.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203159.5, "dur": 7.5, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203174.3, "dur": 1.6, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203172.4, "dur": 4.3, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203179.8, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203183.0, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203184.6, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203186.2, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203187.9, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203190.1, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203196.5, "dur": 0.9, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203195.9, "dur": 1.52, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203197.9, "dur": 432.8, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203189.5, "dur": 442.0, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203170.2, "dur": 462.0, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203157.3, "dur": 476.4, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203155.1, "dur": 480.2, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203647.5, "dur": 2.9, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203654.6, "dur": 5.6, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203665.6, "dur": 0.5, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203666.8, "dur": 132.6, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203665.1, "dur": 134.9, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203662.8, "dur": 137.9, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203653.2, "dur": 148.4, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203651.7, "dur": 150.5, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203808.7, "dur": 2.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203814.9, "dur": 5.0, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203824.6, "dur": 0.9, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203826.8, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203831.0, "dur": 245337.1, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203828.0, "dur": 245341.5, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203822.2, "dur": 245349.1, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203814.0, "dur": 245359.6, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900203812.4, "dur": 245362.7, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449191.7, "dur": 4.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449201.0, "dur": 10.7, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449218.0, "dur": 1.8, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449216.7, "dur": 4.3, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449223.6, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449235.2, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449236.8, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449238.4, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449239.8, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449241.9, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449248.0, "dur": 0.7, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449247.4, "dur": 1.4, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449249.3, "dur": 113.8, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449241.3, "dur": 122.4, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449215.1, "dur": 149.3, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449199.1, "dur": 166.1, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449197.3, "dur": 168.6, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449374.0, "dur": 2.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449379.9, "dur": 4.2, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449388.7, "dur": 0.4, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449389.8, "dur": 53.2, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449388.3, "dur": 54.9, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449386.5, "dur": 57.0, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449378.9, "dur": 64.8, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449377.5, "dur": 66.5, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449447.5, "dur": 1.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449450.7, "dur": 2.0, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449457.1, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449459.0, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449461.9, "dur": 63261.8, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449459.9, "dur": 63265.3, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449455.2, "dur": 63272.0, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449450.3, "dur": 63279.0, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900449449.3, "dur": 63281.3, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900512750.8, "dur": 4.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900512760.9, "dur": 8.1, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900512775.3, "dur": 1.9, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900512773.4, "dur": 4.6, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900512780.8, "dur": 1.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900512785.1, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900512787.0, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900512788.9, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900512790.4, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900512792.9, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900512798.6, "dur": 1.2, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900512798.1, "dur": 1.72, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900512800.4, "dur": 209.8, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900512792.3, "dur": 218.1, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900512771.6, "dur": 239.2, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900512758.6, "dur": 252.8, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900512756.4, "dur": 255.5, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900513016.5, "dur": 1.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900513019.1, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900513022.4, "dur": 2.4, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900513029.2, "dur": 0.4, "name": "collections.OrderedDict.values", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900513029.8, "dur": 0.6, "name": "builtins.iter", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900513027.9, "dur": 2.7, "name": "__iter__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:207)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900513033.7, "dur": 1.2, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900513038.3, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900513041.5, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900513044.3, "dur": 125768.1, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900513042.4, "dur": 125771.2, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900513036.3, "dur": 125779.2, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900513033.4, "dur": 125784.5, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900513032.5, "dur": 125786.5, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900638830.6, "dur": 8.3, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900638846.1, "dur": 1.8, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900638844.1, "dur": 5.0, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900638857.3, "dur": 2.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900638862.4, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900638864.5, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900638866.3, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900638867.9, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900638870.1, "dur": 0.6, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900638876.4, "dur": 0.7, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900638875.7, "dur": 1.5, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900638877.7, "dur": 185.9, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900638869.4, "dur": 194.5, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900638842.2, "dur": 222.1, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900638828.5, "dur": 236.4, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900638825.1, "dur": 240.6, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900513027.2, "dur": 126041.6, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900513022.0, "dur": 126047.5, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900513020.7, "dur": 126049.7, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900639259.2, "dur": 1.9, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900639263.9, "dur": 3.0, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900639272.0, "dur": 0.3, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900639273.0, "dur": 59.5, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900639271.6, "dur": 61.1, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900639269.5, "dur": 63.4, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900639263.2, "dur": 69.9, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900639262.1, "dur": 71.3, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067493.2, "dur": 571840.6, "name": "forward (C:\\Users\\adam/.cache\\torch\\hub\\pytorch_vision_v0.10.0\\torchvision\\models\\resnet.py:121)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067490.4, "dur": 571845.3, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067489.1, "dur": 571847.4, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900639339.7, "dur": 1.8, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900639344.8, "dur": 1.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900639348.1, "dur": 1.3, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900639353.1, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900639355.5, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900639358.2, "dur": 69438.9, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900639356.4, "dur": 69441.9, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900639351.4, "dur": 69448.6, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900639347.8, "dur": 69454.0, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900639346.8, "dur": 69456.3, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900708816.0, "dur": 4.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900708825.2, "dur": 7.9, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900708839.8, "dur": 1.6, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900708838.1, "dur": 4.1, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900708845.0, "dur": 1.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900708848.0, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900708849.7, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900708851.4, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900708852.8, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900708854.9, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900708860.8, "dur": 0.8, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900708860.2, "dur": 1.5, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900708862.2, "dur": 79.0, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900708854.3, "dur": 87.1, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900708836.4, "dur": 105.4, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900708823.2, "dur": 119.0, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900708821.6, "dur": 121.0, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900708947.2, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900708950.5, "dur": 2.5, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900708956.5, "dur": 0.3, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900708957.3, "dur": 50.0, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900708956.2, "dur": 51.3, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900708954.6, "dur": 53.2, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900708950.0, "dur": 58.0, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900708949.1, "dur": 59.2, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900709010.1, "dur": 0.9, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900709012.9, "dur": 4.6, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900709020.9, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900709022.6, "dur": 0.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900709025.1, "dur": 204769.4, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900709023.3, "dur": 204772.5, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900709019.0, "dur": 204778.8, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900709012.6, "dur": 204787.0, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900709011.6, "dur": 204789.4, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900913816.7, "dur": 3.8, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900913825.8, "dur": 7.9, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900913840.7, "dur": 1.5, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900913838.8, "dur": 4.3, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900913846.2, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900913849.6, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900913851.4, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900913853.1, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900913854.9, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900913857.1, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900913863.2, "dur": 0.8, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900913862.6, "dur": 1.5, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900913864.6, "dur": 105.1, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900913856.4, "dur": 113.6, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900913836.9, "dur": 133.4, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900913823.8, "dur": 146.9, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900913822.1, "dur": 149.0, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900913975.2, "dur": 1.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900913979.0, "dur": 2.4, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900913985.6, "dur": 0.2, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900913986.8, "dur": 55.3, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900913985.2, "dur": 57.2, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900913983.2, "dur": 59.4, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900913978.4, "dur": 64.5, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900913977.4, "dur": 65.8, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900914045.9, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900914049.2, "dur": 1.8, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900914054.4, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900914056.2, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900914058.9, "dur": 61360.5, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900914057.2, "dur": 61363.8, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900914052.7, "dur": 61370.1, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900914048.9, "dur": 61375.7, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900914047.8, "dur": 61378.3, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900975445.1, "dur": 4.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900975455.0, "dur": 8.0, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900975469.9, "dur": 1.7, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900975468.1, "dur": 4.4, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900975475.6, "dur": 1.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900975479.2, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900975481.1, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900975483.0, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900975484.6, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900975486.8, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900975492.9, "dur": 0.8, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900975492.3, "dur": 1.5, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900975494.3, "dur": 202.4, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900975486.1, "dur": 211.2, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900975466.0, "dur": 232.0, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900975452.8, "dur": 246.1, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900975450.9, "dur": 249.0, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900975977.6, "dur": 2.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900975983.5, "dur": 4.2, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900975992.6, "dur": 0.4, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900975993.8, "dur": 111.5, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900975992.2, "dur": 113.6, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900975990.0, "dur": 116.4, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900975982.5, "dur": 124.4, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900975981.1, "dur": 126.3, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900639342.9, "dur": 336765.0, "name": "forward (C:\\Users\\adam/.cache\\torch\\hub\\pytorch_vision_v0.10.0\\torchvision\\models\\resnet.py:121)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900639339.3, "dur": 336769.2, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900639337.9, "dur": 336771.2, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900976118.4, "dur": 3.8, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900976128.2, "dur": 2.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900976132.5, "dur": 1.5, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900976138.3, "dur": 0.8, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900976140.4, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900976143.7, "dur": 70613.8, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900976141.6, "dur": 70617.4, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900976135.7, "dur": 70625.3, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900976132.1, "dur": 70630.9, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900976131.1, "dur": 70633.6, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046776.9, "dur": 4.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046786.9, "dur": 7.8, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046801.8, "dur": 1.7, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046799.7, "dur": 4.7, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046807.0, "dur": 1.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046810.5, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046812.1, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046813.7, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046815.2, "dur": 0.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046817.5, "dur": 0.3, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046823.4, "dur": 0.9, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046822.7, "dur": 1.7, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046824.9, "dur": 71.8, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046816.6, "dur": 80.3, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046798.0, "dur": 99.3, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046784.6, "dur": 113.1, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046782.9, "dur": 115.1, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046902.4, "dur": 1.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046906.2, "dur": 2.3, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046912.6, "dur": 0.3, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046913.5, "dur": 49.3, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046912.1, "dur": 50.9, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046910.2, "dur": 53.1, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046905.8, "dur": 57.7, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046904.7, "dur": 59.1, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046965.7, "dur": 0.9, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046968.8, "dur": 1.6, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046973.8, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046975.7, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046981.8, "dur": 209950.4, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046976.5, "dur": 209957.0, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046971.8, "dur": 209964.4, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046968.5, "dur": 209969.7, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901046967.5, "dur": 209972.0, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901256956.0, "dur": 4.8, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901256966.3, "dur": 7.3, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901256980.1, "dur": 1.7, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901256978.4, "dur": 4.5, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901256985.7, "dur": 1.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901256988.9, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901256990.5, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901256992.2, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901256993.7, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901256995.9, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901257001.8, "dur": 0.8, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901257001.2, "dur": 1.5, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901257003.2, "dur": 89.1, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901256995.3, "dur": 97.2, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901256976.8, "dur": 116.1, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901256963.9, "dur": 129.4, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901256962.1, "dur": 131.5, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901257097.1, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901257100.6, "dur": 2.3, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901257107.2, "dur": 0.3, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901257108.1, "dur": 51.3, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901257106.5, "dur": 53.1, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901257104.6, "dur": 55.2, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901257100.1, "dur": 60.0, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901257099.0, "dur": 61.4, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901257162.7, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901257165.7, "dur": 1.6, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901257170.7, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901257172.4, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901257175.0, "dur": 61751.4, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901257173.3, "dur": 61754.6, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901257168.8, "dur": 61761.2, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901257165.4, "dur": 61766.2, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901257164.3, "dur": 61768.7, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901318957.5, "dur": 4.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901318968.5, "dur": 8.8, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901318984.4, "dur": 2.0, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901318982.7, "dur": 5.1, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901318990.7, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901318994.3, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901318995.9, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901318997.6, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901318999.1, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901319001.2, "dur": 0.5, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901319007.9, "dur": 1.1, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901319007.0, "dur": 2.02, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901319009.8, "dur": 203.0, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901319000.6, "dur": 212.9, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901318980.9, "dur": 233.4, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901318965.8, "dur": 249.3, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901318963.6, "dur": 252.4, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901319470.7, "dur": 2.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901319476.7, "dur": 4.6, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901319486.0, "dur": 0.4, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901319487.1, "dur": 104.9, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901319485.6, "dur": 106.7, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901319483.6, "dur": 108.9, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901319475.7, "dur": 117.1, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901319474.4, "dur": 118.7, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900976124.1, "dur": 343469.4, "name": "forward (C:\\Users\\adam/.cache\\torch\\hub\\pytorch_vision_v0.10.0\\torchvision\\models\\resnet.py:121)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900976117.4, "dur": 343477.1, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900976114.9, "dur": 343480.5, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901319601.3, "dur": 2.0, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901319606.6, "dur": 4.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901319612.7, "dur": 1.2, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901319617.4, "dur": 0.8, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901319619.3, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901319622.1, "dur": 69638.7, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901319620.2, "dur": 69642.1, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901319615.6, "dur": 69648.6, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901319612.5, "dur": 69653.7, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901319611.5, "dur": 69656.2, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389280.5, "dur": 4.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389289.7, "dur": 7.8, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389304.6, "dur": 1.9, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389302.9, "dur": 4.5, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389310.3, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389313.9, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389315.8, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389317.5, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389319.0, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389321.3, "dur": 0.6, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389327.2, "dur": 0.8, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389326.7, "dur": 1.4, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389328.6, "dur": 72.0, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389320.6, "dur": 80.2, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389301.0, "dur": 100.1, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389287.8, "dur": 113.7, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389285.9, "dur": 116.2, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389406.3, "dur": 1.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389410.1, "dur": 2.3, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389416.5, "dur": 0.3, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389417.5, "dur": 52.6, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389416.2, "dur": 54.1, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389414.3, "dur": 56.2, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389409.6, "dur": 61.2, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389408.5, "dur": 62.5, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389473.1, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389476.2, "dur": 1.7, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389481.6, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389483.5, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389486.5, "dur": 234946.5, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389484.5, "dur": 234949.8, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389479.8, "dur": 234956.5, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389476.0, "dur": 234961.9, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901389474.8, "dur": 234964.5, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624454.4, "dur": 3.9, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624463.5, "dur": 7.5, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624477.7, "dur": 1.5, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624476.0, "dur": 4.0, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624482.7, "dur": 1.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624485.7, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624487.3, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624489.0, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624490.7, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624492.7, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624499.0, "dur": 0.8, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624498.3, "dur": 1.6, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624500.4, "dur": 98.7, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624492.1, "dur": 107.2, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624474.3, "dur": 125.4, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624461.4, "dur": 138.7, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624459.7, "dur": 140.8, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624604.1, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624608.1, "dur": 2.4, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624614.7, "dur": 0.3, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624615.6, "dur": 54.0, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624614.1, "dur": 55.7, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624612.2, "dur": 57.8, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624607.6, "dur": 62.7, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624606.1, "dur": 64.4, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624672.9, "dur": 1.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624676.1, "dur": 1.6, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624680.8, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624682.5, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624685.4, "dur": 61032.8, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624683.3, "dur": 61036.2, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624679.2, "dur": 61043.3, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624675.8, "dur": 61048.6, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901624674.7, "dur": 61051.2, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901685745.8, "dur": 4.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901685756.8, "dur": 7.9, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901685771.8, "dur": 1.9, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901685770.3, "dur": 4.3, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901685777.4, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901685780.9, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901685782.7, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901685784.8, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901685786.4, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901685788.4, "dur": 0.5, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901685794.4, "dur": 0.8, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901685793.7, "dur": 1.6, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901685795.9, "dur": 204.6, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901685787.8, "dur": 213.4, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901685768.1, "dur": 233.8, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901685754.6, "dur": 248.2, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901685752.2, "dur": 251.6, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901686257.9, "dur": 2.8, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901686265.2, "dur": 4.9, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901686275.2, "dur": 0.5, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901686276.3, "dur": 115.5, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901686274.7, "dur": 117.7, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901686272.5, "dur": 120.6, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901686264.0, "dur": 129.8, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901686262.3, "dur": 132.1, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901319604.5, "dur": 366790.4, "name": "forward (C:\\Users\\adam/.cache\\torch\\hub\\pytorch_vision_v0.10.0\\torchvision\\models\\resnet.py:121)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901319600.8, "dur": 366794.7, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901319598.7, "dur": 366797.6, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901686427.7, "dur": 4.1, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901686438.1, "dur": 1.9, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901686442.4, "dur": 1.6, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901686448.5, "dur": 0.8, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901686450.7, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901686454.0, "dur": 72107.2, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901686451.9, "dur": 72110.8, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901686446.2, "dur": 72118.2, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901686442.1, "dur": 72123.9, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901686440.9, "dur": 72126.4, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758580.6, "dur": 4.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758590.4, "dur": 8.2, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758605.0, "dur": 1.9, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758603.4, "dur": 4.3, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758610.6, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758613.9, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758615.6, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758617.4, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758618.9, "dur": 0.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758621.0, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758626.9, "dur": 0.8, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758626.3, "dur": 1.42, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758628.2, "dur": 72.1, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758620.4, "dur": 80.1, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758601.6, "dur": 99.2, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758588.3, "dur": 112.9, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758586.5, "dur": 115.0, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758706.3, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758710.2, "dur": 2.3, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758716.3, "dur": 0.2, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758717.2, "dur": 49.5, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758715.9, "dur": 51.1, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758714.3, "dur": 52.9, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758709.4, "dur": 58.1, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758708.3, "dur": 59.4, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758769.5, "dur": 1.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758772.5, "dur": 1.7, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758777.3, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758779.0, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758781.7, "dur": 247853.4, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758779.8, "dur": 247856.7, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758775.6, "dur": 247862.9, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758772.2, "dur": 247868.7, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901758771.1, "dur": 247871.2, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006660.3, "dur": 4.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006671.4, "dur": 8.5, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006687.4, "dur": 1.7, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006685.7, "dur": 4.2, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006692.8, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006696.5, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006698.4, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006700.3, "dur": 0.8, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006702.3, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006704.5, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006711.3, "dur": 1.1, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006710.7, "dur": 1.8, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006713.4, "dur": 120.1, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006703.9, "dur": 130.4, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006683.3, "dur": 151.8, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006668.7, "dur": 167.5, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006666.5, "dur": 170.6, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006845.9, "dur": 2.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006852.6, "dur": 5.2, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006863.2, "dur": 0.6, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006864.6, "dur": 53.2, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006862.5, "dur": 55.5, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006860.5, "dur": 57.8, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006851.5, "dur": 67.0, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006850.0, "dur": 68.8, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006922.2, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006926.0, "dur": 2.4, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006932.5, "dur": 1.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006934.6, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006937.6, "dur": 61157.6, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006935.6, "dur": 61161.3, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006930.3, "dur": 61168.4, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006925.6, "dur": 61174.9, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902006924.2, "dur": 61177.8, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068121.5, "dur": 4.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068131.7, "dur": 8.7, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068147.5, "dur": 1.9, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068146.0, "dur": 4.2, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068153.4, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068160.2, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068162.1, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068163.9, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068165.9, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068168.2, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068175.0, "dur": 0.8, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068173.9, "dur": 1.92, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068176.4, "dur": 206.5, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068167.5, "dur": 215.8, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068144.0, "dur": 240.2, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068129.4, "dur": 255.2, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068127.0, "dur": 258.2, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068636.2, "dur": 1.8, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068641.7, "dur": 3.1, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068650.2, "dur": 0.4, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068651.3, "dur": 109.9, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068649.6, "dur": 112.5, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068647.2, "dur": 115.8, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068640.9, "dur": 123.0, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068639.0, "dur": 125.7, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901686433.8, "dur": 382331.7, "name": "forward (C:\\Users\\adam/.cache\\torch\\hub\\pytorch_vision_v0.10.0\\torchvision\\models\\resnet.py:121)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901686426.7, "dur": 382339.5, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292901686402.4, "dur": 382364.6, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068778.4, "dur": 4.1, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068789.2, "dur": 2.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068793.8, "dur": 1.8, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068800.0, "dur": 1.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068802.3, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068805.4, "dur": 71839.0, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068803.5, "dur": 71842.2, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068797.8, "dur": 71850.2, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068793.4, "dur": 71856.6, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068792.3, "dur": 71859.4, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140665.5, "dur": 4.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140675.9, "dur": 8.4, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140691.4, "dur": 1.8, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140689.5, "dur": 4.7, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140697.5, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140701.0, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140702.8, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140704.6, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140706.1, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140708.5, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140714.6, "dur": 0.9, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140714.0, "dur": 1.52, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140716.5, "dur": 125.7, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140707.8, "dur": 134.7, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140687.8, "dur": 155.0, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140673.6, "dur": 169.6, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140671.5, "dur": 172.2, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140848.9, "dur": 1.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140853.3, "dur": 2.5, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140860.9, "dur": 0.2, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140861.7, "dur": 64.5, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140860.5, "dur": 66.0, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140858.3, "dur": 68.7, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140852.7, "dur": 74.6, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140851.5, "dur": 76.2, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140930.1, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140933.6, "dur": 2.1, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140939.9, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140941.9, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140944.8, "dur": 219241.2, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140943.0, "dur": 219243.9, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140937.9, "dur": 219250.8, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140933.2, "dur": 219257.6, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902140932.1, "dur": 219259.9, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360208.3, "dur": 3.8, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360217.8, "dur": 7.3, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360232.0, "dur": 1.6, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360230.4, "dur": 4.2, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360237.8, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360241.3, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360243.2, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360245.0, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360246.5, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360248.8, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360255.1, "dur": 0.9, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360254.5, "dur": 1.6, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360256.6, "dur": 110.0, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360248.1, "dur": 119.1, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360228.0, "dur": 140.0, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360215.7, "dur": 153.2, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360213.6, "dur": 156.0, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360377.6, "dur": 2.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360383.3, "dur": 4.0, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360391.9, "dur": 0.4, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360393.0, "dur": 78.2, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360391.6, "dur": 80.2, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360389.7, "dur": 82.4, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360382.4, "dur": 90.1, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360381.0, "dur": 91.8, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360476.3, "dur": 1.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360480.2, "dur": 2.6, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360487.2, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360489.2, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360492.6, "dur": 63990.7, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360490.3, "dur": 63994.3, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360485.1, "dur": 64002.1, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360479.8, "dur": 64009.4, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902360478.4, "dur": 64012.4, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902424509.2, "dur": 4.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902424519.3, "dur": 7.4, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902424534.0, "dur": 1.8, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902424531.9, "dur": 4.8, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902424539.7, "dur": 0.9, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902424543.0, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902424544.6, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902424546.4, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902424547.9, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902424550.6, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902424560.2, "dur": 0.8, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902424559.5, "dur": 1.6, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902424561.6, "dur": 203.3, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902424549.9, "dur": 215.5, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902424529.7, "dur": 236.4, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902424516.8, "dur": 250.2, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902424514.8, "dur": 252.9, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425022.2, "dur": 1.8, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425027.0, "dur": 3.3, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425034.5, "dur": 0.4, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425035.5, "dur": 74.5, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425034.1, "dur": 76.2, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425032.2, "dur": 78.3, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425026.3, "dur": 84.6, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425025.2, "dur": 86.0, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068784.4, "dur": 356327.1, "name": "forward (C:\\Users\\adam/.cache\\torch\\hub\\pytorch_vision_v0.10.0\\torchvision\\models\\resnet.py:121)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068777.3, "dur": 356334.7, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902068774.5, "dur": 356338.5, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067483.0, "dur": 2357633.5, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067477.1, "dur": 2357639.8, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292900067475.5, "dur": 2357642.3, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425122.6, "dur": 1.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425126.8, "dur": 1.7, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425132.1, "dur": 0.6, "name": "collections.OrderedDict.values", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425132.9, "dur": 0.6, "name": "builtins.iter", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425130.9, "dur": 2.9, "name": "__iter__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:207)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425136.4, "dur": 1.2, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425140.0, "dur": 1.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425142.7, "dur": 1.1, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425146.9, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425148.6, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425151.4, "dur": 135940.1, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425149.5, "dur": 135943.6, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425145.2, "dur": 135949.8, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425142.5, "dur": 135954.3, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425141.8, "dur": 135956.8, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561111.5, "dur": 3.8, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561120.9, "dur": 8.0, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561135.5, "dur": 2.0, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561133.8, "dur": 4.6, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561141.3, "dur": 1.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561144.6, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561146.5, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561148.3, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561150.0, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561152.2, "dur": 0.6, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561158.4, "dur": 0.9, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561157.7, "dur": 1.62, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561159.9, "dur": 94.1, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561151.5, "dur": 102.7, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561132.0, "dur": 122.5, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561118.7, "dur": 136.2, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561116.8, "dur": 138.6, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561260.1, "dur": 1.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561264.0, "dur": 2.6, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561270.8, "dur": 0.3, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561271.8, "dur": 49.8, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561270.4, "dur": 51.4, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561268.6, "dur": 53.4, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561263.5, "dur": 58.8, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561262.4, "dur": 60.2, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561324.6, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561327.9, "dur": 1.8, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561333.6, "dur": 0.8, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561335.6, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561338.3, "dur": 203420.7, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561336.5, "dur": 203424.5, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561331.5, "dur": 203431.5, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561327.6, "dur": 203437.1, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902561326.4, "dur": 203440.0, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902764783.2, "dur": 4.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902764793.6, "dur": 7.9, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902764808.8, "dur": 1.4, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902764806.6, "dur": 4.5, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902764813.9, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902764817.8, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902764819.7, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902764821.6, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902764823.1, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902764825.4, "dur": 0.3, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902764831.7, "dur": 0.9, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902764830.7, "dur": 2.0, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902764833.2, "dur": 94.8, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902764824.7, "dur": 103.6, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902764804.4, "dur": 124.3, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902764790.7, "dur": 138.4, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902764788.8, "dur": 140.7, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902764933.7, "dur": 1.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902764937.5, "dur": 2.3, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902764944.1, "dur": 0.3, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902764945.0, "dur": 51.1, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902764943.7, "dur": 52.7, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902764942.0, "dur": 54.6, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902764937.1, "dur": 59.7, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902764936.0, "dur": 61.1, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902764999.4, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902765002.6, "dur": 1.5, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902765007.3, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902765009.0, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902765012.0, "dur": 62253.1, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902765009.9, "dur": 62256.2, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902765005.6, "dur": 62262.4, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902765002.2, "dur": 62267.8, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902765001.3, "dur": 62272.9, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827292.2, "dur": 4.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827302.2, "dur": 7.8, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827316.7, "dur": 1.6, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827314.6, "dur": 4.7, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827322.1, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827326.2, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827328.1, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827330.0, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827331.6, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827334.5, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827340.4, "dur": 0.8, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827339.6, "dur": 1.7, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827341.8, "dur": 105.8, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827333.5, "dur": 114.4, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827312.9, "dur": 135.3, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827299.8, "dur": 149.4, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827297.9, "dur": 151.9, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827453.8, "dur": 1.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827456.4, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827460.0, "dur": 2.4, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827466.4, "dur": 0.5, "name": "collections.OrderedDict.values", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827467.1, "dur": 0.7, "name": "builtins.iter", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827465.3, "dur": 2.7, "name": "__iter__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:207)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827470.8, "dur": 1.2, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827476.0, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827477.7, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827480.8, "dur": 128020.8, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827478.6, "dur": 128024.4, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827473.9, "dur": 128031.3, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827470.5, "dur": 128036.5, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827469.4, "dur": 128038.8, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902955520.4, "dur": 8.8, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902955536.7, "dur": 1.6, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902955534.6, "dur": 4.6, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902955547.4, "dur": 2.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902955553.3, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902955555.3, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902955557.5, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902955559.1, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902955573.1, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902955579.1, "dur": 0.8, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902955578.6, "dur": 1.8, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902955580.9, "dur": 169.2, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902955572.3, "dur": 178.1, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902955532.8, "dur": 217.9, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902955517.6, "dur": 233.7, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902955514.4, "dur": 237.7, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827464.6, "dur": 128290.5, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827459.5, "dur": 128296.1, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902827457.9, "dur": 128298.5, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902955985.2, "dur": 2.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902955991.1, "dur": 3.4, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902955999.0, "dur": 0.3, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902955999.9, "dur": 67.3, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902955998.7, "dur": 68.7, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902955996.8, "dur": 70.8, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902955990.3, "dur": 77.5, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902955988.9, "dur": 79.2, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425138.6, "dur": 530929.8, "name": "forward (C:\\Users\\adam/.cache\\torch\\hub\\pytorch_vision_v0.10.0\\torchvision\\models\\resnet.py:121)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425136.2, "dur": 530934.2, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425135.0, "dur": 530936.2, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902956075.2, "dur": 1.6, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902956079.6, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902956082.8, "dur": 1.0, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902956086.9, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902956088.6, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902956091.4, "dur": 63480.0, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902956089.6, "dur": 63483.3, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902956085.2, "dur": 63489.8, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902956082.5, "dur": 63494.2, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902956081.6, "dur": 63496.5, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019592.3, "dur": 4.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019602.7, "dur": 8.7, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019618.3, "dur": 1.8, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019616.7, "dur": 4.5, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019624.0, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019627.7, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019629.6, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019631.5, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019633.1, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019635.6, "dur": 0.8, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019641.8, "dur": 1.2, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019641.1, "dur": 1.92, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019643.6, "dur": 78.6, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019635.0, "dur": 87.5, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019614.9, "dur": 107.9, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019600.7, "dur": 122.5, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019598.8, "dur": 124.8, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019728.3, "dur": 1.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019732.4, "dur": 2.1, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019739.0, "dur": 0.2, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019739.8, "dur": 47.2, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019738.6, "dur": 48.6, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019736.6, "dur": 50.8, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019731.8, "dur": 55.8, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019730.7, "dur": 57.2, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019789.7, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019793.2, "dur": 1.5, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019798.1, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019799.9, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019802.9, "dur": 183230.3, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019801.1, "dur": 183233.4, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019796.3, "dur": 183240.6, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019792.9, "dur": 183245.7, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903019791.6, "dur": 183248.4, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203056.0, "dur": 4.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203066.7, "dur": 7.1, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203081.1, "dur": 1.8, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203078.9, "dur": 4.9, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203086.6, "dur": 1.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203090.0, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203091.6, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203093.3, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203094.7, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203097.1, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203102.7, "dur": 1.0, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203102.1, "dur": 1.7, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203104.3, "dur": 64.3, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203096.4, "dur": 72.5, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203077.1, "dur": 92.1, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203063.8, "dur": 105.8, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203062.1, "dur": 107.9, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203173.4, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203176.9, "dur": 2.3, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203183.6, "dur": 0.3, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203184.5, "dur": 51.8, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203182.9, "dur": 53.6, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203181.1, "dur": 55.6, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203176.5, "dur": 60.4, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203175.4, "dur": 61.7, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203239.9, "dur": 0.9, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203242.9, "dur": 1.6, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203247.8, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203249.5, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203252.1, "dur": 61950.8, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203250.3, "dur": 61954.1, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203246.0, "dur": 61960.3, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203242.6, "dur": 61965.4, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903203241.5, "dur": 61968.3, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265227.2, "dur": 4.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265237.3, "dur": 7.8, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265251.4, "dur": 1.5, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265249.9, "dur": 3.8, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265256.4, "dur": 1.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265259.9, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265261.9, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265263.8, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265265.3, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265267.6, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265273.7, "dur": 0.9, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265273.1, "dur": 1.6, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265275.2, "dur": 107.3, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265266.9, "dur": 115.8, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265247.9, "dur": 135.2, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265235.3, "dur": 148.2, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265233.1, "dur": 150.8, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265490.7, "dur": 1.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265495.0, "dur": 2.9, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265502.1, "dur": 0.3, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265503.0, "dur": 46.2, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265501.8, "dur": 47.6, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265500.2, "dur": 49.4, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265494.4, "dur": 55.5, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265493.2, "dur": 57.0, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902956078.0, "dur": 309472.6, "name": "forward (C:\\Users\\adam/.cache\\torch\\hub\\pytorch_vision_v0.10.0\\torchvision\\models\\resnet.py:121)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902956074.8, "dur": 309476.3, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902956073.2, "dur": 309479.1, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265557.3, "dur": 1.5, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265561.7, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265564.5, "dur": 1.1, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265568.9, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265570.5, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265573.1, "dur": 62279.0, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265571.4, "dur": 62284.8, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265567.2, "dur": 62290.9, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265564.3, "dur": 62295.6, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265563.5, "dur": 62298.3, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903327874.8, "dur": 4.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903327885.0, "dur": 8.0, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903327899.5, "dur": 1.8, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903327897.7, "dur": 4.4, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903327904.7, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903327908.3, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903327910.2, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903327912.1, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903327913.7, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903327916.1, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903327922.0, "dur": 0.8, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903327921.2, "dur": 1.7, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903327923.7, "dur": 63.9, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903327915.3, "dur": 72.5, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903327895.9, "dur": 92.2, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903327882.9, "dur": 105.7, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903327881.0, "dur": 108.1, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903327993.8, "dur": 1.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903327997.6, "dur": 2.6, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903328004.4, "dur": 0.3, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903328005.3, "dur": 47.8, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903328004.1, "dur": 49.3, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903328002.3, "dur": 51.3, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903327997.1, "dur": 56.7, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903327996.1, "dur": 57.9, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903328056.0, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903328059.3, "dur": 1.6, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903328065.4, "dur": 0.8, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903328067.3, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903328070.1, "dur": 200192.4, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903328068.3, "dur": 200196.0, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903328063.2, "dur": 200205.6, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903328059.0, "dur": 200211.6, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903328057.8, "dur": 200214.5, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528288.2, "dur": 4.0, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528298.3, "dur": 7.2, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528312.7, "dur": 1.6, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528310.9, "dur": 4.2, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528318.3, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528321.9, "dur": 0.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528323.7, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528325.6, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528327.2, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528329.4, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528335.8, "dur": 0.8, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528335.2, "dur": 1.5, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528337.2, "dur": 71.6, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528328.7, "dur": 80.3, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528308.8, "dur": 100.6, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528296.2, "dur": 113.6, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528294.1, "dur": 116.0, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528414.1, "dur": 1.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528417.9, "dur": 2.4, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528424.1, "dur": 0.3, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528425.0, "dur": 47.6, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528423.8, "dur": 49.0, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528422.1, "dur": 50.9, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528417.4, "dur": 55.8, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528416.3, "dur": 57.1, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528476.1, "dur": 1.1, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528479.3, "dur": 1.6, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528484.6, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528486.5, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528489.4, "dur": 62151.0, "name": "torch.conv2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528487.5, "dur": 62154.7, "name": "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528482.8, "dur": 62161.2, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528479.0, "dur": 62166.6, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903528477.9, "dur": 62169.0, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590665.0, "dur": 3.9, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590675.1, "dur": 7.4, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590689.0, "dur": 1.6, "name": "Tensor.dim", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590687.1, "dur": 4.4, "name": "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590694.0, "dur": 1.2, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590698.1, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590700.0, "dur": 0.7, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590701.8, "dur": 0.4, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590703.3, "dur": 0.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590705.6, "dur": 0.4, "name": "torch._C._has_torch_function_variadic", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590711.8, "dur": 0.8, "name": "torch._C._get_cudnn_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590710.9, "dur": 1.8, "name": "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590713.2, "dur": 99.4, "name": "torch.batch_norm", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590705.0, "dur": 107.9, "name": "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590685.4, "dur": 127.9, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590673.0, "dur": 140.7, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590670.9, "dur": 143.3, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590920.9, "dur": 1.5, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590925.1, "dur": 2.5, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590931.9, "dur": 0.3, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590932.8, "dur": 44.1, "name": "torch.relu_", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590931.6, "dur": 45.5, "name": "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590929.6, "dur": 47.8, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590924.6, "dur": 53.0, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590923.5, "dur": 54.4, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265560.0, "dur": 325418.1, "name": "forward (C:\\Users\\adam/.cache\\torch\\hub\\pytorch_vision_v0.10.0\\torchvision\\models\\resnet.py:121)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265557.0, "dur": 325421.7, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903265555.6, "dur": 325423.9, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425130.1, "dur": 1165851.7, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425126.4, "dur": 1165855.9, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292902425125.0, "dur": 1165858.0, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590987.9, "dur": 1.9, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590993.6, "dur": 1.8, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903591003.7, "dur": 0.2, "name": "torch._C._has_torch_function_unary", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903591005.1, "dur": 3.7, "name": "Tensor.size", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903591016.5, "dur": 0.7, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903591017.6, "dur": 0.3, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903591018.0, "dur": 0.1, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903591019.9, "dur": 0.1, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903591024.6, "dur": 1.6, "name": "<listcomp> (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\utils.py:41)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903591011.3, "dur": 15.6, "name": "_list_with_default (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\utils.py:33)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903591028.3, "dur": 887.1, "name": "torch._C._nn.adaptive_avg_pool2d", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903591003.5, "dur": 912.4, "name": "adaptive_avg_pool2d (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1221)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590999.8, "dur": 916.8, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:1189)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590993.2, "dur": 923.9, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903590991.6, "dur": 926.1, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903591922.5, "dur": 19.7, "name": "torch.flatten", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903591947.2, "dur": 1.6, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903591952.1, "dur": 2.6, "name": "torch._C._get_tracing_state", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903591961.1, "dur": 0.9, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903591963.2, "dur": 0.3, "name": "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903591963.7, "dur": 3139.6, "name": "torch._C._nn.linear", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903591958.8, "dur": 3145.2, "name": "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:115)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903591951.6, "dur": 3153.7, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903591950.2, "dur": 3155.9, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897569490.8, "dur": 6025620.6, "name": "_forward_impl (C:\\Users\\adam/.cache\\torch\\hub\\pytorch_vision_v0.10.0\\torchvision\\models\\resnet.py:230)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897569489.8, "dur": 6025622.1, "name": "forward (C:\\Users\\adam/.cache\\torch\\hub\\pytorch_vision_v0.10.0\\torchvision\\models\\resnet.py:248)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897569477.6, "dur": 6025634.8, "name": "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292897569475.1, "dur": 6025638.1, "name": "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903595132.4, "dur": 0.7, "name": "torch.is_grad_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903595134.5, "dur": 1.9, "name": "torch._C._set_grad_enabled", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903595131.5, "dur": 5.1, "name": "__init__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\autograd\\grad_mode.py:183)", "ph": "X", "cat": "FEE"}, {"pid": 29736, "tid": 14104, "ts": 2292903595127.5, "dur": 9.9, "name": "__exit__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\autograd\\grad_mode.py:83)", "ph": "X", "cat": "FEE"}], "viztracer_metadata": {"version": "0.16.2", "overflow": false}, "file_info": {"files": {"C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py": ["# Extra utilities for working with context managers that should have been\n# in the standard library but are not\n\nimport functools\nimport inspect\nimport warnings\nimport sys\nfrom typing import Any, Callable, TypeVar, cast\n\n# Used for annotating the decorator usage of _DecoratorContextManager (e.g.,\n# 'no_grad' and 'enable_grad').\n# See https://mypy.readthedocs.io/en/latest/generics.html#declaring-decorators\nFuncType = Callable[..., Any]\nF = TypeVar('F', bound=FuncType)\n\n\ndef _wrap_generator(ctx_factory, func):\n    \"\"\"\n    Wrap each generator invocation with the context manager factory.\n\n    The input should be a function that returns a context manager,\n    not a context manager itself, to handle one-shot context managers.\n    \"\"\"\n    @functools.wraps(func)\n    def generator_context(*args, **kwargs):\n        gen = func(*args, **kwargs)\n\n        # Generators are suspended and unsuspended at `yield`, hence we\n        # make sure the grad mode is properly set every time the execution\n        # flow returns into the wrapped generator and restored when it\n        # returns through our `yield` to our caller (see PR #49017).\n        try:\n            # Issuing `None` to a generator fires it up\n            with ctx_factory():\n                response = gen.send(None)\n\n            while True:\n                try:\n                    # Forward the response to our caller and get its next request\n                    request = yield response\n\n                except GeneratorExit:\n                    # Inform the still active generator about its imminent closure\n                    with ctx_factory():\n                        gen.close()\n                    raise\n\n                except BaseException:\n                    # Propagate the exception thrown at us by the caller\n                    with ctx_factory():\n                        response = gen.throw(*sys.exc_info())\n\n                else:\n                    # Pass the last request to the generator and get its response\n                    with ctx_factory():\n                        response = gen.send(request)\n\n        # We let the exceptions raised above by the generator's `.throw` or\n        # `.send` methods bubble up to our caller, except for StopIteration\n        except StopIteration as e:\n            # The generator informed us that it is done: take whatever its\n            # returned value (if any) was and indicate that we're done too\n            # by returning it (see docs for python's return-statement).\n            return e.value\n\n    return generator_context\n\n\ndef context_decorator(ctx, func):\n    \"\"\"\n    Like contextlib.ContextDecorator.\n\n    But with the following differences:\n    1. Is done by wrapping, rather than inheritance, so it works with context\n       managers that are implemented from C and thus cannot easily inherit from\n       Python classes\n    2. Wraps generators in the intuitive way (c.f. https://bugs.python.org/issue37743)\n    3. Errors out if you try to wrap a class, because it is ambiguous whether\n       or not you intended to wrap only the constructor\n\n    The input argument can either be a context manager (in which case it must\n    be a multi-shot context manager that can be directly invoked multiple times)\n    or a callable that produces a context manager.\n    \"\"\"\n    assert not (callable(ctx) and hasattr(ctx, '__enter__')), (\n        f\"Passed in {ctx} is both callable and also a valid context manager \"\n        \"(has __enter__), making it ambiguous which interface to use.  If you \"\n        \"intended to pass a context manager factory, rewrite your call as \"\n        \"context_decorator(lambda: ctx()); if you intended to pass a context \"\n        \"manager directly, rewrite your call as context_decorator(lambda: ctx)\"\n    )\n\n    if not callable(ctx):\n        def ctx_factory():\n            return ctx\n    else:\n        ctx_factory = ctx\n\n    if inspect.isclass(func):\n        raise RuntimeError(\n            \"Cannot decorate classes; it is ambiguous whether or not only the \"\n            \"constructor or all methods should have the context manager applied; \"\n            \"additionally, decorating a class at definition-site will prevent \"\n            \"use of the identifier as a conventional type.  \"\n            \"To specify which methods to decorate, decorate each of them \"\n            \"individually.\"\n        )\n\n    if inspect.isgeneratorfunction(func):\n        return _wrap_generator(ctx_factory, func)\n\n    @functools.wraps(func)\n    def decorate_context(*args, **kwargs):\n        with ctx_factory():\n            return func(*args, **kwargs)\n\n    return decorate_context\n\n\nclass _DecoratorContextManager:\n    \"\"\"Allow a context manager to be used as a decorator.\"\"\"\n\n    def __call__(self, orig_func: F) -> F:\n        if inspect.isclass(orig_func):\n            warnings.warn(\"Decorating classes is deprecated and will be disabled in \"\n                          \"future versions. You should only decorate functions or methods. \"\n                          \"To preserve the current behavior of class decoration, you can \"\n                          \"directly decorate the `__init__` method and nothing else.\")\n            func = cast(F, lambda *args, **kwargs: orig_func(*args, **kwargs))\n        else:\n            func = orig_func\n\n        return cast(F, context_decorator(self.clone, func))\n\n    def __enter__(self) -> None:\n        raise NotImplementedError\n\n    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n        raise NotImplementedError\n\n    def clone(self):\n        # override this method if your children class takes __init__ parameters\n        return self.__class__()\n\n\nclass _NoParamDecoratorContextManager(_DecoratorContextManager):\n    \"\"\"Allow a context manager to be used as a decorator without parentheses.\"\"\"\n\n    def __new__(cls, orig_func=None):\n        if orig_func is None:\n            return super().__new__(cls)\n        return cls()(orig_func)\n", 152], "C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\_jit_internal.py": ["\"\"\"\nThe weak_script annotation needs to be here instead of inside torch/jit/ so it\ncan be used in other places in torch/ (namely torch.nn) without running into\ncircular dependency problems\n\"\"\"\n\nimport ast\nimport builtins\nimport collections\nimport contextlib\nimport enum\nimport inspect\nimport io\nimport pickle\nimport sys\nimport threading\nimport types\nimport typing\nimport warnings\nimport weakref\nfrom textwrap import dedent\nfrom typing import (  # noqa: F401\n    Any,\n    Callable,\n    Dict,\n    Final,\n    ForwardRef,\n    Generic,\n    get_args,  # new in 3.8\n    get_origin,  # new in 3.8\n    List,\n    Optional,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n)\n\nimport torch\n\n# This is needed. `torch._jit_internal` is imported before `torch.distributed.__init__`.\n# Explicitly ask to import `torch.distributed.__init__` first.\n# Otherwise, \"AttributeError: module 'torch' has no attribute 'distributed'\" is raised.\nimport torch.distributed.rpc\nimport torch.package._mangling as package_mangling\nfrom torch._awaits import _Await\nfrom torch._C import _Await as CAwait, Future as CFuture\nfrom torch._sources import fake_range, get_source_lines_and_file, parse_def\nfrom torch.futures import Future\n\nIS_PY39_PLUS: Final[bool] = sys.version_info >= (3, 9)\nIS_PY310_PLUS: Final[bool] = sys.version_info >= (3, 10)\n\nBuiltinUnionType: Union[Type, Tuple[Type, ...]]\nif sys.version_info >= (3, 10):\n    # NOTE: IS_PY310_PLUS doesn't work with mypy.\n    # cf. https://mypy.readthedocs.io/en/stable/common_issues.html#python-version-and-system-platform-checks\n    BuiltinUnionType = types.UnionType\nelse:\n    BuiltinUnionType = ()  # trick: this makes isinstance short circuit.\n\nLockType: Type\ntry:\n    import _thread\n\n    LockType = _thread.LockType\nexcept ImportError:\n    import _dummy_thread\n\n    LockType = _dummy_thread.LockType\n\n# Wrapper functions that can call either of 2 functions depending on a boolean\n# argument\nboolean_dispatched: \"weakref.WeakKeyDictionary[Callable, Dict[str, Callable]]\" = (\n    weakref.WeakKeyDictionary()\n)  # noqa: T484\n\n\nFAKE_FILENAME_PREFIX = \"__torch_jit_dataclass\"\n\n\nclass SourceLoader:\n    def __init__(self):\n        self.content = {}\n\n    def cache(self, fn, source):\n        self.content[fn] = source\n\n    def get_source(self, fn):\n        return self.content.get(fn)\n\n\nloader = SourceLoader()\n\n\ndef createResolutionCallbackFromEnv(lookup_base):\n    \"\"\"\n    Creates a resolution callback that will look up qualified names in an\n    environment, starting with `lookup_base` for the base of any qualified\n    names, then proceeding down the lookup chain with the resolved object.\n\n    You should not use this directly, it should only be used from the other\n    createResolutionCallbackFrom* functions.\n    \"\"\"\n\n    def lookupInModule(qualified_name, module):\n        if \".\" in qualified_name:\n            parts = qualified_name.split(\".\")\n            base = parts[0]\n            remaining_pieces = \".\".join(parts[1:])\n            module_value = getattr(module, base)\n            return lookupInModule(remaining_pieces, module_value)\n        else:\n            return getattr(module, qualified_name)\n\n    def parseNestedExpr(expr, module) -> Tuple[Any, int]:\n        i = 0\n        while i < len(expr) and expr[i] not in (\",\", \"[\", \"]\"):\n            i += 1\n\n        # Special case logic for the empty Tuple as a subscript (used\n        # in the type annotation `Tuple[()]`)\n        if expr[:i] == \"()\":\n            return (), i\n\n        base = lookupInModule(expr[:i].strip(), module)\n        assert base is not None, f\"Unresolvable type {expr[:i]}\"\n        if i == len(expr) or expr[i] != \"[\":\n            return base, i\n\n        assert expr[i] == \"[\"\n        parts = []\n        while expr[i] != \"]\":\n            part_len = 0\n            i += 1\n            part, part_len = parseNestedExpr(expr[i:], module)\n            parts.append(part)\n            i += part_len\n        if len(parts) > 1:\n            return base[tuple(parts)], i + 1\n        else:\n            return base[parts[0]], i + 1\n\n    def parseExpr(expr, module):\n        try:\n            value, len_parsed = parseNestedExpr(expr, module)\n            assert len_parsed == len(\n                expr\n            ), \"whole expression was not parsed, falling back to c++ parser\"\n            return value\n        except Exception:\n            \"\"\"\n            The python resolver fails in several cases in known unit tests, and is intended\n            to fall back gracefully to the c++ resolver in general.  For example, python 2 style\n            annotations which are frequent in our unit tests often fail with types e.g. int not\n            resolvable from the calling frame.\n            \"\"\"\n            return None\n\n    return lambda expr: parseExpr(expr, lookup_base)\n\n\ndef createResolutionCallbackFromFrame(frames_up: int = 0):\n    \"\"\"\n    Creates a function which, given a string variable name,\n    returns the value of the variable in the scope of the caller of\n    the function which called createResolutionCallbackFromFrame (by default).\n\n    This is used to enable access in-scope Python variables inside\n    TorchScript fragments.\n\n    frames_up is number of additional frames to go up on the stack.\n    The default value is 0, which correspond to the frame of the caller\n    of createResolutionCallbackFromFrame. Also for example, if frames_up is set\n    to 1, then the frame of the caller's caller of createResolutionCallbackFromFrame\n    will be taken.\n\n    For example, the following program prints 2::\n\n        def bar():\n            cb = createResolutionCallbackFromFrame(1)\n            print(cb(\"foo\"))\n\n        def baz():\n            foo = 2\n            bar()\n\n        baz()\n    \"\"\"\n    frame = inspect.currentframe()\n    i = 0\n    while i < frames_up + 1:\n        assert frame is not None\n        frame = frame.f_back\n        i += 1\n\n    assert frame is not None\n    f_locals = frame.f_locals\n    f_globals = frame.f_globals\n\n    class env:\n        def __getattr__(self, key):\n            if key in f_locals:\n                return f_locals[key]\n            elif key in f_globals:\n                return f_globals[key]\n            elif key in dir(builtins):\n                return getattr(builtins, key)\n\n    return createResolutionCallbackFromEnv(env())\n\n\ndef get_closure(fn):\n    \"\"\"\n    Get a dictionary of closed over variables from a function\n    \"\"\"\n    captures = {}\n    captures.update(fn.__globals__)\n\n    for index, captured_name in enumerate(fn.__code__.co_freevars):\n        captures[captured_name] = fn.__closure__[index].cell_contents\n\n    return captures\n\n\n# [local resolution in python]\n# Depending on where a variable is defined, and where it is used, we may\n# or may not be able to recover its value when recursively compiling a\n# script function. Remember in the general case, a module or function is\n# first defined and then later scripted. This means we do not have a\n# chance to capture the active frames when the function is defined. Hence any\n# name resolution has to happen later on the created closure. The way\n# python captures type annotations restricts what we can recover. The\n# follow example illustrates the different cases:\n#\n#         class MyGlobalClass:\n#         ...\n#         def my_local_scope():\n#             @torch.jit.script\n#             class MyClass:\n#                 ...\n#             @torch.jit.script\n#             class MyClassUsedAsVar:\n#                 ...\n#             def eg(x: MyClass, y: MyGlobalClass):\n#                 a_local_capture : Foo\n#                 return MyClassUsedAsVar(x)\n#\n# MyGlobalClass is defined in the __globals__ dictionary of function\n# 'eg', so it is always recoverable. my_local_scope introduces a new local\n# variable scope in the function. Classes defined here are only visible as\n# local variables. For the case of MyClassUsedAsVar, it is captured\n# because it is used as a variable inside the body of the function, and we\n# can resolve it using the captures returned from `get_closure`. However,\n# the type annotations are not captured by the closure. In Python\n# 3.0--3.9, the _value_ of MyClass and MyGlobalClass will be available as\n# annotations on `eg``, but starting in Python 4.0, they will represented as\n# strings and no longer present. Furthermore, since the body of `eg` does\n# not reference those names, they do not appear in the list of closed over\n# variables. In Python 2.x, type annotations are in comments, leading to a\n# similar situation where their definitions are not available. We anticipate\n# that most users will not run into this issue because their modules and\n# functions will be defined at a global scope like MyGlobalClass. In cases\n# where they are not, it is possible to work around issues by declaring the\n# values global in the function.\n# In Python 3.9 declaring class as global will make it invisible to\n# `inspect.getsource`, see https://bugs.python.org/issue42666 .\n# This could be worked around by manualy adding it to `global()` dictionary.\n\n\ndef createResolutionCallbackFromClosure(fn):\n    \"\"\"\n    Create a resolutionCallback by introspecting the function instead of\n    looking up the stack for the enclosing scope\n    \"\"\"\n    closure = get_closure(fn)\n\n    class closure_lookup:\n        # This is a class since `closure` is a dict and it's easier in\n        # `env_helper` if everything just works with `getattr` calls\n        def __getattr__(self, key):\n            if key in closure:\n                return closure[key]\n            elif hasattr(typing, key):\n                return getattr(typing, key)\n            elif hasattr(builtins, key):\n                return getattr(builtins, key)\n            return None\n\n    return createResolutionCallbackFromEnv(closure_lookup())\n\n\ndef can_compile_class(cls) -> bool:\n    # If any of the functions on a type don't have a code object, this type can't\n    # be compiled and is probably a builtin / bound from C\n    if is_ignored_fn(cls):\n        return False\n\n    # Ignore the following list of built-in classes.\n    ignored_builtin_classes = (torch.nn.Module, tuple, list, Exception)\n    if issubclass(cls, ignored_builtin_classes):\n        return False\n\n    names = cls.__dict__\n    fns = [\n        getattr(cls, name)\n        for name in names\n        if inspect.isroutine(getattr(cls, name, None))\n    ]\n    has_code = [hasattr(fn, \"__code__\") for fn in fns]\n    return all(has_code)\n\n\ndef get_callable_argument_names(fn) -> List[str]:\n    \"\"\"\n    Gets names of all POSITIONAL_OR_KEYWORD arguments for callable `fn`.\n    Returns an empty list when other types of arguments are present.\n\n    This is used by `torch.jit.trace` to assign meaningful argument names to\n    traced functions and modules.\n\n    Args:\n        fn: A callable.\n    Returns:\n        Argument names: List[str]\n    \"\"\"\n    # inspect.signature may fail, give up in that case.\n    try:\n        callable_signature = inspect.signature(fn)\n    except Exception:\n        return []\n\n    argument_names = []\n    for name, param in callable_signature.parameters.items():\n        # All four other types of arguments do not map to individual values\n        # with a keyword as name.\n        if not param.kind == param.POSITIONAL_OR_KEYWORD:\n            continue\n\n        argument_names.append(name)\n\n    return argument_names\n\n\ndef get_annotation_str(annotation):\n    \"\"\"\n    Convert an AST node containing a type annotation to the string present in the source\n    that represents the same annotation.\n    \"\"\"\n    if isinstance(annotation, ast.Name):\n        return annotation.id\n    elif isinstance(annotation, ast.Attribute):\n        return \".\".join([get_annotation_str(annotation.value), annotation.attr])\n    elif isinstance(annotation, ast.Subscript):\n        # In Python3.9+ subscript indicies are not wrapped in ast.Index\n        subscript_slice = annotation.slice if IS_PY39_PLUS else annotation.slice.value  # type: ignore[attr-defined]\n        return f\"{get_annotation_str(annotation.value)}[{get_annotation_str(subscript_slice)}]\"\n    elif isinstance(annotation, ast.Tuple):\n        return \",\".join([get_annotation_str(elt) for elt in annotation.elts])\n    elif isinstance(annotation, (ast.Constant, ast.NameConstant)):\n        return f\"{annotation.value}\"\n\n    # If an AST node is not handled here, it's probably handled in ScriptTypeParser.\n    return None\n\n\ndef get_type_hint_captures(fn):\n    \"\"\"\n    Get a dictionary containing type resolution mappings necessary to resolve types\n    for the literal annotations on 'fn'. These are not considered to be closed-over by fn\n    and must be obtained separately (e.g. using this function).\n\n    Args:\n        fn: A callable.\n    Returns:\n        A Dict[str, Any] containing a mapping from the literal annotations used on\n        fn to the Python objects they refer to.\n    \"\"\"\n    # First, try to get the source of the function. We'll need to parse it to find the actual string names\n    # that were used to annotate the types, since inspect.signature() will only return the class object that\n    # the annotation refers to, not the string name. If we can't get the source, simply return an empty dict.\n    # This may happen in cases where the function is synthesized dynamically at runtime.\n    src = loader.get_source(fn)\n    if src is None:\n        src = inspect.getsource(fn)\n\n    # Gather a dictionary of parameter name -> type, skipping any parameters whose annotated\n    # types are strings. These are only understood by TorchScript in the context of a type annotation\n    # that refers to a class in its own definition, but trying to include a mapping for this in the result\n    # function would cause infinite recursion because the class is currently being compiled.\n    # In addition, there is logic in ScriptTypeParser to handle this.\n    signature = inspect.signature(fn)\n    name_to_type = {\n        name: parameter.annotation\n        for name, parameter in signature.parameters.items()\n        if parameter.annotation is not inspect.Parameter.empty\n        and not isinstance(parameter.annotation, str)\n    }\n\n    # Then, get the literal type annotations from the function declaration\n    # by source inspection. This accounts for the case in which aliases are used\n    # to annotate the arguments (e.g device_t = torch.device, and then d: device_t).\n    # frontend.py cannot be used here because it includes _jit_internal, so use ast instead.\n    a = ast.parse(dedent(src))\n    if len(a.body) != 1 or not isinstance(a.body[0], ast.FunctionDef):\n        raise RuntimeError(f\"Expected {fn} to be a function\")\n    f = a.body[0]\n\n    # Prepare a dictionary of source annotation -> type, which will be the final result of this function,\n    # by using the parsed AST (f) to reconstruct source annotations as strings for each parameter and mapping\n    # them to the type object corresponding to the annotation via name_to_type using the parameter name.\n    annotation_to_type = {}\n\n    for arg in f.args.args:\n        # Get the source type annotation string for this argument if possible.\n        arg_annotation_str = (\n            get_annotation_str(arg.annotation) if arg.annotation else None\n        )\n\n        # If the argument has no annotation or get_annotation_str cannot convert it to a string,\n        # arg_annotation_str will be None. Skip this arg; ScriptTypeParser will probably handle\n        # this in the latter case.\n        if arg_annotation_str is None:\n            continue\n\n        # Insert {arg_annotation_str: type} into annotation_to_type if possible. One reason arg_name may not\n        # be present in name_to_type is that the annotation itself is a string and not a type object\n        # (common for self-refential annotations in classes). Once again, let ScriptTypeParser handle this.\n        arg_name = arg.arg\n        if arg_name in name_to_type:\n            annotation_to_type[arg_annotation_str] = name_to_type[arg_name]\n\n    # If there is a valid return annotation, include it in annotation_to_type. As with argument annotations,\n    # the literal annotation has to be convertible to a string by get_annotation_str, and the actual type\n    # of the annotation cannot be a string.\n    literal_return_annotation = get_annotation_str(f.returns)\n    valid_literal_annotation = literal_return_annotation is not None\n    return_annotation = signature.return_annotation\n    valid_return_annotation_type = (\n        return_annotation is not inspect.Parameter.empty\n        and not isinstance(return_annotation, str)\n    )\n    if valid_literal_annotation and valid_return_annotation_type:\n        annotation_to_type[literal_return_annotation] = return_annotation\n\n    return annotation_to_type\n\n\ndef createResolutionCallbackForClassMethods(cls):\n    \"\"\"\n    This looks at all the methods defined in a class and pulls their closed-over\n    variables into a dictionary and uses that to resolve variables.\n    \"\"\"\n    # cls is a type here, so `ismethod` is false since the methods on the type\n    # aren't bound to anything, so Python treats them as regular functions\n    fns = [\n        getattr(cls, name)\n        for name in cls.__dict__\n        if inspect.isroutine(getattr(cls, name))\n    ]\n    # Skip built-ins, as they do not have global scope nor type hints\n    # Needed to support `enum.Enum` derived classes in Python-3.11\n    # That adds `_new_member_` property which is an alias to `__new__`\n    fns = [fn for fn in fns if not inspect.isbuiltin(fn) and hasattr(fn, \"__globals__\")]\n    captures = {}\n\n    for fn in fns:\n        captures.update(get_closure(fn))\n        captures.update(get_type_hint_captures(fn))\n\n    def lookup_in_class(key):\n        if key in captures:\n            return captures[key]\n        else:\n            return getattr(builtins, key, None)\n\n    return lookup_in_class\n\n\ndef boolean_dispatch(\n    arg_name, arg_index, default, if_true, if_false, module_name, func_name\n):\n    \"\"\"\n    Dispatches to either of 2 script functions based on a boolean argument.\n    In TorchScript, the boolean argument must be constant so that the correct\n    function to use can be determined at compile time.\n    \"\"\"\n\n    def fn(*args, **kwargs):\n        dispatch_flag = default\n        if arg_name in kwargs:\n            dispatch_flag = kwargs[arg_name]\n        elif arg_index < len(args):\n            dispatch_flag = args[arg_index]\n\n        if dispatch_flag:\n            return if_true(*args, **kwargs)\n        else:\n            return if_false(*args, **kwargs)\n\n    if if_true.__doc__ is None and if_false.__doc__ is not None:\n        doc = if_false.__doc__\n        if_true.__doc__ = doc\n    elif if_false.__doc__ is None and if_true.__doc__ is not None:\n        doc = if_true.__doc__\n        if_false.__doc__ = doc\n    elif if_false.__doc__ is None and if_true.__doc__ is None:\n        # neither function has a docstring\n        doc = None\n    else:\n        raise RuntimeError(\"only one function can have a docstring\")\n    fn.__doc__ = doc\n\n    if module_name is not None:\n        fn.__module__ = module_name\n    if func_name is not None:\n        fn.__name__ = func_name\n\n    boolean_dispatched[fn] = {\n        \"if_true\": if_true,\n        \"if_false\": if_false,\n        \"index\": arg_index,\n        \"default\": default,\n        \"arg_name\": arg_name,\n    }\n    return fn\n\n\nclass FunctionModifiers:\n    \"\"\"\n    Used to denote the behavior of a function in TorchScript. See export() and\n    ignore() for details.\n    \"\"\"\n\n    UNUSED = \"unused (ignored and replaced with raising of an exception)\"\n    IGNORE = \"ignore (leave as a call to Python, cannot be torch.jit.save'd)\"\n    EXPORT = \"export (compile this function even if nothing calls it)\"\n    DEFAULT = \"default (compile if called from a exported function / forward)\"\n    COPY_TO_SCRIPT_WRAPPER = (\n        \"if this method is not scripted, copy the python method onto the scripted model\"\n    )\n    _DROP = \"_drop (function is fully ignored, declaration can be unscriptable)\"\n\n\ndef export(fn):\n    \"\"\"\n    This decorator indicates that a method on an ``nn.Module`` is used as an entry point into a\n    :class:`ScriptModule` and should be compiled.\n\n    ``forward`` implicitly is assumed to be an entry point, so it does not need this decorator.\n    Functions and methods called from ``forward`` are compiled as they are seen\n    by the compiler, so they do not need this decorator either.\n\n    Example (using ``@torch.jit.export`` on a method):\n\n    .. testcode::\n\n        import torch\n        import torch.nn as nn\n\n        class MyModule(nn.Module):\n            def implicitly_compiled_method(self, x):\n                return x + 99\n\n            # `forward` is implicitly decorated with `@torch.jit.export`,\n            # so adding it here would have no effect\n            def forward(self, x):\n                return x + 10\n\n            @torch.jit.export\n            def another_forward(self, x):\n                # When the compiler sees this call, it will compile\n                # `implicitly_compiled_method`\n                return self.implicitly_compiled_method(x)\n\n            def unused_method(self, x):\n                return x - 20\n\n        # `m` will contain compiled methods:\n        #     `forward`\n        #     `another_forward`\n        #     `implicitly_compiled_method`\n        # `unused_method` will not be compiled since it was not called from\n        # any compiled methods and wasn't decorated with `@torch.jit.export`\n        m = torch.jit.script(MyModule())\n    \"\"\"\n    fn._torchscript_modifier = FunctionModifiers.EXPORT\n    return fn\n\n\ndef unused(fn):\n    \"\"\"\n    This decorator indicates to the compiler that a function or method should\n    be ignored and replaced with the raising of an exception. This allows you\n    to leave code in your model that is not yet TorchScript compatible and still\n    export your model.\n\n        Example (using ``@torch.jit.unused`` on a method)::\n\n            import torch\n            import torch.nn as nn\n\n            class MyModule(nn.Module):\n                def __init__(self, use_memory_efficient):\n                    super().__init__()\n                    self.use_memory_efficient = use_memory_efficient\n\n                @torch.jit.unused\n                def memory_efficient(self, x):\n                    import pdb\n                    pdb.set_trace()\n                    return x + 10\n\n                def forward(self, x):\n                    # Use not-yet-scriptable memory efficient mode\n                    if self.use_memory_efficient:\n                        return self.memory_efficient(x)\n                    else:\n                        return x + 10\n\n            m = torch.jit.script(MyModule(use_memory_efficient=False))\n            m.save(\"m.pt\")\n\n            m = torch.jit.script(MyModule(use_memory_efficient=True))\n            # exception raised\n            m(torch.rand(100))\n    \"\"\"\n    if isinstance(fn, property):\n        prop = fn\n        setattr(  # noqa: B010\n            prop.fget, \"_torchscript_modifier\", FunctionModifiers.UNUSED\n        )\n\n        if prop.fset:\n            setattr(  # noqa: B010\n                prop.fset, \"_torchscript_modifier\", FunctionModifiers.UNUSED\n            )\n\n        return prop\n\n    fn._torchscript_modifier = FunctionModifiers.UNUSED\n    return fn\n\n\n# No op context manager from python side\nclass _IgnoreContextManager(contextlib.AbstractContextManager):\n    def __init__(self, **kwargs):\n        pass\n\n    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n        pass\n\n\ndef ignore(drop=False, **kwargs):\n    \"\"\"\n    This decorator indicates to the compiler that a function or method should\n    be ignored and left as a Python function. This allows you to leave code in\n    your model that is not yet TorchScript compatible. If called from TorchScript,\n    ignored functions will dispatch the call to the Python interpreter. Models with ignored\n    functions cannot be exported; use :func:`@torch.jit.unused <torch.jit.unused>` instead.\n\n    Example (using ``@torch.jit.ignore`` on a method)::\n\n        import torch\n        import torch.nn as nn\n\n        class MyModule(nn.Module):\n            @torch.jit.ignore\n            def debugger(self, x):\n                import pdb\n                pdb.set_trace()\n\n            def forward(self, x):\n                x += 10\n                # The compiler would normally try to compile `debugger`,\n                # but since it is `@ignore`d, it will be left as a call\n                # to Python\n                self.debugger(x)\n                return x\n\n        m = torch.jit.script(MyModule())\n\n        # Error! The call `debugger` cannot be saved since it calls into Python\n        m.save(\"m.pt\")\n\n    Example (using ``@torch.jit.ignore(drop=True)`` on a method):\n\n    .. testcode::\n\n        import torch\n        import torch.nn as nn\n\n        class MyModule(nn.Module):\n            @torch.jit.ignore(drop=True)\n            def training_method(self, x):\n                import pdb\n                pdb.set_trace()\n\n            def forward(self, x):\n                if self.training:\n                    self.training_method(x)\n                return x\n\n        m = torch.jit.script(MyModule())\n\n        # This is OK since `training_method` is not saved, the call is replaced\n        # with a `raise`.\n        m.save(\"m.pt\")\n\n    .. testcleanup::\n\n        import os\n        os.remove('m.pt')\n    \"\"\"\n\n    if callable(drop):\n        # used without any args, so drop is actually a function\n        #   @torch.jit.ignore\n        #   def fn(...):\n        fn = drop\n        fn._torchscript_modifier = FunctionModifiers.IGNORE\n        return fn\n\n    if not isinstance(drop, bool):\n        raise RuntimeError(\n            \"Argument to @torch.jit.ignore must be a bool or \"\n            f\"a function but got {drop}\"\n        )\n\n    # for backwards compat\n    drop_on_export = kwargs.pop(\"drop_on_export\", None)\n    if drop_on_export:\n        warnings.warn(\n            \"ignore(drop_on_export=True) has been deprecated. TorchScript will now drop the function \"\n            \"call on compilation. Use torch.jit.unused now. {}\",\n            category=FutureWarning,\n        )\n\n        drop = drop_on_export\n    elif drop:\n        warnings.warn(\n            \"ignore(True) has been deprecated. TorchScript will now drop the function \"\n            \"call on compilation. Use torch.jit.unused now. {}\",\n            category=FutureWarning,\n        )\n\n    def decorator(fn):\n        if drop:\n            fn._torchscript_modifier = FunctionModifiers.UNUSED\n        else:\n            fn._torchscript_modifier = FunctionModifiers.IGNORE\n        return fn\n\n    return decorator\n\n\ndef _drop(fn):\n    fn._torchscript_modifier = FunctionModifiers._DROP\n    return fn\n\n\ndef _copy_to_script_wrapper(fn):\n    fn._torchscript_modifier = FunctionModifiers.COPY_TO_SCRIPT_WRAPPER\n    return fn\n\n\ndef module_has_exports(mod):\n    for name in dir(mod):\n        if hasattr(mod, name):\n            item = getattr(mod, name)\n            if callable(item):\n                if get_torchscript_modifier(item) is FunctionModifiers.EXPORT:\n                    return True\n    return False\n\n\n# WARNING: should_drop is currently being used by our JIT code coverage plug-in to mark JIT'd code as covered. If you\n# rename this function, please update references in tools/coverage_plugins_package/src/coverage_plugins/jit_plugin.py to\n# allow JIT'd code to still be covered.\ndef should_drop(fn) -> bool:\n    attr = get_torchscript_modifier(fn)\n    if attr is None:\n        return False\n    return attr is FunctionModifiers.UNUSED or attr is FunctionModifiers._DROP\n\n\ndef is_ignored_fn(fn) -> bool:\n    mod = get_torchscript_modifier(fn)\n    return (\n        mod is FunctionModifiers.UNUSED\n        or mod is FunctionModifiers.IGNORE\n        or mod is FunctionModifiers._DROP\n    )\n\n\ndef _is_drop_fn(fn) -> bool:\n    mod = get_torchscript_modifier(fn)\n    return mod is FunctionModifiers._DROP\n\n\ndef is_static_fn(cls, fn) -> bool:\n    return isinstance(inspect.getattr_static(cls, fn, default=None), staticmethod)\n\n\ndef get_static_fn(cls, fn):\n    return inspect.getattr_static(cls, fn).__func__\n\n\ndef get_torchscript_modifier(fn):\n    if not callable(fn):\n        return None\n    if hasattr(fn, \"__func__\"):\n        fn = fn.__func__\n    return getattr(fn, \"_torchscript_modifier\", FunctionModifiers.DEFAULT)\n\n\ndef copy_torchscript_modifier(orig, new) -> None:\n    attr = get_torchscript_modifier(orig)\n    if attr is None:\n        return\n    new._torchscript_modifier = attr\n\n\n# overloading registration\n# overloads get registered in this file, and compiled in torch/jit/__init__.py\n# so that they can be imported in nn/functional.py without an import cycle\n\n# qualified_name => list[overload_functions]\n_overloaded_fns: Dict[str, List[Callable]] = {}  # noqa: T484\n\n\n_OVERLOAD_EXAMPLE = \"\"\"\nExample usage of overload function:\n@torch.jit._overload\ndef my_function(x: type0) -> type0: # decl 1\n    pass\n\n@torch.jit._overload\ndef my_function(x: type1) -> type1: # decl 2\n    pass\n\ndef my_function(x):                 # implementation\n    if isinstance(x, type0):\n        return x\n    elif isinstance(x, type1):\n        return x\n\"\"\"\n\n\ndef get_overload_no_implementation_error_message(kind, obj):\n    sourcelines, file_lineno, filename = get_source_lines_and_file(obj)\n    return (\n        f'Implementation for the {kind} \"{_qualified_name(obj)}\" is missing. Please make '\n        f\"sure a definition is provided and defined after all overload declarations.\\n\"\n        f'File \"{filename}\", line {file_lineno}:\\n'\n        + \"\".join(sourcelines)\n        + \"\\n\"\n        + _OVERLOAD_EXAMPLE\n    )\n\n\ndef _check_overload_body(func):\n    try:\n        parsed_def = parse_def(func)\n    except OSError as e:\n        # Parsing the function definition can raise an OSError if source is unavailable.\n        # Since this is just an initial check, just raise a warning if this is the case.\n        warnings.warn(\n            f\"Unable to retrieve source for @torch.jit._overload function: {func}.\"\n        )\n        return\n\n    body = parsed_def.ast.body[0].body\n\n    def is_pass(x):\n        return isinstance(x, ast.Pass)\n\n    def is_ellipsis(x):\n        return isinstance(x, ast.Expr) and isinstance(x.value, ast.Ellipsis)\n\n    if len(body) != 1 or not (is_pass(body[0]) or is_ellipsis(body[0])):\n        msg = (\n            \"Only `pass` statement or `...` can be the body of overload declaration:\\n\"\n        )\n        msg += \"\\n\".join(parsed_def.source.split(\"\\n\")[:3])\n        msg += \" <- Expecting `pass` or `...` here!\\n\" + _OVERLOAD_EXAMPLE\n        raise RuntimeError(msg)\n\n\ndef _overload(func):\n    _check_overload_body(func)\n    qual_name = _qualified_name(func)\n    global _overloaded_fns\n    fn_overload_list = _overloaded_fns.get(qual_name)\n    if fn_overload_list is None:\n        fn_overload_list = []\n        _overloaded_fns[qual_name] = fn_overload_list\n    fn_overload_list.append(func)\n    return func\n\n\ndef _get_fn_overloads(qual_name):\n    return _overloaded_fns.get(qual_name)\n\n\ndef _clear_fn_overloads(qual_name) -> None:\n    del _overloaded_fns[qual_name]\n\n\ndef get_class_name_lineno(method) -> Tuple[str, int]:\n    current_frame = inspect.currentframe()\n\n    # one for the get_class_name call, one for _overload_method call\n    for i in range(2):\n        assert (\n            current_frame is not None\n        )  # assert current frame is not an Optional[FrameType]\n        current_frame = current_frame.f_back\n\n    assert current_frame is not None  # same here\n    class_name = current_frame.f_code.co_name\n    line_no = current_frame.f_code.co_firstlineno\n    return class_name, line_no\n\n\n# At the point the decorator is applied to class methods the method\n# has no reference to its owning class. _qualified_name would not include\n# the class it is defined in, so any methods with the same name in the same file\n# would have the same _qualified_name, even if they were defined in different\n# classes. This problem only exists in python 2.\n# We get around this problem by looking at the stack frame and identifying\n# the class name, and throwing an error whenever overloads are used\n# when modules of the same name are in the same file\n\n# qualified_name => class name => list[overload_functions]\n_overloaded_methods: Dict[str, Dict[str, List[Callable]]] = {}  # noqa: T484\n\n\n# (qualified_name, class name) => class_fileno\n_overloaded_method_class_fileno = {}\n\n\ndef _overload_method(func):\n    _check_overload_body(func)\n    qual_name = _qualified_name(func)\n    global _overloaded_methods\n    class_name_map = _overloaded_methods.get(qual_name, None)\n    if class_name_map is None:\n        class_name_map = {}\n        _overloaded_methods[qual_name] = class_name_map\n\n    class_name, line_no = get_class_name_lineno(func)\n    method_overloads = class_name_map.get(class_name, None)\n    if method_overloads is None:\n        method_overloads = []\n        class_name_map[class_name] = method_overloads\n        _overloaded_method_class_fileno[(qual_name, class_name)] = line_no\n    else:\n        existing_lineno = _overloaded_method_class_fileno[(qual_name, class_name)]\n        if existing_lineno != line_no:\n            raise RuntimeError(\n                \"Cannot currently overload the same method name in two different\"\n                \" classes with the same name in the same module\"\n            )\n\n    method_overloads.append(func)\n    return func\n\n\ndef _get_overloaded_methods(method, mod_class):\n    # TODO: __name__ not set for submodules in recursive script\n    if not hasattr(method, \"__name__\"):\n        return None\n    qual_name = _qualified_name(method)\n    class_name_map = _overloaded_methods.get(qual_name, None)\n    if class_name_map is None:\n        return None\n    overloads = class_name_map.get(mod_class.__name__, None)\n    if overloads is None:\n        return None\n\n    method_line_no = get_source_lines_and_file(method)[1]\n    mod_class_fileno = get_source_lines_and_file(mod_class)[1]\n    mod_end_fileno = mod_class_fileno + len(get_source_lines_and_file(mod_class)[0])\n    if not (method_line_no >= mod_class_fileno and method_line_no <= mod_end_fileno):\n        raise Exception(\n            \"Overloads are not useable when a module is redeclared within the same file: \"\n            + str(method)\n        )\n    return overloads\n\n\ndef is_tuple(ann) -> bool:\n    if ann is Tuple:\n        raise_error_container_parameter_missing(\"Tuple\")\n\n    # For some reason Python 3.7 violates the Type[A, B].__origin__ == Type rule\n    if not hasattr(ann, \"__module__\"):\n        return False\n\n    ann_origin = get_origin(ann)\n    if IS_PY39_PLUS and ann.__module__ == \"builtins\" and ann_origin is tuple:\n        return True\n    return ann.__module__ == \"typing\" and (ann_origin is Tuple or ann_origin is tuple)\n\n\ndef is_list(ann) -> bool:\n    if ann is List:\n        raise_error_container_parameter_missing(\"List\")\n\n    if not hasattr(ann, \"__module__\"):\n        return False\n\n    ann_origin = get_origin(ann)\n    if IS_PY39_PLUS and ann.__module__ == \"builtins\" and ann_origin is list:\n        return True\n    return ann.__module__ == \"typing\" and (ann_origin is List or ann_origin is list)\n\n\ndef is_dict(ann) -> bool:\n    if ann is Dict:\n        raise_error_container_parameter_missing(\"Dict\")\n\n    if not hasattr(ann, \"__module__\"):\n        return False\n\n    ann_origin = get_origin(ann)\n    if IS_PY39_PLUS and ann.__module__ == \"builtins\" and ann_origin is dict:\n        return True\n    return ann.__module__ == \"typing\" and (ann_origin is Dict or ann_origin is dict)\n\n\ndef is_union(ann):\n    if ann is Union:\n        raise_error_container_parameter_missing(\"Union\")\n\n    return isinstance(ann, BuiltinUnionType) or (\n        hasattr(ann, \"__module__\")\n        and ann.__module__ == \"typing\"\n        and (get_origin(ann) is Union)\n    )\n\n\ndef is_optional(ann):\n    if ann is Optional:\n        raise_error_container_parameter_missing(\"Optional\")\n\n    def is_optional_as_optional(ann):\n        return (\n            hasattr(ann, \"__module__\")\n            and ann.__module__ == \"typing\"\n            and (get_origin(ann) is Optional)\n        )\n\n    def is_union_as_optional(ann):\n        ann_args = get_args(ann)\n        return len(ann_args) == 2 and (None in ann_args or type(None) in ann_args)\n\n    return is_optional_as_optional(ann) or (is_union(ann) and is_union_as_optional(ann))\n\n\ndef is_future(ann) -> bool:\n    if ann is Future:\n        raise RuntimeError(\n            \"Attempted to use Future without a \"\n            \"contained type. Please add a contained type, e.g. \"\n            \"Future[int]\"\n        )\n    return get_origin(ann) is Future\n\n\ndef is_await(ann) -> bool:\n    if ann is _Await:\n        return True\n    return get_origin(ann) is _Await\n\n\nif torch.distributed.rpc.is_available():\n    from torch._C._distributed_rpc import PyRRef\n    from torch.distributed.rpc import RRef\n\n    def is_rref(ann) -> bool:\n        if ann is RRef:\n            raise RuntimeError(\n                \"Attempted to use RRef without a \"\n                \"contained type. Please add a contained type, e.g. \"\n                \"RRef[int]\"\n            )\n        return get_origin(ann) is RRef\n\n    def is_rref_instance(obj) -> bool:\n        return isinstance(obj, PyRRef)\n\nelse:\n\n    def is_rref_instance(obj) -> bool:\n        # If the RPC module doesn't exist then RRefs don't exist either.\n        return False\n\n\ndef is_final(ann) -> bool:\n    return ann.__module__ in {\"typing\", \"typing_extensions\"} and (\n        get_origin(ann) is Final or isinstance(ann, type(Final))\n    )\n\n\n# allows BroadcastingList instance to be subscriptable\nclass BroadcastingListCls:\n    def __getitem__(self, types):\n        return\n\n\n# mypy doesn't support parameters on types, so we have to explicitly type each\n# list size\nBroadcastingList1 = BroadcastingListCls()\nfor i in range(2, 7):\n    globals()[f\"BroadcastingList{i}\"] = BroadcastingList1\n\n\ndef is_scripting() -> bool:\n    r\"\"\"\n    Function that returns True when in compilation and False otherwise. This\n    is useful especially with the @unused decorator to leave code in your\n    model that is not yet TorchScript compatible.\n    .. testcode::\n\n        import torch\n\n        @torch.jit.unused\n        def unsupported_linear_op(x):\n            return x\n\n        def linear(x):\n           if torch.jit.is_scripting():\n              return torch.linear(x)\n           else:\n              return unsupported_linear_op(x)\n    \"\"\"\n    return False\n\n\n# Retrieves a fully-qualified name (module hierarchy + classname) for a given obj.\ndef _qualified_name(obj, mangle_name=True) -> str:\n    # This special case allows us to override the qualified name on a type.\n    # It's currently used in conjunction with tracing, where we create a\n    # fake module to filter only supported attributes. However, since this\n    # new type is defined as a local class, we need a mechanism to override\n    # its qualname so it appears correctly in the TorchScript system. This,\n    # we set '_jit_override_qualname' with the original traced module's\n    # qualified name, which is picked up here\n    if hasattr(obj, \"_jit_override_qualname\"):\n        return obj._jit_override_qualname\n    # short-circuit in cases where the object already has a known qualified name\n    if isinstance(obj, torch._C.ScriptFunction):\n        return obj.qualified_name\n\n    if getattr(obj, \"__name__\", None):\n        name = obj.__name__\n    # Enum classes do not have `__name__` attr, instead they have `name`.\n    elif isinstance(obj, enum.Enum):\n        name = obj.name\n    else:\n        raise RuntimeError(\"Could not get name of python class object\")\n\n    if name == \"<lambda>\":\n        name = \"_lambda\"  # make name a valid identifier\n\n    module_name = obj.__module__\n\n    # If the module is actually a torchbind module, then we should short circuit\n    if module_name == \"torch._classes\":\n        return obj.qualified_name\n\n    # The Python docs are very clear that `__module__` can be None, but I can't\n    # figure out when it actually would be.\n    if module_name is None:\n        raise RuntimeError(\n            f\"Could not get qualified name for class '{name}': \"\n            \"__module__ can't be None.\"\n        )\n\n    # if getattr(sys.modules[module_name], name) is not obj:\n    #     raise RuntimeError(f\"Could not get qualified name for class '{name}': \"\n    #                        f\"the attr {name} on module {module_name} is not the class\")\n\n    # torch.package and TorchScript have separate mangling schemes to avoid\n    # name collisions from multiple packages. To avoid them interfering with\n    # each other, normalize the package manging here.\n    if package_mangling.is_mangled(module_name):\n        module_name = module_name.replace(\"<\", \"_\")\n        module_name = module_name.replace(\">\", \"_\")\n\n    # The PythonExceptionValue C++ class in torch/csrc/jit/python/python_sugared_value.h\n    # does not need mangle the python class name.\n    if mangle_name:\n        # __main__ is a builtin module, so rewrite it to \"__torch__\".\n        if module_name == \"__main__\":\n            module_name = \"__torch__\"\n        else:\n            # Everything else gets a \"__torch__\" prefix to avoid name collisions\n            # with the names of user values.\n            module_name = \"__torch__.\" + module_name\n\n    if \".\" in name:\n        raise RuntimeError(\n            f\"Could not get qualified name for class '{name}': \"\n            f\"'{name}' is not a valid identifier\"\n        )\n\n    return module_name + \".\" + name\n\n\ndef _try_get_dispatched_fn(fn):\n    if not callable(fn):\n        return None\n    return boolean_dispatched.get(fn)\n\n\ndef _get_named_tuple_properties(\n    obj, loc: Optional[torch._C._jit_tree_views.SourceRange] = None, rcb=None\n):\n    if loc is None:\n        loc = fake_range()\n\n    assert issubclass(obj, tuple) and hasattr(obj, \"_fields\")\n    if hasattr(obj, \"_field_defaults\"):\n        defaults = [\n            obj._field_defaults[field]\n            for field in obj._fields\n            if field in obj._field_defaults\n        ]\n    else:\n        defaults = []\n    # In 3.10 recommended way to get annotations is to call `inspect.get_annotations` function\n    # Also, annotations from base class are not inherited so they need to be queried explicitly\n    if sys.version_info[:2] < (3, 10):\n        obj_annotations = getattr(obj, \"__annotations__\", {})\n    else:\n        obj_annotations = inspect.get_annotations(obj)\n        if len(obj_annotations) == 0 and hasattr(obj, \"__base__\"):\n            obj_annotations = inspect.get_annotations(obj.__base__)\n\n    annotations = []\n    for field in obj._fields:\n        if field in obj_annotations:\n            field_type = obj_annotations[field]\n            # [Note: ForwardRef annotations in NamedTuple attributes]\n            # NamedTuple types are slightly different from normal types.\n            #\n            # Normally, annotations are evaluted like this (during jit.script):\n            # 1. Load strings of python code into c++ and parse.\n            # 2. Get annotations as strings\n            # 3. Use the PythonResolver's resolution callback (rcb) to convert\n            #    the string into a python object\n            # 4. We call into annotations.py:ann_to_type to convert python obj\n            #    from step 3 into a type that torchscript understands.\n            #\n            # NamedTuples are more complicated, because it has sub-types.\n            # Normally, once we have the NamedTuple type object from #3,\n            # we can just look at the annotation literal values and use\n            # ann_to_type directly on them.\n            #\n            # But sometimes, users will annotate with string literals, e.g.\n            #    x: 'int'\n            # This also happens with PEP563 (from __forward__ import annotations)\n            #\n            # These annotations appear in the annotation dict as ForwardRef('int').\n            #\n            # Then, we need to convert the string into a python object. This\n            # requires having local context for custom objects or imported types.\n            # rcb() is what gives us this. So, we plumb rcb through the stack so\n            # it can be used in this context for the if block below.\n            #\n            # FAQ:\n            # - Why do we need this special handling for NamedTuple but string\n            #   annotations work fine for normal types? Normally, we parse the\n            #   string directly and then call rcb() directly from C++.\n            # - Why not use ForwardRef._evaluate? For that, we need globals()\n            #   and locals() for the local context where the NamedTuple was defined.\n            #   rcb is what lets us look up into these. So, basically rcb does the\n            #   hard work for us.\n            if isinstance(field_type, ForwardRef) and rcb is not None:\n                rcb_type = rcb(field_type.__forward_arg__)\n                # rcb returns None if it can't find anything.\n                if rcb_type is None:\n                    raise ValueError(\n                        f\"Unknown type annotation: '{field_type}' in NamedTuple {obj.__name__}.\"\n                        f\" Likely due to partial support for ForwardRef parameters in NamedTuples, see #95858.\"\n                        f\" Issue occurred at {loc.highlight()}\"\n                    )\n                field_type = rcb_type\n            the_type = torch.jit.annotations.ann_to_type(field_type, loc, rcb)\n            annotations.append(the_type)\n        else:\n            annotations.append(torch._C.TensorType.getInferred())\n    return type(obj).__name__, obj._fields, annotations, defaults\n\n\ndef _create_named_tuple(\n    t, unqual_name: str, field_names: List[str], defaults: Tuple[Any, ...]\n):\n    TupleType = collections.namedtuple(unqual_name, field_names, defaults=defaults)  # type: ignore[call-arg, no-redef, misc]\n    return TupleType(*t)\n\n\n@contextlib.contextmanager\ndef _disable_emit_hooks():\n    hooks = torch._C._jit_get_emit_hooks()\n    torch._C._jit_set_emit_hooks(None, None)\n    try:\n        yield\n    finally:\n        torch._C._jit_set_emit_hooks(hooks[0], hooks[1])\n\n\ndef _disable_emit_hooks_decorator(_DecoratorContextManager) -> None:  # noqa: F811\n    def __enter__(self) -> None:\n        self.hooks = torch._C._jit_get_emit_hooks()\n        torch._C._jit_set_emit_hooks(None, None)\n\n    def __exit__(self, *args) -> None:\n        torch._C._jit_set_emit_hooks(self.hooks[0], self.hooks[1])\n\n\ndef _is_exception(obj) -> bool:\n    if not inspect.isclass(obj):\n        return False\n    return issubclass(obj, Exception)\n\n\ndef raise_error_container_parameter_missing(target_type) -> None:\n    if target_type == \"Dict\":\n        raise RuntimeError(\n            \"Attempted to use Dict without \"\n            \"contained types. Please add contained type, e.g. \"\n            \"Dict[int, int]\"\n        )\n    raise RuntimeError(\n        f\"Attempted to use {target_type} without a \"\n        \"contained type. Please add a contained type, e.g. \"\n        f\"{target_type}[int]\"\n    )\n\n\ndef check_args_exist(target_type) -> None:\n    if target_type is List or target_type is list:\n        raise_error_container_parameter_missing(\"List\")\n    elif target_type is Tuple or target_type is tuple:\n        raise_error_container_parameter_missing(\"Tuple\")\n    elif target_type is Dict or target_type is dict:\n        raise_error_container_parameter_missing(\"Dict\")\n    elif target_type is None or target_type is Optional:\n        raise_error_container_parameter_missing(\"Optional\")\n\n\ndef check_empty_containers(obj) -> None:\n    if obj == [] or obj == {} or obj == ():\n        warnings.warn(\n            \"The inner type of a container is lost when \"\n            \"calling torch.jit.isinstance in eager mode. For \"\n            \"example, List[int] would become list and \"\n            \"therefore falsely return True for List[float] or\"\n            \" List[str].\"\n        )\n\n\n# supports List/Dict/Tuple and Optional types\n# TODO support future\ndef container_checker(obj, target_type) -> bool:\n    origin_type = get_origin(target_type)\n    check_args_exist(target_type)\n    if origin_type is None:\n        return False\n    elif origin_type is list or origin_type is List:\n        check_empty_containers(obj)\n        if not isinstance(obj, list):\n            return False\n        arg_type = get_args(target_type)[0]\n        arg_origin = get_origin(arg_type)\n        for el in obj:\n            # check if nested container, ex: List[List[str]]\n            if arg_origin:  # processes nested container, ex: List[List[str]]\n                if not container_checker(el, arg_type):\n                    return False\n            elif not isinstance(el, arg_type):\n                return False\n        return True\n    elif origin_type is Dict or origin_type is dict:\n        check_empty_containers(obj)\n        if not isinstance(obj, dict):\n            return False\n        key_type = get_args(target_type)[0]\n        val_type = get_args(target_type)[1]\n        for key, val in obj.items():\n            # check if keys are of right type\n            if not isinstance(key, key_type):\n                return False\n            val_origin = get_origin(val_type)\n            if val_origin:\n                if not container_checker(val, val_type):\n                    return False\n            elif not isinstance(val, val_type):\n                return False\n        return True\n    elif origin_type is Tuple or origin_type is tuple:\n        check_empty_containers(obj)\n        if not isinstance(obj, tuple):\n            return False\n        arg_types = get_args(target_type)\n        if len(obj) != len(arg_types):\n            return False\n        for el, el_type in zip(obj, arg_types):\n            el_origin = get_origin(el_type)\n            if el_origin:\n                if not container_checker(el, el_type):\n                    return False\n            elif not isinstance(el, el_type):\n                return False\n        return True\n    elif origin_type is Union or issubclass(\n        origin_type, BuiltinUnionType\n    ):  # also handles Optional\n        if obj is None:  # check before recursion because None is always fine\n            return True\n        inner_types = get_args(target_type)\n        for t in inner_types:\n            t_origin = get_origin(t)\n            if t_origin:\n                return container_checker(obj, t)\n            elif isinstance(obj, t):\n                return True\n    return False\n\n\ndef _isinstance(obj, target_type) -> bool:\n    if isinstance(target_type, collections.abc.Container):\n        if not isinstance(target_type, tuple):\n            raise RuntimeError(\n                \"The second argument to \"\n                \"`torch.jit.isinstance` must be a type \"\n                \"or a tuple of types\"\n            )\n        for t_type in target_type:\n            if _isinstance(obj, t_type):\n                return True\n        return False\n\n    origin_type = get_origin(target_type)\n    if origin_type:\n        return container_checker(obj, target_type)\n\n    # Check to handle non-typed optional origin returns as none instead\n    #    of as optional in 3.7-3.8\n    check_args_exist(target_type)\n\n    # handle non-containers\n    return isinstance(obj, target_type)\n\n\nclass _TensorExtractor(pickle.Pickler):\n    def __init__(self, *args, tensors: List[torch.Tensor], **kwargs):\n        super().__init__(*args, **kwargs)\n        self.tensors = tensors\n\n    def persistent_id(self, obj):\n        if isinstance(obj, torch.Tensor):\n            self.tensors.append(obj)\n            return \"\"\n        # Since we just want to extract tensors, we don't mind if an object is\n        # unpicklable if it doesn't contain tensors, as we can just ignore/skip\n        # it. To play it safe, we only do so for common objects that we're sure\n        # don't contain tensors. Feel free to add new types here. Note also that\n        # even if a type isn't listed here this won't block users, since thet\n        # can just add a __getstate__ or __reduce__ method to their class.\n        if isinstance(obj, LockType):\n            return \"\"\n        # Futures and RRefs don't technically contain a value, they just offer\n        # the means to access a value.\n        if isinstance(obj, CFuture) or is_rref_instance(obj):\n            return \"\"\n        if isinstance(obj, CAwait):\n            return \"\"\n        if isinstance(obj, torch.cuda.Event):\n            return \"\"\n        if isinstance(obj, threading.Thread):\n            return \"\"\n        return None\n\n\ndef _extract_tensors(obj):\n    r\"\"\"\n    This function is exclusively called from C++.\n    See ``torch/csrc/jit/python/python_ivalue.h``.\n\n    It extracts the tensors contained in the given object, through pickling.\n    \"\"\"\n    tensors: List[torch.Tensor] = []\n    extractor = _TensorExtractor(io.BytesIO(), protocol=-1, tensors=tensors)\n    extractor.dump(obj)\n    return tensors\n\n\n# In Python-3.11+ typed enums (i.e. IntEnum for example) retain number of base class methods in subclass\n# that were previously dropped. To preserve the behavior, explicitly drop them there\n\nif sys.version_info > (3, 10):\n    _drop(enum.Enum.__new__)\n    _drop(enum.Enum.__format__)\n    _drop(enum.Enum.__repr__)\n    _drop(enum.Enum.__str__)\n", 1510], "C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\autograd\\grad_mode.py": ["from typing import Any, Optional\n\nimport torch\n\nfrom torch.utils._contextlib import (\n    _DecoratorContextManager,\n    _NoParamDecoratorContextManager,\n    F,\n)\n\n__all__ = [\n    \"no_grad\",\n    \"enable_grad\",\n    \"set_grad_enabled\",\n    \"inference_mode\",\n    \"set_multithreading_enabled\",\n]\n\n\nclass no_grad(_NoParamDecoratorContextManager):\n    r\"\"\"Context-manager that disables gradient calculation.\n\n    Disabling gradient calculation is useful for inference, when you are sure\n    that you will not call :meth:`Tensor.backward()`. It will reduce memory\n    consumption for computations that would otherwise have `requires_grad=True`.\n\n    In this mode, the result of every computation will have\n    `requires_grad=False`, even when the inputs have `requires_grad=True`.\n    There is an exception! All factory functions, or functions that create\n    a new Tensor and take a requires_grad kwarg, will NOT be affected by\n    this mode.\n\n    This context manager is thread local; it will not affect computation\n    in other threads.\n\n    Also functions as a decorator.\n\n    .. note::\n        No-grad is one of several mechanisms that can enable or\n        disable gradients locally see :ref:`locally-disable-grad-doc` for\n        more information on how they compare.\n\n    .. note::\n        This API does not apply to :ref:`forward-mode AD <forward-mode-ad>`.\n        If you want to disable forward AD for a computation, you can unpack\n        your dual tensors.\n\n    Example::\n        >>> # xdoctest: +SKIP\n        >>> x = torch.tensor([1.], requires_grad=True)\n        >>> with torch.no_grad():\n        ...     y = x * 2\n        >>> y.requires_grad\n        False\n        >>> @torch.no_grad()\n        ... def doubler(x):\n        ...     return x * 2\n        >>> z = doubler(x)\n        >>> z.requires_grad\n        False\n        >>> @torch.no_grad\n        ... def tripler(x):\n        ...     return x * 3\n        >>> z = tripler(x)\n        >>> z.requires_grad\n        False\n        >>> # factory function exception\n        >>> with torch.no_grad():\n        ...     a = torch.nn.Parameter(torch.rand(10))\n        >>> a.requires_grad\n        True\n    \"\"\"\n\n    def __init__(self) -> None:\n        if not torch._jit_internal.is_scripting():\n            super().__init__()\n        self.prev = False\n\n    def __enter__(self) -> None:\n        self.prev = torch.is_grad_enabled()\n        torch.set_grad_enabled(False)\n\n    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n        torch.set_grad_enabled(self.prev)\n\n\nclass enable_grad(_NoParamDecoratorContextManager):\n    r\"\"\"Context-manager that enables gradient calculation.\n\n    Enables gradient calculation, if it has been disabled via :class:`~no_grad`\n    or :class:`~set_grad_enabled`.\n\n    This context manager is thread local; it will not affect computation\n    in other threads.\n\n    Also functions as a decorator.\n\n    .. note::\n        enable_grad is one of several mechanisms that can enable or\n        disable gradients locally see :ref:`locally-disable-grad-doc` for\n        more information on how they compare.\n\n    .. note::\n        This API does not apply to :ref:`forward-mode AD <forward-mode-ad>`.\n\n    Example::\n        >>> # xdoctest: +SKIP\n        >>> x = torch.tensor([1.], requires_grad=True)\n        >>> with torch.no_grad():\n        ...     with torch.enable_grad():\n        ...         y = x * 2\n        >>> y.requires_grad\n        True\n        >>> y.backward()\n        >>> x.grad\n        tensor([2.])\n        >>> @torch.enable_grad()\n        ... def doubler(x):\n        ...     return x * 2\n        >>> with torch.no_grad():\n        ...     z = doubler(x)\n        >>> z.requires_grad\n        True\n        >>> @torch.enable_grad\n        ... def tripler(x):\n        ...     return x * 3\n        >>> with torch.no_grad():\n        ...     z = tripler(x)\n        >>> z.requires_grad\n        True\n\n    \"\"\"\n\n    def __enter__(self) -> None:\n        self.prev = torch.is_grad_enabled()\n        torch._C._set_grad_enabled(True)\n\n    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n        torch._C._set_grad_enabled(self.prev)\n\n\nclass set_grad_enabled(_DecoratorContextManager):\n    r\"\"\"Context-manager that sets gradient calculation on or off.\n\n    ``set_grad_enabled`` will enable or disable grads based on its argument :attr:`mode`.\n    It can be used as a context-manager or as a function.\n\n    This context manager is thread local; it will not affect computation\n    in other threads.\n\n    Args:\n        mode (bool): Flag whether to enable grad (``True``), or disable\n                     (``False``). This can be used to conditionally enable\n                     gradients.\n\n    .. note::\n        set_grad_enabled is one of several mechanisms that can enable or\n        disable gradients locally see :ref:`locally-disable-grad-doc` for\n        more information on how they compare.\n\n    .. note::\n        This API does not apply to :ref:`forward-mode AD <forward-mode-ad>`.\n\n    Example::\n        >>> # xdoctest: +SKIP\n        >>> x = torch.tensor([1.], requires_grad=True)\n        >>> is_train = False\n        >>> with torch.set_grad_enabled(is_train):\n        ...     y = x * 2\n        >>> y.requires_grad\n        False\n        >>> _ = torch.set_grad_enabled(True)\n        >>> y = x * 2\n        >>> y.requires_grad\n        True\n        >>> _ = torch.set_grad_enabled(False)\n        >>> y = x * 2\n        >>> y.requires_grad\n        False\n\n    \"\"\"\n\n    def __init__(self, mode: bool) -> None:\n        self.prev = torch.is_grad_enabled()\n        self.mode = mode\n        torch._C._set_grad_enabled(mode)\n\n    def __call__(self, orig_func: F) -> F:\n        torch._C._set_grad_enabled(self.prev)\n        return super().__call__(orig_func)\n\n    def __enter__(self) -> None:\n        torch._C._set_grad_enabled(self.mode)\n\n    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n        torch._C._set_grad_enabled(self.prev)\n\n    def clone(self) -> \"set_grad_enabled\":\n        return self.__class__(self.mode)\n\n\nclass inference_mode(_DecoratorContextManager):\n    r\"\"\"Context-manager that enables or disables inference mode.\n\n    InferenceMode is a new context manager analogous to :class:`~no_grad`\n    to be used when you are certain your operations will have no interactions\n    with autograd (e.g., model training). Code run under this mode gets better\n    performance by disabling view tracking and version counter bumps. Note that\n    unlike some other mechanisms that locally enable or disable grad,\n    entering inference_mode also disables to :ref:`forward-mode AD <forward-mode-ad>`.\n\n    This context manager is thread local; it will not affect computation\n    in other threads.\n\n    Also functions as a decorator.\n\n    .. note::\n        Inference mode is one of several mechanisms that can enable or\n        disable gradients locally see :ref:`locally-disable-grad-doc` for\n        more information on how they compare.\n\n    Args:\n        mode (bool or function): Either a boolean flag whether to enable or\n            disable inference mode or a Python function to decorate with\n            inference mode enabled\n\n    Example::\n        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n        >>> import torch\n        >>> x = torch.ones(1, 2, 3, requires_grad=True)\n        >>> with torch.inference_mode():\n        ...     y = x * x\n        >>> y.requires_grad\n        False\n        >>> # xdoctest: +SKIP(\"want string isnt quite right\")\n        >>> y._version\n        Traceback (most recent call last):\n        File \"<stdin>\", line 1, in <module>\n        RuntimeError: Inference tensors do not track version counter.\n        >>> @torch.inference_mode()\n        ... def func(x):\n        ...     return x * x\n        >>> out = func(x)\n        >>> out.requires_grad\n        False\n        >>> @torch.inference_mode\n        ... def doubler(x):\n        ...     return x * 2\n        >>> out = doubler(x)\n        >>> out.requires_grad\n        False\n\n    \"\"\"\n\n    def __init__(self, mode: bool = True) -> None:\n        if not torch._jit_internal.is_scripting():\n            super().__init__()\n        # Holds a context manager that can enable or disable inference mode\n        self._inference_mode_raii_context: Optional[torch._C._InferenceMode] = None\n        self.mode = mode\n\n    def __new__(cls, mode=True):\n        if isinstance(mode, bool):\n            return super().__new__(cls)\n        return cls()(mode)\n\n    def __enter__(self) -> None:\n        self._inference_mode_context = torch._C._InferenceMode(self.mode)\n        self._inference_mode_context.__enter__()\n\n    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n        self._inference_mode_context.__exit__(exc_type, exc_value, traceback)\n\n    def clone(self) -> \"inference_mode\":\n        return self.__class__(self.mode)\n\n\ndef _enter_inference_mode(mode):\n    mode_context = torch._C._InferenceMode(mode)\n    mode_context.__enter__()\n    return mode_context\n\n\ndef _exit_inference_mode(mode):\n    mode.__exit__(None, None, None)\n\n\nclass set_multithreading_enabled(_DecoratorContextManager):\n    r\"\"\"Context-manager that sets multithreaded backwards on or off.\n\n    ``set_multithreading_enabled`` will enable or disable multithreaded backwards based on its argument :attr:`mode`.\n    It can be used as a context-manager or as a function.\n\n    This context manager is thread local; it will not affect computation\n    in other threads.\n\n    Args:\n        mode (bool): Flag whether to enable multithreaded backwards (``True``), or disable\n                     (``False``).\n\n    .. note::\n        This API does not apply to :ref:`forward-mode AD <forward-mode-ad>`.\n\n    \"\"\"\n\n    def __init__(self, mode: bool) -> None:\n        self.prev = torch._C._is_multithreading_enabled()\n        torch._C._set_multithreading_enabled(mode)\n        self.mode = mode\n\n    def __enter__(self) -> None:\n        pass\n\n    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n        torch._C._set_multithreading_enabled(self.prev)\n\n    def clone(self) -> \"set_multithreading_enabled\":\n        return self.__class__(self.mode)\n\n\nclass _force_original_view_tracking(_DecoratorContextManager):\n    r\"\"\"Context-manager that sets whether or not to always enable view-replay in autograd.\n\n    ``set_view_replay_enabled`` will enable or disable view-replay based on its argument :attr:`mode`.\n    It can be used as a context-manager or as a function.\n\n    This context manager is thread local; it will not affect computation\n    in other threads.\n\n    When a tensor view is mutated, the autograd engine needs to decide whether or not\n    to regenerate the \"updated view\" by either replaying the chain of views from the updated base,\n    or with a single call to as_strided.\n\n    If set_view_replay_enabled is set to True, then autograd will always use view replay.\n    Otherwise, it will fall back to its existing logic.\n\n    Args:\n        mode (bool): Flag whether to enable view-replay (``True``), or disable\n                     (``False``).\n\n    \"\"\"\n\n    def __init__(self, mode: bool) -> None:\n        self.prev = torch._C._is_view_replay_enabled()\n        torch._C._set_view_replay_enabled(mode)\n        self.mode = mode\n\n    def __enter__(self) -> None:\n        pass\n\n    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n        torch._C._set_view_replay_enabled(self.prev)\n\n    def clone(self):\n        return self.__class__(self.mode)\n\n\nclass _unsafe_preserve_version_counter(_DecoratorContextManager):\n    r\"\"\"DO NOT USE THIS UNLESS YOU KNOW EXACTLY WHAT YOU'RE DOING.\n\n    This context manager can lead to arbitrary silent-correctness issues in any other part of your code\n    (even the ones not touched directly by the context manager)!\n\n    Ordinarily, autograd will track mutations to tensors by incrementing it's `._version` attribute.\n    This is generally important for correctness, as for example, mutating a tensor that autograd has saved\n    for the backwards pass can result in incorrect gradients, and autograd uses the version counter to detect\n    and error out in this situation.\n\n    However, there are rare instances where it might be useful to hide mutations from autograd. For example:\n    if a tensor is very large, and you'd like to free its memory by storing it elsewhere, and re-populate\n    the tensor right before it is needed by autograd.\n\n    Args:\n        tensor (torch.Tensor): the tensor in question, that you would like to preserve the version counter of.\n\n    .. note::\n        This API does not apply to :ref:`forward-mode AD <forward-mode-ad>`.\n\n    \"\"\"\n\n    def __init__(self, tensor: torch.Tensor) -> None:\n        self.tensor = tensor\n        self.prev_version = tensor._version\n\n    def __enter__(self) -> None:\n        pass\n\n    def __exit__(self, *args) -> None:\n        torch._C._autograd._unsafe_set_version_counter(self.tensor, self.prev_version)\n", 389], "C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py": ["from collections import OrderedDict, namedtuple\nimport itertools\nimport warnings\nimport functools\nimport weakref\n\nimport torch\nfrom torch._prims_common import DeviceLikeType\nfrom ..parameter import Parameter\nimport torch.utils.hooks as hooks\n\nfrom torch import Tensor, device, dtype\nfrom typing import Union, Tuple, Any, Callable, Iterator, Set, Optional, overload, TypeVar, Mapping, Dict, List\nfrom typing_extensions import Self\nfrom ...utils.hooks import RemovableHandle\n\n__all__ = ['register_module_forward_pre_hook', 'register_module_forward_hook',\n           'register_module_full_backward_pre_hook', 'register_module_backward_hook',\n           'register_module_full_backward_hook', 'register_module_buffer_registration_hook',\n           'register_module_module_registration_hook', 'register_module_parameter_registration_hook', 'Module']\n\n_grad_t = Union[Tuple[Tensor, ...], Tensor]\n# See https://mypy.readthedocs.io/en/latest/generics.html#generic-methods-and-generic-self for the use\n# of `T` to annotate `self`. Many methods of `Module` return `self` and we want those return values to be\n# the type of the subclass, not the looser type of `Module`.\nT = TypeVar('T', bound='Module')\n\n\nclass _IncompatibleKeys(namedtuple('IncompatibleKeys', ['missing_keys', 'unexpected_keys'])):\n    def __repr__(self):\n        if not self.missing_keys and not self.unexpected_keys:\n            return '<All keys matched successfully>'\n        return super().__repr__()\n\n    __str__ = __repr__\n\n\ndef _addindent(s_, numSpaces):\n    s = s_.split('\\n')\n    # don't do anything for single-line stuff\n    if len(s) == 1:\n        return s_\n    first = s.pop(0)\n    s = [(numSpaces * ' ') + line for line in s]\n    s = '\\n'.join(s)\n    s = first + '\\n' + s\n    return s\n\nr\"\"\"This tracks hooks common to all modules that are executed immediately before\n.registering the buffer/module/parameter\"\"\"\n_global_buffer_registration_hooks: Dict[int, Callable] = OrderedDict()\n_global_module_registration_hooks: Dict[int, Callable] = OrderedDict()\n_global_parameter_registration_hooks: Dict[int, Callable] = OrderedDict()\n\nclass _WrappedHook:\n    def __init__(self, hook: Callable, module: Optional[\"Module\"] = None):\n        self.hook: Callable = hook\n        functools.update_wrapper(self, hook)\n\n        self.with_module: bool = False\n\n        if module is not None:\n            self.module: weakref.ReferenceType[Module] = weakref.ref(module)\n            self.with_module = True\n\n    def __call__(self, *args: Any, **kwargs: Any) -> Any:\n        if self.with_module:\n            module = self.module()\n            if module is None:\n                raise RuntimeError(\"You are trying to call the hook of a dead Module!\")\n            return self.hook(module, *args, **kwargs)\n        return self.hook(*args, **kwargs)\n\n    def __getstate__(self) -> Dict:\n        result = {\"hook\": self.hook, \"with_module\": self.with_module}\n        if self.with_module:\n            result[\"module\"] = self.module()\n\n        return result\n\n    def __setstate__(self, state: Dict):\n        self.hook = state[\"hook\"]\n        self.with_module = state[\"with_module\"]\n\n        if self.with_module:\n            if state[\"module\"] is None:\n                raise RuntimeError(\"You are trying to revive the hook of a dead Module!\")\n            self.module = weakref.ref(state[\"module\"])\n\n\nr\"\"\"This tracks hooks common to all modules that are executed before/after\ncalling forward and backward. This is global state used for debugging/profiling\npurposes\"\"\"\n_global_backward_pre_hooks: Dict[int, Callable] = OrderedDict()\n_global_backward_hooks: Dict[int, Callable] = OrderedDict()\n_global_is_full_backward_hook: Optional[bool] = None\n_global_forward_pre_hooks: Dict[int, Callable] = OrderedDict()\n_global_forward_hooks: Dict[int, Callable] = OrderedDict()\n_global_forward_hooks_always_called: Dict[int, bool] = OrderedDict()\n\n_EXTRA_STATE_KEY_SUFFIX = '_extra_state'\n\n\ndef register_module_buffer_registration_hook(hook: Callable[..., None]) -> RemovableHandle:\n    r\"\"\"Register a buffer registration hook common to all modules.\n\n    .. warning ::\n\n        This adds global state to the `nn.Module` module\n\n    The hook will be called every time :func:`register_buffer` is invoked.\n    It should have the following signature::\n\n        hook(module, name, buffer) -> None or new buffer\n\n    The hook can modify the input or return a single modified value in the hook.\n\n    Returns:\n        :class:`torch.utils.hooks.RemovableHandle`:\n            a handle that can be used to remove the added hook by calling\n            ``handle.remove()``\n    \"\"\"\n    handle = hooks.RemovableHandle(_global_buffer_registration_hooks)\n    _global_buffer_registration_hooks[handle.id] = hook\n    return handle\n\n\ndef register_module_module_registration_hook(hook: Callable[..., None]) -> RemovableHandle:\n    r\"\"\"Register a module registration hook common to all modules.\n\n    .. warning ::\n\n        This adds global state to the `nn.Module` module\n\n    The hook will be called every time :func:`register_module` is invoked.\n    It should have the following signature::\n\n        hook(module, name, submodule) -> None or new submodule\n\n    The hook can modify the input or return a single modified value in the hook.\n\n    Returns:\n        :class:`torch.utils.hooks.RemovableHandle`:\n            a handle that can be used to remove the added hook by calling\n            ``handle.remove()``\n    \"\"\"\n    handle = hooks.RemovableHandle(_global_module_registration_hooks)\n    _global_module_registration_hooks[handle.id] = hook\n    return handle\n\n\ndef register_module_parameter_registration_hook(hook: Callable[..., None]) -> RemovableHandle:\n    r\"\"\"Register a parameter registration hook common to all modules.\n\n    .. warning ::\n\n        This adds global state to the `nn.Module` module\n\n    The hook will be called every time :func:`register_parameter` is invoked.\n    It should have the following signature::\n\n        hook(module, name, param) -> None or new parameter\n\n    The hook can modify the input or return a single modified value in the hook.\n\n    Returns:\n        :class:`torch.utils.hooks.RemovableHandle`:\n            a handle that can be used to remove the added hook by calling\n            ``handle.remove()``\n    \"\"\"\n    handle = hooks.RemovableHandle(_global_parameter_registration_hooks)\n    _global_parameter_registration_hooks[handle.id] = hook\n    return handle\n\n\ndef register_module_forward_pre_hook(hook: Callable[..., None]) -> RemovableHandle:\n    r\"\"\"Register a forward pre-hook common to all modules.\n\n    .. warning ::\n\n        This adds global state to the `nn.module` module\n        and it is only intended for debugging/profiling purposes.\n\n    The hook will be called every time before :func:`forward` is invoked.\n    It should have the following signature::\n\n        hook(module, input) -> None or modified input\n\n    The input contains only the positional arguments given to the module.\n    Keyword arguments won't be passed to the hooks and only to the ``forward``.\n    The hook can modify the input. User can either return a tuple or a\n    single modified value in the hook. We will wrap the value into a tuple\n    if a single value is returned(unless that value is already a tuple).\n\n    This hook has precedence over the specific module hooks registered with\n    ``register_forward_pre_hook``.\n\n    Returns:\n        :class:`torch.utils.hooks.RemovableHandle`:\n            a handle that can be used to remove the added hook by calling\n            ``handle.remove()``\n    \"\"\"\n    handle = hooks.RemovableHandle(_global_forward_pre_hooks)\n    _global_forward_pre_hooks[handle.id] = hook\n    return handle\n\n\ndef register_module_forward_hook(hook: Callable[..., None], *, always_call: bool = False) -> RemovableHandle:\n    r\"\"\"Register a global forward hook for all the modules.\n\n    .. warning ::\n\n        This adds global state to the `nn.module` module\n        and it is only intended for debugging/profiling purposes.\n\n    The hook will be called every time after :func:`forward` has computed an output.\n    It should have the following signature::\n\n        hook(module, input, output) -> None or modified output\n\n    The input contains only the positional arguments given to the module.\n    Keyword arguments won't be passed to the hooks and only to the ``forward``.\n    The hook can modify the output. It can modify the input inplace but\n    it will not have effect on forward since this is called after\n    :func:`forward` is called.\n\n    Parameters:\n        hook (Callable): The user defined hook to be registered.\n        always_call (bool): If ``True`` the ``hook`` will be run regardless of\n            whether an exception is raised while calling the Module.\n            Default: ``False``\n    Returns:\n        :class:`torch.utils.hooks.RemovableHandle`:\n            a handle that can be used to remove the added hook by calling\n            ``handle.remove()``\n\n    This hook will be executed before specific module hooks registered with\n    ``register_forward_hook``.\n    \"\"\"\n    handle = hooks.RemovableHandle(_global_forward_hooks,\n                                   extra_dict=_global_forward_hooks_always_called)\n    _global_forward_hooks[handle.id] = hook\n    if always_call:\n        _global_forward_hooks_always_called[handle.id] = True\n    return handle\n\n\ndef register_module_backward_hook(\n    hook: Callable[['Module', _grad_t, _grad_t], Union[None, _grad_t]]\n) -> RemovableHandle:\n    r\"\"\"Register a backward hook common to all the modules.\n\n    This function is deprecated in favor of\n    :func:`torch.nn.modules.module.register_module_full_backward_hook`\n    and the behavior of this function will change in future versions.\n\n    Returns:\n        :class:`torch.utils.hooks.RemovableHandle`:\n            a handle that can be used to remove the added hook by calling\n            ``handle.remove()``\n\n    \"\"\"\n    global _global_is_full_backward_hook\n    if _global_is_full_backward_hook is True:\n        raise RuntimeError(\"Cannot use both regular backward hooks and full backward hooks as a \"\n                           \"global Module hook. Please use only one of them.\")\n\n    _global_is_full_backward_hook = False\n\n    handle = hooks.RemovableHandle(_global_backward_hooks)\n    _global_backward_hooks[handle.id] = hook\n    return handle\n\n\ndef register_module_full_backward_pre_hook(\n    hook: Callable[['Module', _grad_t], Union[None, _grad_t]]\n) -> RemovableHandle:\n    r\"\"\"Register a backward pre-hook common to all the modules.\n\n    .. warning ::\n        This adds global state to the `nn.module` module\n        and it is only intended for debugging/profiling purposes.\n\n    The hook will be called every time the gradients for the module are computed.\n    The hook should have the following signature::\n\n        hook(module, grad_output) -> Tensor or None\n\n    The :attr:`grad_output` is a tuple. The hook should\n    not modify its arguments, but it can optionally return a new gradient with\n    respect to the output that will be used in place of :attr:`grad_output` in\n    subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n    all non-Tensor arguments.\n\n    For technical reasons, when this hook is applied to a Module, its forward function will\n    receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n    of each Tensor returned by the Module's forward function.\n\n    Global hooks are called before hooks registered with `register_backward_pre_hook`\n\n    Returns:\n        :class:`torch.utils.hooks.RemovableHandle`:\n            a handle that can be used to remove the added hook by calling\n            ``handle.remove()``\n\n    \"\"\"\n    handle = hooks.RemovableHandle(_global_backward_pre_hooks)\n    _global_backward_pre_hooks[handle.id] = hook\n    return handle\n\n\ndef register_module_full_backward_hook(\n    hook: Callable[['Module', _grad_t, _grad_t], Union[None, _grad_t]]\n) -> RemovableHandle:\n    r\"\"\"Register a backward hook common to all the modules.\n\n    .. warning ::\n        This adds global state to the `nn.module` module\n        and it is only intended for debugging/profiling purposes.\n\n    The hook will be called every time the gradients with respect to a module\n    are computed, i.e. the hook will execute if and only if the gradients with\n    respect to module outputs are computed. The hook should have the following\n    signature::\n\n        hook(module, grad_input, grad_output) -> Tensor or None\n\n    The :attr:`grad_input` and :attr:`grad_output` are tuples. The hook should\n    not modify its arguments, but it can optionally return a new gradient with\n    respect to the input that will be used in place of :attr:`grad_input` in\n    subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n    as positional arguments and all kwarg arguments will not appear in the hook. Entries\n    in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n    arguments.\n\n    For technical reasons, when this hook is applied to a Module, its forward function will\n    receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n    of each Tensor returned by the Module's forward function.\n\n    Global hooks are called before hooks registered with `register_backward_hook`\n\n    Returns:\n        :class:`torch.utils.hooks.RemovableHandle`:\n            a handle that can be used to remove the added hook by calling\n            ``handle.remove()``\n\n    \"\"\"\n    global _global_is_full_backward_hook\n    if _global_is_full_backward_hook is False:\n        raise RuntimeError(\"Cannot use both regular backward hooks and full backward hooks as a \"\n                           \"global Module hook. Please use only one of them.\")\n\n    _global_is_full_backward_hook = True\n\n    handle = hooks.RemovableHandle(_global_backward_hooks)\n    _global_backward_hooks[handle.id] = hook\n    return handle\n\n\n# Trick mypy into not applying contravariance rules to inputs by defining\n# forward as a value, rather than a function.  See also\n# https://github.com/python/mypy/issues/8795\ndef _forward_unimplemented(self, *input: Any) -> None:\n    r\"\"\"Define the computation performed at every call.\n\n    Should be overridden by all subclasses.\n\n    .. note::\n        Although the recipe for forward pass needs to be defined within\n        this function, one should call the :class:`Module` instance afterwards\n        instead of this since the former takes care of running the\n        registered hooks while the latter silently ignores them.\n    \"\"\"\n    raise NotImplementedError(f\"Module [{type(self).__name__}] is missing the required \\\"forward\\\" function\")\n\n\nclass Module:\n    r\"\"\"Base class for all neural network modules.\n\n    Your models should also subclass this class.\n\n    Modules can also contain other Modules, allowing to nest them in\n    a tree structure. You can assign the submodules as regular attributes::\n\n        import torch.nn as nn\n        import torch.nn.functional as F\n\n        class Model(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.conv1 = nn.Conv2d(1, 20, 5)\n                self.conv2 = nn.Conv2d(20, 20, 5)\n\n            def forward(self, x):\n                x = F.relu(self.conv1(x))\n                return F.relu(self.conv2(x))\n\n    Submodules assigned in this way will be registered, and will have their\n    parameters converted too when you call :meth:`to`, etc.\n\n    .. note::\n        As per the example above, an ``__init__()`` call to the parent class\n        must be made before assignment on the child.\n\n    :ivar training: Boolean represents whether this module is in training or\n                    evaluation mode.\n    :vartype training: bool\n    \"\"\"\n\n    dump_patches: bool = False\n\n    _version: int = 1\n    r\"\"\"This allows better BC support for :meth:`load_state_dict`. In\n    :meth:`state_dict`, the version number will be saved as in the attribute\n    `_metadata` of the returned state dict, and thus pickled. `_metadata` is a\n    dictionary with keys that follow the naming convention of state dict. See\n    ``_load_from_state_dict`` on how to use this information in loading.\n\n    If new parameters/buffers are added/removed from a module, this number shall\n    be bumped, and the module's `_load_from_state_dict` method can compare the\n    version number and do appropriate changes if the state dict is from before\n    the change.\"\"\"\n\n    training: bool\n    _parameters: Dict[str, Optional[Parameter]]\n    _buffers: Dict[str, Optional[Tensor]]\n    _non_persistent_buffers_set: Set[str]\n    _backward_pre_hooks: Dict[int, Callable]\n    _backward_hooks: Dict[int, Callable]\n    _is_full_backward_hook: Optional[bool]\n    _forward_hooks: Dict[int, Callable]\n    # Marks whether the corresponding _forward_hooks accept kwargs or not.\n    # As JIT does not support Set[int], this dict is used as a set, where all\n    # hooks represented in this dict accept kwargs.\n    _forward_hooks_with_kwargs: Dict[int, bool]\n    # forward hooks that should always be called even if an exception is raised\n    _forward_hooks_always_called: Dict[int, bool]\n    _forward_pre_hooks: Dict[int, Callable]\n    # Marks whether the corresponding _forward_hooks accept kwargs or not.\n    # As JIT does not support Set[int], this dict is used as a set, where all\n    # hooks represented in this dict accept kwargs.\n    _forward_pre_hooks_with_kwargs: Dict[int, bool]\n    _state_dict_hooks: Dict[int, Callable]\n    _load_state_dict_pre_hooks: Dict[int, Callable]\n    _state_dict_pre_hooks: Dict[int, Callable]\n    _load_state_dict_post_hooks: Dict[int, Callable]\n    _modules: Dict[str, Optional['Module']]\n    call_super_init: bool = False\n    _compiled_call_impl : Optional[Callable] = None\n\n    def __init__(self, *args, **kwargs) -> None:\n        \"\"\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\"\"\"\n        torch._C._log_api_usage_once(\"python.nn_module\")\n\n        # Backward compatibility: no args used to be allowed when call_super_init=False\n        if self.call_super_init is False and bool(kwargs):\n            raise TypeError(\"{}.__init__() got an unexpected keyword argument '{}'\"\n                            \"\".format(type(self).__name__, next(iter(kwargs))))\n\n        if self.call_super_init is False and bool(args):\n            raise TypeError(f\"{type(self).__name__}.__init__() takes 1 positional argument but {len(args) + 1} were\"\n                            \" given\")\n\n        \"\"\"\n        Calls super().__setattr__('a', a) instead of the typical self.a = a\n        to avoid Module.__setattr__ overhead. Module's __setattr__ has special\n        handling for parameters, submodules, and buffers but simply calls into\n        super().__setattr__ for all other attributes.\n        \"\"\"\n        super().__setattr__('training', True)\n        super().__setattr__('_parameters', OrderedDict())\n        super().__setattr__('_buffers', OrderedDict())\n        super().__setattr__('_non_persistent_buffers_set', set())\n        super().__setattr__('_backward_pre_hooks', OrderedDict())\n        super().__setattr__('_backward_hooks', OrderedDict())\n        super().__setattr__('_is_full_backward_hook', None)\n        super().__setattr__('_forward_hooks', OrderedDict())\n        super().__setattr__('_forward_hooks_with_kwargs', OrderedDict())\n        super().__setattr__('_forward_hooks_always_called', OrderedDict())\n        super().__setattr__('_forward_pre_hooks', OrderedDict())\n        super().__setattr__('_forward_pre_hooks_with_kwargs', OrderedDict())\n        super().__setattr__('_state_dict_hooks', OrderedDict())\n        super().__setattr__('_state_dict_pre_hooks', OrderedDict())\n        super().__setattr__('_load_state_dict_pre_hooks', OrderedDict())\n        super().__setattr__('_load_state_dict_post_hooks', OrderedDict())\n        super().__setattr__('_modules', OrderedDict())\n\n        if self.call_super_init:\n            super().__init__(*args, **kwargs)\n\n    forward: Callable[..., Any] = _forward_unimplemented\n\n    def register_buffer(self, name: str, tensor: Optional[Tensor], persistent: bool = True) -> None:\n        r\"\"\"Add a buffer to the module.\n\n        This is typically used to register a buffer that should not to be\n        considered a model parameter. For example, BatchNorm's ``running_mean``\n        is not a parameter, but is part of the module's state. Buffers, by\n        default, are persistent and will be saved alongside parameters. This\n        behavior can be changed by setting :attr:`persistent` to ``False``. The\n        only difference between a persistent buffer and a non-persistent buffer\n        is that the latter will not be a part of this module's\n        :attr:`state_dict`.\n\n        Buffers can be accessed as attributes using given names.\n\n        Args:\n            name (str): name of the buffer. The buffer can be accessed\n                from this module using the given name\n            tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n                that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n                the buffer is **not** included in the module's :attr:`state_dict`.\n            persistent (bool): whether the buffer is part of this module's\n                :attr:`state_dict`.\n\n        Example::\n\n            >>> # xdoctest: +SKIP(\"undefined vars\")\n            >>> self.register_buffer('running_mean', torch.zeros(num_features))\n\n        \"\"\"\n        if persistent is False and isinstance(self, torch.jit.ScriptModule):\n            raise RuntimeError(\"ScriptModule does not support non-persistent buffers\")\n\n        if '_buffers' not in self.__dict__:\n            raise AttributeError(\n                \"cannot assign buffer before Module.__init__() call\")\n        elif not isinstance(name, str):\n            raise TypeError(f\"buffer name should be a string. Got {torch.typename(name)}\")\n        elif '.' in name:\n            raise KeyError(\"buffer name can't contain \\\".\\\"\")\n        elif name == '':\n            raise KeyError(\"buffer name can't be empty string \\\"\\\"\")\n        elif hasattr(self, name) and name not in self._buffers:\n            raise KeyError(f\"attribute '{name}' already exists\")\n        elif tensor is not None and not isinstance(tensor, torch.Tensor):\n            raise TypeError(f\"cannot assign '{torch.typename(tensor)}' object to buffer '{name}' \"\n                            \"(torch Tensor or None required)\"\n                            )\n        else:\n            for hook in _global_buffer_registration_hooks.values():\n                output = hook(self, name, tensor)\n                if output is not None:\n                    tensor = output\n            self._buffers[name] = tensor\n            if persistent:\n                self._non_persistent_buffers_set.discard(name)\n            else:\n                self._non_persistent_buffers_set.add(name)\n\n    def register_parameter(self, name: str, param: Optional[Parameter]) -> None:\n        r\"\"\"Add a parameter to the module.\n\n        The parameter can be accessed as an attribute using given name.\n\n        Args:\n            name (str): name of the parameter. The parameter can be accessed\n                from this module using the given name\n            param (Parameter or None): parameter to be added to the module. If\n                ``None``, then operations that run on parameters, such as :attr:`cuda`,\n                are ignored. If ``None``, the parameter is **not** included in the\n                module's :attr:`state_dict`.\n        \"\"\"\n        if '_parameters' not in self.__dict__:\n            raise AttributeError(\n                \"cannot assign parameter before Module.__init__() call\")\n\n        elif not isinstance(name, str):\n            raise TypeError(f\"parameter name should be a string. Got {torch.typename(name)}\")\n        elif '.' in name:\n            raise KeyError(\"parameter name can't contain \\\".\\\"\")\n        elif name == '':\n            raise KeyError(\"parameter name can't be empty string \\\"\\\"\")\n        elif hasattr(self, name) and name not in self._parameters:\n            raise KeyError(f\"attribute '{name}' already exists\")\n\n        if param is None:\n            self._parameters[name] = None\n        elif not isinstance(param, Parameter):\n            raise TypeError(f\"cannot assign '{torch.typename(param)}' object to parameter '{name}' \"\n                            \"(torch.nn.Parameter or None required)\"\n                            )\n        elif param.grad_fn:\n            raise ValueError(\n                f\"Cannot assign non-leaf Tensor to parameter '{name}'. Model \"\n                f\"parameters must be created explicitly. To express '{name}' \"\n                \"as a function of another Tensor, compute the value in \"\n                \"the forward() method.\")\n        else:\n            for hook in _global_parameter_registration_hooks.values():\n                output = hook(self, name, param)\n                if output is not None:\n                    param = output\n            self._parameters[name] = param\n\n    def add_module(self, name: str, module: Optional['Module']) -> None:\n        r\"\"\"Add a child module to the current module.\n\n        The module can be accessed as an attribute using the given name.\n\n        Args:\n            name (str): name of the child module. The child module can be\n                accessed from this module using the given name\n            module (Module): child module to be added to the module.\n        \"\"\"\n        if not isinstance(module, Module) and module is not None:\n            raise TypeError(f\"{torch.typename(module)} is not a Module subclass\")\n        elif not isinstance(name, str):\n            raise TypeError(f\"module name should be a string. Got {torch.typename(name)}\")\n        elif hasattr(self, name) and name not in self._modules:\n            raise KeyError(f\"attribute '{name}' already exists\")\n        elif '.' in name:\n            raise KeyError(f\"module name can't contain \\\".\\\", got: {name}\")\n        elif name == '':\n            raise KeyError(\"module name can't be empty string \\\"\\\"\")\n        for hook in _global_module_registration_hooks.values():\n            output = hook(self, name, module)\n            if output is not None:\n                module = output\n        self._modules[name] = module\n\n    def register_module(self, name: str, module: Optional['Module']) -> None:\n        r\"\"\"Alias for :func:`add_module`.\"\"\"\n        self.add_module(name, module)\n\n    def get_submodule(self, target: str) -> \"Module\":\n        \"\"\"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\n        For example, let's say you have an ``nn.Module`` ``A`` that\n        looks like this:\n\n        .. code-block:: text\n\n            A(\n                (net_b): Module(\n                    (net_c): Module(\n                        (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n                    )\n                    (linear): Linear(in_features=100, out_features=200, bias=True)\n                )\n            )\n\n        (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n        submodule ``net_b``, which itself has two submodules ``net_c``\n        and ``linear``. ``net_c`` then has a submodule ``conv``.)\n\n        To check whether or not we have the ``linear`` submodule, we\n        would call ``get_submodule(\"net_b.linear\")``. To check whether\n        we have the ``conv`` submodule, we would call\n        ``get_submodule(\"net_b.net_c.conv\")``.\n\n        The runtime of ``get_submodule`` is bounded by the degree\n        of module nesting in ``target``. A query against\n        ``named_modules`` achieves the same result, but it is O(N) in\n        the number of transitive modules. So, for a simple check to see\n        if some submodule exists, ``get_submodule`` should always be\n        used.\n\n        Args:\n            target: The fully-qualified string name of the submodule\n                to look for. (See above example for how to specify a\n                fully-qualified string.)\n\n        Returns:\n            torch.nn.Module: The submodule referenced by ``target``\n\n        Raises:\n            AttributeError: If the target string references an invalid\n                path or resolves to something that is not an\n                ``nn.Module``\n        \"\"\"\n        if target == \"\":\n            return self\n\n        atoms: List[str] = target.split(\".\")\n        mod: torch.nn.Module = self\n\n        for item in atoms:\n\n            if not hasattr(mod, item):\n                raise AttributeError(mod._get_name() + \" has no \"\n                                     \"attribute `\" + item + \"`\")\n\n            mod = getattr(mod, item)\n\n            if not isinstance(mod, torch.nn.Module):\n                raise AttributeError(\"`\" + item + \"` is not \"\n                                     \"an nn.Module\")\n\n        return mod\n\n    def get_parameter(self, target: str) -> \"Parameter\":\n        \"\"\"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\n        See the docstring for ``get_submodule`` for a more detailed\n        explanation of this method's functionality as well as how to\n        correctly specify ``target``.\n\n        Args:\n            target: The fully-qualified string name of the Parameter\n                to look for. (See ``get_submodule`` for how to specify a\n                fully-qualified string.)\n\n        Returns:\n            torch.nn.Parameter: The Parameter referenced by ``target``\n\n        Raises:\n            AttributeError: If the target string references an invalid\n                path or resolves to something that is not an\n                ``nn.Parameter``\n        \"\"\"\n        module_path, _, param_name = target.rpartition(\".\")\n\n        mod: torch.nn.Module = self.get_submodule(module_path)\n\n        if not hasattr(mod, param_name):\n            raise AttributeError(mod._get_name() + \" has no attribute `\"\n                                 + param_name + \"`\")\n\n        param: torch.nn.Parameter = getattr(mod, param_name)\n\n        if not isinstance(param, torch.nn.Parameter):\n            raise AttributeError(\"`\" + param_name + \"` is not an \"\n                                 \"nn.Parameter\")\n\n        return param\n\n    def get_buffer(self, target: str) -> \"Tensor\":\n        \"\"\"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\n        See the docstring for ``get_submodule`` for a more detailed\n        explanation of this method's functionality as well as how to\n        correctly specify ``target``.\n\n        Args:\n            target: The fully-qualified string name of the buffer\n                to look for. (See ``get_submodule`` for how to specify a\n                fully-qualified string.)\n\n        Returns:\n            torch.Tensor: The buffer referenced by ``target``\n\n        Raises:\n            AttributeError: If the target string references an invalid\n                path or resolves to something that is not a\n                buffer\n        \"\"\"\n        module_path, _, buffer_name = target.rpartition(\".\")\n\n        mod: torch.nn.Module = self.get_submodule(module_path)\n\n        if not hasattr(mod, buffer_name):\n            raise AttributeError(mod._get_name() + \" has no attribute `\"\n                                 + buffer_name + \"`\")\n\n        buffer: torch.Tensor = getattr(mod, buffer_name)\n\n        if buffer_name not in mod._buffers:\n            raise AttributeError(\"`\" + buffer_name + \"` is not a buffer\")\n\n        return buffer\n\n    def get_extra_state(self) -> Any:\n        \"\"\"Return any extra state to include in the module's state_dict.\n\n        Implement this and a corresponding :func:`set_extra_state` for your module\n        if you need to store extra state. This function is called when building the\n        module's `state_dict()`.\n\n        Note that extra state should be picklable to ensure working serialization\n        of the state_dict. We only provide provide backwards compatibility guarantees\n        for serializing Tensors; other objects may break backwards compatibility if\n        their serialized pickled form changes.\n\n        Returns:\n            object: Any extra state to store in the module's state_dict\n        \"\"\"\n        raise RuntimeError(\n            \"Reached a code path in Module.get_extra_state() that should never be called. \"\n            \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml \"\n            \"to report this bug.\")\n\n    def set_extra_state(self, state: Any):\n        \"\"\"Set extra state contained in the loaded `state_dict`.\n\n        This function is called from :func:`load_state_dict` to handle any extra state\n        found within the `state_dict`. Implement this function and a corresponding\n        :func:`get_extra_state` for your module if you need to store extra state within its\n        `state_dict`.\n\n        Args:\n            state (dict): Extra state from the `state_dict`\n        \"\"\"\n        raise RuntimeError(\n            \"Reached a code path in Module.set_extra_state() that should never be called. \"\n            \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml \"\n            \"to report this bug.\")\n\n    def _apply(self, fn, recurse=True):\n        if recurse:\n            for module in self.children():\n                module._apply(fn)\n\n        def compute_should_use_set_data(tensor, tensor_applied):\n            if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n                # If the new tensor has compatible tensor type as the existing tensor,\n                # the current behavior is to change the tensor in-place using `.data =`,\n                # and the future behavior is to overwrite the existing tensor. However,\n                # changing the current behavior is a BC-breaking change, and we want it\n                # to happen in future releases. So for now we introduce the\n                # `torch.__future__.get_overwrite_module_params_on_conversion()`\n                # global flag to let the user control whether they want the future\n                # behavior of overwriting the existing tensor or not.\n                return not torch.__future__.get_overwrite_module_params_on_conversion()\n            else:\n                return False\n\n        for key, param in self._parameters.items():\n            if param is None:\n                continue\n            # Tensors stored in modules are graph leaves, and we don't want to\n            # track autograd history of `param_applied`, so we have to use\n            # `with torch.no_grad():`\n            with torch.no_grad():\n                param_applied = fn(param)\n            should_use_set_data = compute_should_use_set_data(param, param_applied)\n            if should_use_set_data:\n                param.data = param_applied\n                out_param = param\n            else:\n                assert isinstance(param, Parameter)\n                assert param.is_leaf\n                out_param = Parameter(param_applied, param.requires_grad)\n                self._parameters[key] = out_param\n\n            if param.grad is not None:\n                with torch.no_grad():\n                    grad_applied = fn(param.grad)\n                should_use_set_data = compute_should_use_set_data(param.grad, grad_applied)\n                if should_use_set_data:\n                    assert out_param.grad is not None\n                    out_param.grad.data = grad_applied\n                else:\n                    assert param.grad.is_leaf\n                    out_param.grad = grad_applied.requires_grad_(param.grad.requires_grad)\n\n        for key, buf in self._buffers.items():\n            if buf is not None:\n                self._buffers[key] = fn(buf)\n\n        return self\n\n    def apply(self: T, fn: Callable[['Module'], None]) -> T:\n        r\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\n        Typical use includes initializing the parameters of a model\n        (see also :ref:`nn-init-doc`).\n\n        Args:\n            fn (:class:`Module` -> None): function to be applied to each submodule\n\n        Returns:\n            Module: self\n\n        Example::\n\n            >>> @torch.no_grad()\n            >>> def init_weights(m):\n            >>>     print(m)\n            >>>     if type(m) == nn.Linear:\n            >>>         m.weight.fill_(1.0)\n            >>>         print(m.weight)\n            >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n            >>> net.apply(init_weights)\n            Linear(in_features=2, out_features=2, bias=True)\n            Parameter containing:\n            tensor([[1., 1.],\n                    [1., 1.]], requires_grad=True)\n            Linear(in_features=2, out_features=2, bias=True)\n            Parameter containing:\n            tensor([[1., 1.],\n                    [1., 1.]], requires_grad=True)\n            Sequential(\n              (0): Linear(in_features=2, out_features=2, bias=True)\n              (1): Linear(in_features=2, out_features=2, bias=True)\n            )\n\n        \"\"\"\n        for module in self.children():\n            module.apply(fn)\n        fn(self)\n        return self\n\n    def cuda(self: T, device: Optional[Union[int, device]] = None) -> T:\n        r\"\"\"Move all model parameters and buffers to the GPU.\n\n        This also makes associated parameters and buffers different objects. So\n        it should be called before constructing optimizer if the module will\n        live on GPU while being optimized.\n\n        .. note::\n            This method modifies the module in-place.\n\n        Args:\n            device (int, optional): if specified, all parameters will be\n                copied to that device\n\n        Returns:\n            Module: self\n        \"\"\"\n        return self._apply(lambda t: t.cuda(device))\n\n    def ipu(self: T, device: Optional[Union[int, device]] = None) -> T:\n        r\"\"\"Move all model parameters and buffers to the IPU.\n\n        This also makes associated parameters and buffers different objects. So\n        it should be called before constructing optimizer if the module will\n        live on IPU while being optimized.\n\n        .. note::\n            This method modifies the module in-place.\n\n        Arguments:\n            device (int, optional): if specified, all parameters will be\n                copied to that device\n\n        Returns:\n            Module: self\n        \"\"\"\n        return self._apply(lambda t: t.ipu(device))\n\n    def xpu(self: T, device: Optional[Union[int, device]] = None) -> T:\n        r\"\"\"Move all model parameters and buffers to the XPU.\n\n        This also makes associated parameters and buffers different objects. So\n        it should be called before constructing optimizer if the module will\n        live on XPU while being optimized.\n\n        .. note::\n            This method modifies the module in-place.\n\n        Arguments:\n            device (int, optional): if specified, all parameters will be\n                copied to that device\n\n        Returns:\n            Module: self\n        \"\"\"\n        return self._apply(lambda t: t.xpu(device))\n\n    def cpu(self: T) -> T:\n        r\"\"\"Move all model parameters and buffers to the CPU.\n\n        .. note::\n            This method modifies the module in-place.\n\n        Returns:\n            Module: self\n        \"\"\"\n        return self._apply(lambda t: t.cpu())\n\n    def type(self: T, dst_type: Union[dtype, str]) -> T:\n        r\"\"\"Casts all parameters and buffers to :attr:`dst_type`.\n\n        .. note::\n            This method modifies the module in-place.\n\n        Args:\n            dst_type (type or string): the desired type\n\n        Returns:\n            Module: self\n        \"\"\"\n        return self._apply(lambda t: t.type(dst_type))\n\n    def float(self: T) -> T:\n        r\"\"\"Casts all floating point parameters and buffers to ``float`` datatype.\n\n        .. note::\n            This method modifies the module in-place.\n\n        Returns:\n            Module: self\n        \"\"\"\n        return self._apply(lambda t: t.float() if t.is_floating_point() else t)\n\n    def double(self: T) -> T:\n        r\"\"\"Casts all floating point parameters and buffers to ``double`` datatype.\n\n        .. note::\n            This method modifies the module in-place.\n\n        Returns:\n            Module: self\n        \"\"\"\n        return self._apply(lambda t: t.double() if t.is_floating_point() else t)\n\n    def half(self: T) -> T:\n        r\"\"\"Casts all floating point parameters and buffers to ``half`` datatype.\n\n        .. note::\n            This method modifies the module in-place.\n\n        Returns:\n            Module: self\n        \"\"\"\n        return self._apply(lambda t: t.half() if t.is_floating_point() else t)\n\n    def bfloat16(self: T) -> T:\n        r\"\"\"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n        .. note::\n            This method modifies the module in-place.\n\n        Returns:\n            Module: self\n        \"\"\"\n        return self._apply(lambda t: t.bfloat16() if t.is_floating_point() else t)\n\n    def to_empty(self: T, *, device: Optional[DeviceLikeType], recurse: bool = True) -> T:\n        r\"\"\"Move the parameters and buffers to the specified device without copying storage.\n\n        Args:\n            device (:class:`torch.device`): The desired device of the parameters\n                and buffers in this module.\n            recurse (bool): Whether parameters and buffers of submodules should\n                be recursively moved to the specified device.\n\n        Returns:\n            Module: self\n        \"\"\"\n        return self._apply(lambda t: torch.empty_like(t, device=device), recurse=recurse)\n\n    @overload\n    def to(self, device: Optional[DeviceLikeType] = ..., dtype: Optional[Union[dtype, str]] = ...,\n           non_blocking: bool = ...) -> Self:\n        ...\n\n    @overload\n    def to(self, dtype: Union[dtype, str], non_blocking: bool = ...) -> Self:\n        ...\n\n    @overload\n    def to(self, tensor: Tensor, non_blocking: bool = ...) -> Self:\n        ...\n\n    def to(self, *args, **kwargs):\n        r\"\"\"Move and/or cast the parameters and buffers.\n\n        This can be called as\n\n        .. function:: to(device=None, dtype=None, non_blocking=False)\n           :noindex:\n\n        .. function:: to(dtype, non_blocking=False)\n           :noindex:\n\n        .. function:: to(tensor, non_blocking=False)\n           :noindex:\n\n        .. function:: to(memory_format=torch.channels_last)\n           :noindex:\n\n        Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n        floating point or complex :attr:`dtype`\\ s. In addition, this method will\n        only cast the floating point or complex parameters and buffers to :attr:`dtype`\n        (if given). The integral parameters and buffers will be moved\n        :attr:`device`, if that is given, but with dtypes unchanged. When\n        :attr:`non_blocking` is set, it tries to convert/move asynchronously\n        with respect to the host if possible, e.g., moving CPU Tensors with\n        pinned memory to CUDA devices.\n\n        See below for examples.\n\n        .. note::\n            This method modifies the module in-place.\n\n        Args:\n            device (:class:`torch.device`): the desired device of the parameters\n                and buffers in this module\n            dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n                the parameters and buffers in this module\n            tensor (torch.Tensor): Tensor whose dtype and device are the desired\n                dtype and device for all parameters and buffers in this module\n            memory_format (:class:`torch.memory_format`): the desired memory\n                format for 4D parameters and buffers in this module (keyword\n                only argument)\n\n        Returns:\n            Module: self\n\n        Examples::\n\n            >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n            >>> linear = nn.Linear(2, 2)\n            >>> linear.weight\n            Parameter containing:\n            tensor([[ 0.1913, -0.3420],\n                    [-0.5113, -0.2325]])\n            >>> linear.to(torch.double)\n            Linear(in_features=2, out_features=2, bias=True)\n            >>> linear.weight\n            Parameter containing:\n            tensor([[ 0.1913, -0.3420],\n                    [-0.5113, -0.2325]], dtype=torch.float64)\n            >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n            >>> gpu1 = torch.device(\"cuda:1\")\n            >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n            Linear(in_features=2, out_features=2, bias=True)\n            >>> linear.weight\n            Parameter containing:\n            tensor([[ 0.1914, -0.3420],\n                    [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n            >>> cpu = torch.device(\"cpu\")\n            >>> linear.to(cpu)\n            Linear(in_features=2, out_features=2, bias=True)\n            >>> linear.weight\n            Parameter containing:\n            tensor([[ 0.1914, -0.3420],\n                    [-0.5112, -0.2324]], dtype=torch.float16)\n\n            >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n            >>> linear.weight\n            Parameter containing:\n            tensor([[ 0.3741+0.j,  0.2382+0.j],\n                    [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n            >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n            tensor([[0.6122+0.j, 0.1150+0.j],\n                    [0.6122+0.j, 0.1150+0.j],\n                    [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n\n        \"\"\"\n        device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(*args, **kwargs)\n\n        if dtype is not None:\n            if not (dtype.is_floating_point or dtype.is_complex):\n                raise TypeError('nn.Module.to only accepts floating point or complex '\n                                f'dtypes, but got desired dtype={dtype}')\n            if dtype.is_complex:\n                warnings.warn(\n                    \"Complex modules are a new feature under active development whose design may change, \"\n                    \"and some modules might not work as expected when using complex tensors as parameters or buffers. \"\n                    \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml \"\n                    \"if a complex module does not work as expected.\")\n\n        def convert(t):\n            if convert_to_format is not None and t.dim() in (4, 5):\n                return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n                            non_blocking, memory_format=convert_to_format)\n            return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n\n        return self._apply(convert)\n\n    def register_full_backward_pre_hook(\n        self,\n        hook: Callable[[\"Module\", _grad_t], Union[None, _grad_t]],\n        prepend: bool = False,\n    ) -> RemovableHandle:\n        r\"\"\"Register a backward pre-hook on the module.\n\n        The hook will be called every time the gradients for the module are computed.\n        The hook should have the following signature::\n\n            hook(module, grad_output) -> tuple[Tensor] or None\n\n        The :attr:`grad_output` is a tuple. The hook should\n        not modify its arguments, but it can optionally return a new gradient with\n        respect to the output that will be used in place of :attr:`grad_output` in\n        subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n        all non-Tensor arguments.\n\n        For technical reasons, when this hook is applied to a Module, its forward function will\n        receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n        of each Tensor returned by the Module's forward function.\n\n        .. warning ::\n            Modifying inputs inplace is not allowed when using backward hooks and\n            will raise an error.\n\n        Args:\n            hook (Callable): The user-defined hook to be registered.\n            prepend (bool): If true, the provided ``hook`` will be fired before\n                all existing ``backward_pre`` hooks on this\n                :class:`torch.nn.modules.Module`. Otherwise, the provided\n                ``hook`` will be fired after all existing ``backward_pre`` hooks\n                on this :class:`torch.nn.modules.Module`. Note that global\n                ``backward_pre`` hooks registered with\n                :func:`register_module_full_backward_pre_hook` will fire before\n                all hooks registered by this method.\n\n        Returns:\n            :class:`torch.utils.hooks.RemovableHandle`:\n                a handle that can be used to remove the added hook by calling\n                ``handle.remove()``\n\n        \"\"\"\n        handle = hooks.RemovableHandle(self._backward_pre_hooks)\n        self._backward_pre_hooks[handle.id] = hook\n        if prepend:\n            self._backward_pre_hooks.move_to_end(handle.id, last=False)  # type: ignore[attr-defined]\n        return handle\n\n    def register_backward_hook(\n        self, hook: Callable[['Module', _grad_t, _grad_t], Union[None, _grad_t]]\n    ) -> RemovableHandle:\n        r\"\"\"Register a backward hook on the module.\n\n        This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n        the behavior of this function will change in future versions.\n\n        Returns:\n            :class:`torch.utils.hooks.RemovableHandle`:\n                a handle that can be used to remove the added hook by calling\n                ``handle.remove()``\n\n        \"\"\"\n        if self._is_full_backward_hook is True:\n            raise RuntimeError(\"Cannot use both regular backward hooks and full backward hooks on a \"\n                               \"single Module. Please use only one of them.\")\n\n        self._is_full_backward_hook = False\n\n        handle = hooks.RemovableHandle(self._backward_hooks)\n        self._backward_hooks[handle.id] = hook\n        return handle\n\n    def register_full_backward_hook(\n        self,\n        hook: Callable[[\"Module\", _grad_t, _grad_t], Union[None, _grad_t]],\n        prepend: bool = False,\n    ) -> RemovableHandle:\n        r\"\"\"Register a backward hook on the module.\n\n        The hook will be called every time the gradients with respect to a module\n        are computed, i.e. the hook will execute if and only if the gradients with\n        respect to module outputs are computed. The hook should have the following\n        signature::\n\n            hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n\n        The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n        with respect to the inputs and outputs respectively. The hook should\n        not modify its arguments, but it can optionally return a new gradient with\n        respect to the input that will be used in place of :attr:`grad_input` in\n        subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n        as positional arguments and all kwarg arguments are ignored. Entries\n        in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n        arguments.\n\n        For technical reasons, when this hook is applied to a Module, its forward function will\n        receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n        of each Tensor returned by the Module's forward function.\n\n        .. warning ::\n            Modifying inputs or outputs inplace is not allowed when using backward hooks and\n            will raise an error.\n\n        Args:\n            hook (Callable): The user-defined hook to be registered.\n            prepend (bool): If true, the provided ``hook`` will be fired before\n                all existing ``backward`` hooks on this\n                :class:`torch.nn.modules.Module`. Otherwise, the provided\n                ``hook`` will be fired after all existing ``backward`` hooks on\n                this :class:`torch.nn.modules.Module`. Note that global\n                ``backward`` hooks registered with\n                :func:`register_module_full_backward_hook` will fire before\n                all hooks registered by this method.\n\n        Returns:\n            :class:`torch.utils.hooks.RemovableHandle`:\n                a handle that can be used to remove the added hook by calling\n                ``handle.remove()``\n\n        \"\"\"\n        if self._is_full_backward_hook is False:\n            raise RuntimeError(\"Cannot use both regular backward hooks and full backward hooks on a \"\n                               \"single Module. Please use only one of them.\")\n\n        self._is_full_backward_hook = True\n\n        handle = hooks.RemovableHandle(self._backward_hooks)\n        self._backward_hooks[handle.id] = hook\n        if prepend:\n            self._backward_hooks.move_to_end(handle.id, last=False)  # type: ignore[attr-defined]\n        return handle\n\n    def _get_backward_hooks(self):\n        r\"\"\"Return the backward hooks for use in the call function.\n\n        It returns two lists, one with the full backward hooks and one with the non-full\n        backward hooks.\n        \"\"\"\n        full_backward_hooks: List[Callable] = []\n        if (_global_is_full_backward_hook is True):\n            full_backward_hooks += _global_backward_hooks.values()\n        if (self._is_full_backward_hook is True):\n            full_backward_hooks += self._backward_hooks.values()\n\n        non_full_backward_hooks: List[Callable] = []\n        if (_global_is_full_backward_hook is False):\n            non_full_backward_hooks += _global_backward_hooks.values()\n        if (self._is_full_backward_hook is False):\n            non_full_backward_hooks += self._backward_hooks.values()\n\n        return full_backward_hooks, non_full_backward_hooks\n\n    def _get_backward_pre_hooks(self):\n        backward_pre_hooks: List[Callable] = []\n        backward_pre_hooks += _global_backward_pre_hooks.values()\n        backward_pre_hooks += self._backward_pre_hooks.values()\n\n        return backward_pre_hooks\n\n    def _maybe_warn_non_full_backward_hook(self, inputs, result, grad_fn):\n        if not isinstance(result, torch.Tensor):\n            if not (isinstance(result, tuple) and all(isinstance(r, torch.Tensor) for r in result)):\n                warnings.warn(\"Using non-full backward hooks on a Module that does not return a \"\n                              \"single Tensor or a tuple of Tensors is deprecated and will be removed \"\n                              \"in future versions. This hook will be missing some of the grad_output. \"\n                              \"Please use register_full_backward_hook to get the documented behavior.\")\n                return\n        else:\n            result = (result,)\n\n        if not isinstance(inputs, torch.Tensor):\n            if not (isinstance(inputs, tuple) and all(isinstance(i, torch.Tensor) for i in inputs)):\n                warnings.warn(\"Using non-full backward hooks on a Module that does not take as input a \"\n                              \"single Tensor or a tuple of Tensors is deprecated and will be removed \"\n                              \"in future versions. This hook will be missing some of the grad_input. \"\n                              \"Please use register_full_backward_hook to get the documented behavior.\")\n                return\n        else:\n            inputs = (inputs,)\n\n        # At this point we are sure that inputs and result are tuple of Tensors\n        out_grad_fn = {r.grad_fn for r in result if r.grad_fn is not None}\n        if len(out_grad_fn) == 0 or (len(out_grad_fn) == 1 and grad_fn not in out_grad_fn):\n            warnings.warn(\"Using a non-full backward hook when outputs are nested in python data structure \"\n                          \"is deprecated and will be removed in future versions. This hook will be missing \"\n                          \"some grad_output.\")\n        elif len(out_grad_fn) > 1:\n            warnings.warn(\"Using a non-full backward hook when outputs are generated by different autograd Nodes \"\n                          \"is deprecated and will be removed in future versions. This hook will be missing \"\n                          \"some grad_output. Please use register_full_backward_hook to get the documented behavior.\")\n        else:\n            # At this point the grad_output part of the hook will most likely be correct\n            inputs_grad_fn = {i.grad_fn for i in inputs if i.grad_fn is not None}\n\n            next_functions = {n[0] for n in grad_fn.next_functions}\n\n            if inputs_grad_fn != next_functions:\n                warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n                              \"is deprecated and will be removed in future versions. This hook will be missing \"\n                              \"some grad_input. Please use register_full_backward_hook to get the documented \"\n                              \"behavior.\")\n\n    def register_forward_pre_hook(\n        self,\n        hook: Union[\n            Callable[[T, Tuple[Any, ...]], Optional[Any]],\n            Callable[[T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]],\n        ],\n        *,\n        prepend: bool = False,\n        with_kwargs: bool = False,\n    ) -> RemovableHandle:\n        r\"\"\"Register a forward pre-hook on the module.\n\n        The hook will be called every time before :func:`forward` is invoked.\n\n\n        If ``with_kwargs`` is false or not specified, the input contains only\n        the positional arguments given to the module. Keyword arguments won't be\n        passed to the hooks and only to the ``forward``. The hook can modify the\n        input. User can either return a tuple or a single modified value in the\n        hook. We will wrap the value into a tuple if a single value is returned\n        (unless that value is already a tuple). The hook should have the\n        following signature::\n\n            hook(module, args) -> None or modified input\n\n        If ``with_kwargs`` is true, the forward pre-hook will be passed the\n        kwargs given to the forward function. And if the hook modifies the\n        input, both the args and kwargs should be returned. The hook should have\n        the following signature::\n\n            hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n\n        Args:\n            hook (Callable): The user defined hook to be registered.\n            prepend (bool): If true, the provided ``hook`` will be fired before\n                all existing ``forward_pre`` hooks on this\n                :class:`torch.nn.modules.Module`. Otherwise, the provided\n                ``hook`` will be fired after all existing ``forward_pre`` hooks\n                on this :class:`torch.nn.modules.Module`. Note that global\n                ``forward_pre`` hooks registered with\n                :func:`register_module_forward_pre_hook` will fire before all\n                hooks registered by this method.\n                Default: ``False``\n            with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n                given to the forward function.\n                Default: ``False``\n\n        Returns:\n            :class:`torch.utils.hooks.RemovableHandle`:\n                a handle that can be used to remove the added hook by calling\n                ``handle.remove()``\n        \"\"\"\n        handle = hooks.RemovableHandle(\n            self._forward_pre_hooks,\n            extra_dict=self._forward_pre_hooks_with_kwargs\n        )\n        self._forward_pre_hooks[handle.id] = hook\n        if with_kwargs:\n            self._forward_pre_hooks_with_kwargs[handle.id] = True\n\n        if prepend:\n            self._forward_pre_hooks.move_to_end(handle.id, last=False)  # type: ignore[attr-defined]\n        return handle\n\n    def register_forward_hook(\n        self,\n        hook: Union[\n            Callable[[T, Tuple[Any, ...], Any], Optional[Any]],\n            Callable[[T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]],\n        ],\n        *,\n        prepend: bool = False,\n        with_kwargs: bool = False,\n        always_call: bool = False,\n    ) -> RemovableHandle:\n        r\"\"\"Register a forward hook on the module.\n\n        The hook will be called every time after :func:`forward` has computed an output.\n\n        If ``with_kwargs`` is ``False`` or not specified, the input contains only\n        the positional arguments given to the module. Keyword arguments won't be\n        passed to the hooks and only to the ``forward``. The hook can modify the\n        output. It can modify the input inplace but it will not have effect on\n        forward since this is called after :func:`forward` is called. The hook\n        should have the following signature::\n\n            hook(module, args, output) -> None or modified output\n\n        If ``with_kwargs`` is ``True``, the forward hook will be passed the\n        ``kwargs`` given to the forward function and be expected to return the\n        output possibly modified. The hook should have the following signature::\n\n            hook(module, args, kwargs, output) -> None or modified output\n\n        Args:\n            hook (Callable): The user defined hook to be registered.\n            prepend (bool): If ``True``, the provided ``hook`` will be fired\n                before all existing ``forward`` hooks on this\n                :class:`torch.nn.modules.Module`. Otherwise, the provided\n                ``hook`` will be fired after all existing ``forward`` hooks on\n                this :class:`torch.nn.modules.Module`. Note that global\n                ``forward`` hooks registered with\n                :func:`register_module_forward_hook` will fire before all hooks\n                registered by this method.\n                Default: ``False``\n            with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n                kwargs given to the forward function.\n                Default: ``False``\n            always_call (bool): If ``True`` the ``hook`` will be run regardless of\n                whether an exception is raised while calling the Module.\n                Default: ``False``\n\n        Returns:\n            :class:`torch.utils.hooks.RemovableHandle`:\n                a handle that can be used to remove the added hook by calling\n                ``handle.remove()``\n        \"\"\"\n        handle = hooks.RemovableHandle(\n            self._forward_hooks,\n            extra_dict=[self._forward_hooks_with_kwargs, self._forward_hooks_always_called],\n        )\n        self._forward_hooks[handle.id] = hook\n        if with_kwargs:\n            self._forward_hooks_with_kwargs[handle.id] = True\n        if always_call:\n            self._forward_hooks_always_called[handle.id] = True\n        if prepend:\n            self._forward_hooks.move_to_end(handle.id, last=False)  # type: ignore[attr-defined]\n        return handle\n\n    def _slow_forward(self, *input, **kwargs):\n        tracing_state = torch._C._get_tracing_state()\n        if not tracing_state or isinstance(self.forward, torch._C.ScriptMethod):\n            return self.forward(*input, **kwargs)\n        recording_scopes = torch.jit._trace._trace_module_map is not None\n        if recording_scopes:\n            # type ignore was added because at this point one knows that\n            # torch.jit._trace._trace_module_map is not Optional and has type Dict[Any, Any]\n            name = torch.jit._trace._trace_module_map[self] if self in torch.jit._trace._trace_module_map else None  # type: ignore[index, operator] # noqa: B950\n            if name:\n                tracing_state.push_scope(name)\n            else:\n                recording_scopes = False\n        try:\n            result = self.forward(*input, **kwargs)\n        finally:\n            if recording_scopes:\n                tracing_state.pop_scope()\n        return result\n\n    def _wrapped_call_impl(self, *args, **kwargs):\n        if self._compiled_call_impl is not None:\n            return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n        else:\n            return self._call_impl(*args, **kwargs)\n\n    def _call_impl(self, *args, **kwargs):\n        forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)\n        # If we don't have any hooks, we want to skip the rest of the logic in\n        # this function, and just call forward.\n        if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n                or _global_backward_pre_hooks or _global_backward_hooks\n                or _global_forward_hooks or _global_forward_pre_hooks):\n            return forward_call(*args, **kwargs)\n\n        try:\n            result = None\n            called_always_called_hooks = set()\n\n            full_backward_hooks, non_full_backward_hooks = [], []\n            backward_pre_hooks = []\n            if self._backward_pre_hooks or _global_backward_pre_hooks:\n                backward_pre_hooks = self._get_backward_pre_hooks()\n\n            if self._backward_hooks or _global_backward_hooks:\n                full_backward_hooks, non_full_backward_hooks = self._get_backward_hooks()\n\n            if _global_forward_pre_hooks or self._forward_pre_hooks:\n                for hook_id, hook in (\n                    *_global_forward_pre_hooks.items(),\n                    *self._forward_pre_hooks.items(),\n                ):\n                    if hook_id in self._forward_pre_hooks_with_kwargs:\n                        args_kwargs_result = hook(self, args, kwargs)  # type: ignore[misc]\n                        if args_kwargs_result is not None:\n                            if isinstance(args_kwargs_result, tuple) and len(args_kwargs_result) == 2:\n                                args, kwargs = args_kwargs_result\n                            else:\n                                raise RuntimeError(\n                                    \"forward pre-hook must return None or a tuple \"\n                                    f\"of (new_args, new_kwargs), but got {args_kwargs_result}.\"\n                                )\n                    else:\n                        args_result = hook(self, args)\n                        if args_result is not None:\n                            if not isinstance(args_result, tuple):\n                                args_result = (args_result,)\n                            args = args_result\n\n            bw_hook = None\n            if full_backward_hooks or backward_pre_hooks:\n                bw_hook = hooks.BackwardHook(self, full_backward_hooks, backward_pre_hooks)\n                args = bw_hook.setup_input_hook(args)\n\n            result = forward_call(*args, **kwargs)\n            if _global_forward_hooks or self._forward_hooks:\n                for hook_id, hook in (\n                    *_global_forward_hooks.items(),\n                    *self._forward_hooks.items(),\n                ):\n                    # mark that always called hook is run\n                    if hook_id in self._forward_hooks_always_called or hook_id in _global_forward_hooks_always_called:\n                        called_always_called_hooks.add(hook_id)\n\n                    if hook_id in self._forward_hooks_with_kwargs:\n                        hook_result = hook(self, args, kwargs, result)\n                    else:\n                        hook_result = hook(self, args, result)\n\n                    if hook_result is not None:\n                        result = hook_result\n\n            if bw_hook:\n                if not isinstance(result, (torch.Tensor, tuple)):\n                    warnings.warn(\"For backward hooks to be called,\"\n                                  \" module output should be a Tensor or a tuple of Tensors\"\n                                  f\" but received {type(result)}\")\n                result = bw_hook.setup_output_hook(result)\n\n            # Handle the non-full backward hooks\n            if non_full_backward_hooks:\n                var = result\n                while not isinstance(var, torch.Tensor):\n                    if isinstance(var, dict):\n                        var = next(v for v in var.values() if isinstance(v, torch.Tensor))\n                    else:\n                        var = var[0]\n                grad_fn = var.grad_fn\n                if grad_fn is not None:\n                    for hook in non_full_backward_hooks:\n                        grad_fn.register_hook(_WrappedHook(hook, self))\n                    self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n\n            return result\n\n        except Exception:\n            # run always called hooks if they have not already been run\n            # For now only forward hooks have the always_call option but perhaps\n            # this functionality should be added to full backward hooks as well.\n            for hook_id, hook in _global_forward_hooks.items():\n                if hook_id in _global_forward_hooks_always_called and hook_id not in called_always_called_hooks:\n                    try:\n                        hook_result = hook(self, args, result)\n                        if hook_result is not None:\n                            result = hook_result\n                    except Exception as e:\n                        warnings.warn(\"global module forward hook with ``always_call=True`` raised an exception \"\n                                      f\"that was silenced as another error was raised in forward: {str(e)}\")\n                        continue\n\n            for hook_id, hook in self._forward_hooks.items():\n                if hook_id in self._forward_hooks_always_called and hook_id not in called_always_called_hooks:\n                    try:\n                        if hook_id in self._forward_hooks_with_kwargs:\n                            hook_result = hook(self, args, kwargs, result)\n                        else:\n                            hook_result = hook(self, args, result)\n                        if hook_result is not None:\n                            result = hook_result\n                    except Exception as e:\n                        warnings.warn(\"module forward hook with ``always_call=True`` raised an exception \"\n                                      f\"that was silenced as another error was raised in forward: {str(e)}\")\n                        continue\n            # raise exception raised in try block\n            raise\n\n\n    __call__ : Callable[..., Any] = _wrapped_call_impl\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        state.pop(\"_compiled_call_impl\", None)\n        return state\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n\n        # Support loading old checkpoints that don't have the following attrs:\n        if '_forward_pre_hooks' not in self.__dict__:\n            self._forward_pre_hooks = OrderedDict()\n        if '_forward_pre_hooks_with_kwargs' not in self.__dict__:\n            self._forward_pre_hooks_with_kwargs = OrderedDict()\n        if '_forward_hooks_with_kwargs' not in self.__dict__:\n            self._forward_hooks_with_kwargs = OrderedDict()\n        if '_forward_hooks_always_called' not in self.__dict__:\n            self._forward_hooks_always_called = OrderedDict()\n        if '_state_dict_hooks' not in self.__dict__:\n            self._state_dict_hooks = OrderedDict()\n        if '_state_dict_pre_hooks' not in self.__dict__:\n            self._state_dict_pre_hooks = OrderedDict()\n        if '_load_state_dict_pre_hooks' not in self.__dict__:\n            self._load_state_dict_pre_hooks = OrderedDict()\n        if '_load_state_dict_post_hooks' not in self.__dict__:\n            self._load_state_dict_post_hooks = OrderedDict()\n        if '_non_persistent_buffers_set' not in self.__dict__:\n            self._non_persistent_buffers_set = set()\n        if '_is_full_backward_hook' not in self.__dict__:\n            self._is_full_backward_hook = None\n        if '_backward_pre_hooks' not in self.__dict__:\n            self._backward_pre_hooks = OrderedDict()\n\n    # On the return type:\n    # We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\n    # This is done for better interop with various type checkers for the end users.\n    # Having a stricter return type doesn't play nicely with `register_buffer()` and forces\n    # people to excessively use type-ignores, asserts, casts, etc.\n    # See full discussion on the problems with returning `Union` here\n    # https://github.com/microsoft/pyright/issues/4213\n    def __getattr__(self, name: str) -> Any:\n        if '_parameters' in self.__dict__:\n            _parameters = self.__dict__['_parameters']\n            if name in _parameters:\n                return _parameters[name]\n        if '_buffers' in self.__dict__:\n            _buffers = self.__dict__['_buffers']\n            if name in _buffers:\n                return _buffers[name]\n        if '_modules' in self.__dict__:\n            modules = self.__dict__['_modules']\n            if name in modules:\n                return modules[name]\n        raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n\n    def __setattr__(self, name: str, value: Union[Tensor, 'Module']) -> None:\n        def remove_from(*dicts_or_sets):\n            for d in dicts_or_sets:\n                if name in d:\n                    if isinstance(d, dict):\n                        del d[name]\n                    else:\n                        d.discard(name)\n\n        params = self.__dict__.get('_parameters')\n        if isinstance(value, Parameter):\n            if params is None:\n                raise AttributeError(\n                    \"cannot assign parameters before Module.__init__() call\")\n            remove_from(self.__dict__, self._buffers, self._modules, self._non_persistent_buffers_set)\n            self.register_parameter(name, value)\n        elif params is not None and name in params:\n            if value is not None:\n                raise TypeError(f\"cannot assign '{torch.typename(value)}' as parameter '{name}' \"\n                                \"(torch.nn.Parameter or None expected)\"\n                                )\n            self.register_parameter(name, value)\n        else:\n            modules = self.__dict__.get('_modules')\n            if isinstance(value, Module):\n                if modules is None:\n                    raise AttributeError(\n                        \"cannot assign module before Module.__init__() call\")\n                remove_from(self.__dict__, self._parameters, self._buffers, self._non_persistent_buffers_set)\n                for hook in _global_module_registration_hooks.values():\n                    output = hook(self, name, value)\n                    if output is not None:\n                        value = output\n                modules[name] = value\n            elif modules is not None and name in modules:\n                if value is not None:\n                    raise TypeError(f\"cannot assign '{torch.typename(value)}' as child module '{name}' \"\n                                    \"(torch.nn.Module or None expected)\"\n                                    )\n                for hook in _global_module_registration_hooks.values():\n                    output = hook(self, name, value)\n                    if output is not None:\n                        value = output\n                modules[name] = value\n            else:\n                buffers = self.__dict__.get('_buffers')\n                if buffers is not None and name in buffers:\n                    if value is not None and not isinstance(value, torch.Tensor):\n                        raise TypeError(f\"cannot assign '{torch.typename(value)}' as buffer '{name}' \"\n                                        \"(torch.Tensor or None expected)\"\n                                        )\n                    for hook in _global_buffer_registration_hooks.values():\n                        output = hook(self, name, value)\n                        if output is not None:\n                            value = output\n                    buffers[name] = value\n                else:\n                    super().__setattr__(name, value)\n\n    def __delattr__(self, name):\n        if name in self._parameters:\n            del self._parameters[name]\n        elif name in self._buffers:\n            del self._buffers[name]\n            self._non_persistent_buffers_set.discard(name)\n        elif name in self._modules:\n            del self._modules[name]\n        else:\n            super().__delattr__(name)\n\n    def _register_state_dict_hook(self, hook):\n        r\"\"\"Register a state-dict hook.\n\n        These hooks will be called with arguments: `self`, `state_dict`,\n        `prefix`, `local_metadata`, after the `state_dict` of `self` is set.\n        Note that only parameters and buffers of `self` or its children are\n        guaranteed to exist in `state_dict`. The hooks may modify `state_dict`\n        inplace or return a new one.\n        \"\"\"\n        handle = hooks.RemovableHandle(self._state_dict_hooks)\n        self._state_dict_hooks[handle.id] = hook\n        return handle\n\n    def register_state_dict_pre_hook(self, hook):\n        r\"\"\"Register a pre-hook for the :meth:`~torch.nn.Module.load_state_dict` method.\n\n        These hooks will be called with arguments: ``self``, ``prefix``,\n        and ``keep_vars`` before calling ``state_dict`` on ``self``. The registered\n        hooks can be used to perform pre-processing before the ``state_dict``\n        call is made.\n        \"\"\"\n        handle = hooks.RemovableHandle(self._state_dict_pre_hooks)\n        self._state_dict_pre_hooks[handle.id] = hook\n        return handle\n\n    def _save_to_state_dict(self, destination, prefix, keep_vars):\n        r\"\"\"Save module state to the `destination` dictionary.\n\n        The `destination` dictionary will contain the state\n        of the module, but not its descendants. This is called on every\n        submodule in :meth:`~torch.nn.Module.state_dict`.\n\n        In rare cases, subclasses can achieve class-specific behavior by\n        overriding this method with custom logic.\n\n        Args:\n            destination (dict): a dict where state will be stored\n            prefix (str): the prefix for parameters and buffers used in this\n                module\n        \"\"\"\n        for name, param in self._parameters.items():\n            if param is not None:\n                destination[prefix + name] = param if keep_vars else param.detach()\n        for name, buf in self._buffers.items():\n            if buf is not None and name not in self._non_persistent_buffers_set:\n                destination[prefix + name] = buf if keep_vars else buf.detach()\n        extra_state_key = prefix + _EXTRA_STATE_KEY_SUFFIX\n        if getattr(self.__class__, \"get_extra_state\", Module.get_extra_state) is not Module.get_extra_state:\n            destination[extra_state_key] = self.get_extra_state()\n\n    # The user can pass an optional arbitrary mappable object to `state_dict`, in which case `state_dict` returns\n    # back that same object. But if they pass nothing, an `OrderedDict` is created and returned.\n    T_destination = TypeVar('T_destination', bound=Dict[str, Any])\n\n    @overload\n    def state_dict(self, *, destination: T_destination, prefix: str = ..., keep_vars: bool = ...) -> T_destination:\n        ...\n\n    @overload\n    def state_dict(self, *, prefix: str = ..., keep_vars: bool = ...) -> Dict[str, Any]:\n        ...\n\n    # TODO: Change `*args` to `*` and remove the corresponding warning in docs when BC allows.\n    # Also remove the logic for arg parsing together.\n    def state_dict(self, *args, destination=None, prefix='', keep_vars=False):\n        r\"\"\"Return a dictionary containing references to the whole state of the module.\n\n        Both parameters and persistent buffers (e.g. running averages) are\n        included. Keys are corresponding parameter and buffer names.\n        Parameters and buffers set to ``None`` are not included.\n\n        .. note::\n            The returned object is a shallow copy. It contains references\n            to the module's parameters and buffers.\n\n        .. warning::\n            Currently ``state_dict()`` also accepts positional arguments for\n            ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n            this is being deprecated and keyword arguments will be enforced in\n            future releases.\n\n        .. warning::\n            Please avoid the use of argument ``destination`` as it is not\n            designed for end-users.\n\n        Args:\n            destination (dict, optional): If provided, the state of module will\n                be updated into the dict and the same object is returned.\n                Otherwise, an ``OrderedDict`` will be created and returned.\n                Default: ``None``.\n            prefix (str, optional): a prefix added to parameter and buffer\n                names to compose the keys in state_dict. Default: ``''``.\n            keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n                returned in the state dict are detached from autograd. If it's\n                set to ``True``, detaching will not be performed.\n                Default: ``False``.\n\n        Returns:\n            dict:\n                a dictionary containing a whole state of the module\n\n        Example::\n\n            >>> # xdoctest: +SKIP(\"undefined vars\")\n            >>> module.state_dict().keys()\n            ['bias', 'weight']\n\n        \"\"\"\n        # TODO: Remove `args` and the parsing logic when BC allows.\n        if len(args) > 0:\n            if destination is None:\n                destination = args[0]\n            if len(args) > 1 and prefix == '':\n                prefix = args[1]\n            if len(args) > 2 and keep_vars is False:\n                keep_vars = args[2]\n            # DeprecationWarning is ignored by default\n            warnings.warn(\n                \"Positional args are being deprecated, use kwargs instead. Refer to \"\n                \"https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict\"\n                \" for details.\")\n\n        if destination is None:\n            destination = OrderedDict()\n            destination._metadata = OrderedDict()\n\n        local_metadata = dict(version=self._version)\n        if hasattr(destination, \"_metadata\"):\n            destination._metadata[prefix[:-1]] = local_metadata\n\n        for hook in self._state_dict_pre_hooks.values():\n            hook(self, prefix, keep_vars)\n        self._save_to_state_dict(destination, prefix, keep_vars)\n        for name, module in self._modules.items():\n            if module is not None:\n                module.state_dict(destination=destination, prefix=prefix + name + '.', keep_vars=keep_vars)\n        for hook in self._state_dict_hooks.values():\n            hook_result = hook(self, destination, prefix, local_metadata)\n            if hook_result is not None:\n                destination = hook_result\n        return destination\n\n    def _register_load_state_dict_pre_hook(self, hook, with_module=False):\n        r\"\"\"Register a pre-hook for the :meth:`~torch.nn.Module.load_state_dict` method.\n\n        These hooks will be called with arguments: `state_dict`, `prefix`,\n        `local_metadata`, `strict`, `missing_keys`, `unexpected_keys`,\n        `error_msgs`, before loading `state_dict` into `self`. These arguments\n        are exactly the same as those of `_load_from_state_dict`.\n\n        If ``with_module`` is ``True``, then the first argument to the hook is\n        an instance of the module.\n\n        Arguments:\n            hook (Callable): Callable hook that will be invoked before\n                loading the state dict.\n            with_module (bool, optional): Whether or not to pass the module\n                instance to the hook as the first parameter.\n        \"\"\"\n        handle = hooks.RemovableHandle(self._load_state_dict_pre_hooks)\n        self._load_state_dict_pre_hooks[handle.id] = _WrappedHook(hook, self if with_module else None)\n        return handle\n\n    def register_load_state_dict_post_hook(self, hook):\n        r\"\"\"Register a post hook to be run after module's ``load_state_dict`` is called.\n\n        It should have the following signature::\n            hook(module, incompatible_keys) -> None\n\n        The ``module`` argument is the current module that this hook is registered\n        on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n        of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n        is a ``list`` of ``str`` containing the missing keys and\n        ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\n        The given incompatible_keys can be modified inplace if needed.\n\n        Note that the checks performed when calling :func:`load_state_dict` with\n        ``strict=True`` are affected by modifications the hook makes to\n        ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n        set of keys will result in an error being thrown when ``strict=True``, and\n        clearing out both missing and unexpected keys will avoid an error.\n\n        Returns:\n            :class:`torch.utils.hooks.RemovableHandle`:\n                a handle that can be used to remove the added hook by calling\n                ``handle.remove()``\n        \"\"\"\n        handle = hooks.RemovableHandle(self._load_state_dict_post_hooks)\n        self._load_state_dict_post_hooks[handle.id] = hook\n        return handle\n\n\n    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n                              missing_keys, unexpected_keys, error_msgs):\n        r\"\"\"Copy parameters and buffers from :attr:`state_dict` into only this module, but not its descendants.\n\n        This is called on every submodule\n        in :meth:`~torch.nn.Module.load_state_dict`. Metadata saved for this\n        module in input :attr:`state_dict` is provided as :attr:`local_metadata`.\n        For state dicts without metadata, :attr:`local_metadata` is empty.\n        Subclasses can achieve class-specific backward compatible loading using\n        the version number at `local_metadata.get(\"version\", None)`.\n        Additionally, :attr:`local_metadata` can also contain the key\n        `assign_to_params_buffers` that indicates whether keys should be\n        assigned their corresponding tensor in the state_dict.\n\n        .. note::\n            :attr:`state_dict` is not the same object as the input\n            :attr:`state_dict` to :meth:`~torch.nn.Module.load_state_dict`. So\n            it can be modified.\n\n        Args:\n            state_dict (dict): a dict containing parameters and\n                persistent buffers.\n            prefix (str): the prefix for parameters and buffers used in this\n                module\n            local_metadata (dict): a dict containing the metadata for this module.\n                See\n            strict (bool): whether to strictly enforce that the keys in\n                :attr:`state_dict` with :attr:`prefix` match the names of\n                parameters and buffers in this module\n            missing_keys (list of str): if ``strict=True``, add missing keys to\n                this list\n            unexpected_keys (list of str): if ``strict=True``, add unexpected\n                keys to this list\n            error_msgs (list of str): error messages should be added to this\n                list, and will be reported together in\n                :meth:`~torch.nn.Module.load_state_dict`\n        \"\"\"\n        for hook in self._load_state_dict_pre_hooks.values():\n            hook(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\n\n        persistent_buffers = {k: v for k, v in self._buffers.items() if k not in self._non_persistent_buffers_set}\n        local_name_params = itertools.chain(self._parameters.items(), persistent_buffers.items())\n        local_state = {k: v for k, v in local_name_params if v is not None}\n        assign_to_params_buffers = local_metadata.get(\"assign_to_params_buffers\", False)\n\n        for name, param in local_state.items():\n            key = prefix + name\n            if key in state_dict:\n                input_param = state_dict[key]\n                if not torch.overrides.is_tensor_like(input_param):\n                    error_msgs.append(f'While copying the parameter named \"{key}\", '\n                                      'expected torch.Tensor or Tensor-like object from checkpoint but '\n                                      f'received {type(input_param)}'\n                                      )\n                    continue\n\n                # This is used to avoid copying uninitialized parameters into\n                # non-lazy modules, since they dont have the hook to do the checks\n                # in such case, it will error when accessing the .shape attribute.\n                is_param_lazy = torch.nn.parameter.is_lazy(param)\n                # Backward compatibility: loading 1-dim tensor from 0.3.* to version 0.4+\n                if not is_param_lazy and len(param.shape) == 0 and len(input_param.shape) == 1:\n                    input_param = input_param[0]\n\n                if not is_param_lazy and input_param.shape != param.shape:\n                    # local shape should match the one in checkpoint\n                    error_msgs.append('size mismatch for {}: copying a param with shape {} from checkpoint, '\n                                      'the shape in current model is {}.'\n                                      .format(key, input_param.shape, param.shape))\n                    continue\n\n                if param.is_meta and not input_param.is_meta and not assign_to_params_buffers:\n                    warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n                                  'parameter in the current model, which is a no-op. (Did you mean to '\n                                  'pass `assign=True` to assign items in the state dictionary to their '\n                                  'corresponding key in the module instead of copying them in place?)')\n\n                try:\n                    with torch.no_grad():\n                        if assign_to_params_buffers:\n                            # Shape checks are already done above\n                            if (isinstance(param, torch.nn.Parameter) and\n                                    not isinstance(input_param, torch.nn.Parameter)):\n                                setattr(self, name, torch.nn.Parameter(input_param))\n                            else:\n                                setattr(self, name, input_param)\n                        else:\n                            param.copy_(input_param)\n                except Exception as ex:\n                    error_msgs.append(f'While copying the parameter named \"{key}\", '\n                                      f'whose dimensions in the model are {param.size()} and '\n                                      f'whose dimensions in the checkpoint are {input_param.size()}, '\n                                      f'an exception occurred : {ex.args}.'\n                                      )\n            elif strict:\n                missing_keys.append(key)\n\n        extra_state_key = prefix + _EXTRA_STATE_KEY_SUFFIX\n        if getattr(self.__class__, \"set_extra_state\", Module.set_extra_state) is not Module.set_extra_state:\n            if extra_state_key in state_dict:\n                self.set_extra_state(state_dict[extra_state_key])\n            elif strict:\n                missing_keys.append(extra_state_key)\n        elif strict and (extra_state_key in state_dict):\n            unexpected_keys.append(extra_state_key)\n\n        if strict:\n            for key in state_dict.keys():\n                if key.startswith(prefix) and key != extra_state_key:\n                    input_name = key[len(prefix):]\n                    input_name = input_name.split('.', 1)[0]  # get the name of param/buffer/child\n                    if input_name not in self._modules and input_name not in local_state:\n                        unexpected_keys.append(key)\n\n    def load_state_dict(self, state_dict: Mapping[str, Any],\n                        strict: bool = True, assign: bool = False):\n        r\"\"\"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\n        If :attr:`strict` is ``True``, then\n        the keys of :attr:`state_dict` must exactly match the keys returned\n        by this module's :meth:`~torch.nn.Module.state_dict` function.\n\n        .. warning::\n            If :attr:`assign` is ``True`` the optimizer must be created after\n            the call to :attr:`load_state_dict`.\n\n        Args:\n            state_dict (dict): a dict containing parameters and\n                persistent buffers.\n            strict (bool, optional): whether to strictly enforce that the keys\n                in :attr:`state_dict` match the keys returned by this module's\n                :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n            assign (bool, optional): whether to assign items in the state\n                dictionary to their corresponding keys in the module instead\n                of copying them inplace into the module's current parameters and buffers.\n                When ``False``, the properties of the tensors in the current\n                module are preserved while when ``True``, the properties of the\n                Tensors in the state dict are preserved.\n                Default: ``False``\n\n        Returns:\n            ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n                * **missing_keys** is a list of str containing the missing keys\n                * **unexpected_keys** is a list of str containing the unexpected keys\n\n        Note:\n            If a parameter or buffer is registered as ``None`` and its corresponding key\n            exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n            ``RuntimeError``.\n        \"\"\"\n        if not isinstance(state_dict, Mapping):\n            raise TypeError(f\"Expected state_dict to be dict-like, got {type(state_dict)}.\")\n\n        missing_keys: List[str] = []\n        unexpected_keys: List[str] = []\n        error_msgs: List[str] = []\n\n        # copy state_dict so _load_from_state_dict can modify it\n        metadata = getattr(state_dict, '_metadata', None)\n        state_dict = OrderedDict(state_dict)\n        if metadata is not None:\n            # mypy isn't aware that \"_metadata\" exists in state_dict\n            state_dict._metadata = metadata  # type: ignore[attr-defined]\n\n        def load(module, local_state_dict, prefix=''):\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n            if assign:\n                local_metadata['assign_to_params_buffers'] = assign\n            module._load_from_state_dict(\n                local_state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n            for name, child in module._modules.items():\n                if child is not None:\n                    child_prefix = prefix + name + '.'\n                    child_state_dict = {k: v for k, v in local_state_dict.items() if k.startswith(child_prefix)}\n                    load(child, child_state_dict, child_prefix)\n\n            # Note that the hook can modify missing_keys and unexpected_keys.\n            incompatible_keys = _IncompatibleKeys(missing_keys, unexpected_keys)\n            for hook in module._load_state_dict_post_hooks.values():\n                out = hook(module, incompatible_keys)\n                assert out is None, (\n                    \"Hooks registered with ``register_load_state_dict_post_hook`` are not\"\n                    \"expected to return new values, if incompatible_keys need to be modified,\"\n                    \"it should be done inplace.\"\n                )\n\n        load(self, state_dict)\n        del load\n\n        if strict:\n            if len(unexpected_keys) > 0:\n                error_msgs.insert(\n                    0, 'Unexpected key(s) in state_dict: {}. '.format(\n                        ', '.join(f'\"{k}\"' for k in unexpected_keys)))\n            if len(missing_keys) > 0:\n                error_msgs.insert(\n                    0, 'Missing key(s) in state_dict: {}. '.format(\n                        ', '.join(f'\"{k}\"' for k in missing_keys)))\n\n        if len(error_msgs) > 0:\n            raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n                               self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n        return _IncompatibleKeys(missing_keys, unexpected_keys)\n\n    def _named_members(self, get_members_fn, prefix='', recurse=True, remove_duplicate: bool = True):\n        r\"\"\"Help yield various names + members of modules.\"\"\"\n        memo = set()\n        modules = self.named_modules(prefix=prefix, remove_duplicate=remove_duplicate) if recurse else [(prefix, self)]\n        for module_prefix, module in modules:\n            members = get_members_fn(module)\n            for k, v in members:\n                if v is None or v in memo:\n                    continue\n                if remove_duplicate:\n                    memo.add(v)\n                name = module_prefix + ('.' if module_prefix else '') + k\n                yield name, v\n\n    def parameters(self, recurse: bool = True) -> Iterator[Parameter]:\n        r\"\"\"Return an iterator over module parameters.\n\n        This is typically passed to an optimizer.\n\n        Args:\n            recurse (bool): if True, then yields parameters of this module\n                and all submodules. Otherwise, yields only parameters that\n                are direct members of this module.\n\n        Yields:\n            Parameter: module parameter\n\n        Example::\n\n            >>> # xdoctest: +SKIP(\"undefined vars\")\n            >>> for param in model.parameters():\n            >>>     print(type(param), param.size())\n            <class 'torch.Tensor'> (20L,)\n            <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n\n        \"\"\"\n        for name, param in self.named_parameters(recurse=recurse):\n            yield param\n\n    def named_parameters(\n            self,\n            prefix: str = '',\n            recurse: bool = True,\n            remove_duplicate: bool = True\n    ) -> Iterator[Tuple[str, Parameter]]:\n        r\"\"\"Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\n\n        Args:\n            prefix (str): prefix to prepend to all parameter names.\n            recurse (bool): if True, then yields parameters of this module\n                and all submodules. Otherwise, yields only parameters that\n                are direct members of this module.\n            remove_duplicate (bool, optional): whether to remove the duplicated\n                parameters in the result. Defaults to True.\n\n        Yields:\n            (str, Parameter): Tuple containing the name and parameter\n\n        Example::\n\n            >>> # xdoctest: +SKIP(\"undefined vars\")\n            >>> for name, param in self.named_parameters():\n            >>>     if name in ['bias']:\n            >>>         print(param.size())\n\n        \"\"\"\n        gen = self._named_members(\n            lambda module: module._parameters.items(),\n            prefix=prefix, recurse=recurse, remove_duplicate=remove_duplicate)\n        yield from gen\n\n    def buffers(self, recurse: bool = True) -> Iterator[Tensor]:\n        r\"\"\"Return an iterator over module buffers.\n\n        Args:\n            recurse (bool): if True, then yields buffers of this module\n                and all submodules. Otherwise, yields only buffers that\n                are direct members of this module.\n\n        Yields:\n            torch.Tensor: module buffer\n\n        Example::\n\n            >>> # xdoctest: +SKIP(\"undefined vars\")\n            >>> for buf in model.buffers():\n            >>>     print(type(buf), buf.size())\n            <class 'torch.Tensor'> (20L,)\n            <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n\n        \"\"\"\n        for _, buf in self.named_buffers(recurse=recurse):\n            yield buf\n\n    def named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, Tensor]]:\n        r\"\"\"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\n\n        Args:\n            prefix (str): prefix to prepend to all buffer names.\n            recurse (bool, optional): if True, then yields buffers of this module\n                and all submodules. Otherwise, yields only buffers that\n                are direct members of this module. Defaults to True.\n            remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n\n        Yields:\n            (str, torch.Tensor): Tuple containing the name and buffer\n\n        Example::\n\n            >>> # xdoctest: +SKIP(\"undefined vars\")\n            >>> for name, buf in self.named_buffers():\n            >>>     if name in ['running_var']:\n            >>>         print(buf.size())\n\n        \"\"\"\n        gen = self._named_members(\n            lambda module: module._buffers.items(),\n            prefix=prefix, recurse=recurse, remove_duplicate=remove_duplicate)\n        yield from gen\n\n    def children(self) -> Iterator['Module']:\n        r\"\"\"Return an iterator over immediate children modules.\n\n        Yields:\n            Module: a child module\n        \"\"\"\n        for name, module in self.named_children():\n            yield module\n\n    def named_children(self) -> Iterator[Tuple[str, 'Module']]:\n        r\"\"\"Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n\n        Yields:\n            (str, Module): Tuple containing a name and child module\n\n        Example::\n\n            >>> # xdoctest: +SKIP(\"undefined vars\")\n            >>> for name, module in model.named_children():\n            >>>     if name in ['conv4', 'conv5']:\n            >>>         print(module)\n\n        \"\"\"\n        memo = set()\n        for name, module in self._modules.items():\n            if module is not None and module not in memo:\n                memo.add(module)\n                yield name, module\n\n    def modules(self) -> Iterator['Module']:\n        r\"\"\"Return an iterator over all modules in the network.\n\n        Yields:\n            Module: a module in the network\n\n        Note:\n            Duplicate modules are returned only once. In the following\n            example, ``l`` will be returned only once.\n\n        Example::\n\n            >>> l = nn.Linear(2, 2)\n            >>> net = nn.Sequential(l, l)\n            >>> for idx, m in enumerate(net.modules()):\n            ...     print(idx, '->', m)\n\n            0 -> Sequential(\n              (0): Linear(in_features=2, out_features=2, bias=True)\n              (1): Linear(in_features=2, out_features=2, bias=True)\n            )\n            1 -> Linear(in_features=2, out_features=2, bias=True)\n\n        \"\"\"\n        for _, module in self.named_modules():\n            yield module\n\n    def named_modules(self, memo: Optional[Set['Module']] = None, prefix: str = '', remove_duplicate: bool = True):\n        r\"\"\"Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\n\n        Args:\n            memo: a memo to store the set of modules already added to the result\n            prefix: a prefix that will be added to the name of the module\n            remove_duplicate: whether to remove the duplicated module instances in the result\n                or not\n\n        Yields:\n            (str, Module): Tuple of name and module\n\n        Note:\n            Duplicate modules are returned only once. In the following\n            example, ``l`` will be returned only once.\n\n        Example::\n\n            >>> l = nn.Linear(2, 2)\n            >>> net = nn.Sequential(l, l)\n            >>> for idx, m in enumerate(net.named_modules()):\n            ...     print(idx, '->', m)\n\n            0 -> ('', Sequential(\n              (0): Linear(in_features=2, out_features=2, bias=True)\n              (1): Linear(in_features=2, out_features=2, bias=True)\n            ))\n            1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n\n        \"\"\"\n        if memo is None:\n            memo = set()\n        if self not in memo:\n            if remove_duplicate:\n                memo.add(self)\n            yield prefix, self\n            for name, module in self._modules.items():\n                if module is None:\n                    continue\n                submodule_prefix = prefix + ('.' if prefix else '') + name\n                yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n\n    def train(self: T, mode: bool = True) -> T:\n        r\"\"\"Set the module in training mode.\n\n        This has any effect only on certain modules. See documentations of\n        particular modules for details of their behaviors in training/evaluation\n        mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n        etc.\n\n        Args:\n            mode (bool): whether to set training mode (``True``) or evaluation\n                         mode (``False``). Default: ``True``.\n\n        Returns:\n            Module: self\n        \"\"\"\n        if not isinstance(mode, bool):\n            raise ValueError(\"training mode is expected to be boolean\")\n        self.training = mode\n        for module in self.children():\n            module.train(mode)\n        return self\n\n    def eval(self: T) -> T:\n        r\"\"\"Set the module in evaluation mode.\n\n        This has any effect only on certain modules. See documentations of\n        particular modules for details of their behaviors in training/evaluation\n        mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n        etc.\n\n        This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\n        See :ref:`locally-disable-grad-doc` for a comparison between\n        `.eval()` and several similar mechanisms that may be confused with it.\n\n        Returns:\n            Module: self\n        \"\"\"\n        return self.train(False)\n\n    def requires_grad_(self: T, requires_grad: bool = True) -> T:\n        r\"\"\"Change if autograd should record operations on parameters in this module.\n\n        This method sets the parameters' :attr:`requires_grad` attributes\n        in-place.\n\n        This method is helpful for freezing part of the module for finetuning\n        or training parts of a model individually (e.g., GAN training).\n\n        See :ref:`locally-disable-grad-doc` for a comparison between\n        `.requires_grad_()` and several similar mechanisms that may be confused with it.\n\n        Args:\n            requires_grad (bool): whether autograd should record operations on\n                                  parameters in this module. Default: ``True``.\n\n        Returns:\n            Module: self\n        \"\"\"\n        for p in self.parameters():\n            p.requires_grad_(requires_grad)\n        return self\n\n    def zero_grad(self, set_to_none: bool = True) -> None:\n        r\"\"\"Reset gradients of all model parameters.\n\n        See similar function under :class:`torch.optim.Optimizer` for more context.\n\n        Args:\n            set_to_none (bool): instead of setting to zero, set the grads to None.\n                See :meth:`torch.optim.Optimizer.zero_grad` for details.\n        \"\"\"\n        if getattr(self, '_is_replica', False):\n            warnings.warn(\n                \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \"\n                \"The parameters are copied (in a differentiable manner) from the original module. \"\n                \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \"\n                \"If you need gradients in your forward method, consider using autograd.grad instead.\")\n\n        for p in self.parameters():\n            if p.grad is not None:\n                if set_to_none:\n                    p.grad = None\n                else:\n                    if p.grad.grad_fn is not None:\n                        p.grad.detach_()\n                    else:\n                        p.grad.requires_grad_(False)\n                    p.grad.zero_()\n\n    def share_memory(self: T) -> T:\n        r\"\"\"See :meth:`torch.Tensor.share_memory_`.\"\"\"\n        return self._apply(lambda t: t.share_memory_())\n\n    def _get_name(self):\n        return self.__class__.__name__\n\n    def extra_repr(self) -> str:\n        r\"\"\"Set the extra representation of the module.\n\n        To print customized extra information, you should re-implement\n        this method in your own modules. Both single-line and multi-line\n        strings are acceptable.\n        \"\"\"\n        return ''\n\n    def __repr__(self):\n        # We treat the extra repr like the sub-module, one item per line\n        extra_lines = []\n        extra_repr = self.extra_repr()\n        # empty string will be split into list ['']\n        if extra_repr:\n            extra_lines = extra_repr.split('\\n')\n        child_lines = []\n        for key, module in self._modules.items():\n            mod_str = repr(module)\n            mod_str = _addindent(mod_str, 2)\n            child_lines.append('(' + key + '): ' + mod_str)\n        lines = extra_lines + child_lines\n\n        main_str = self._get_name() + '('\n        if lines:\n            # simple one-liner info, which most builtin Modules will use\n            if len(extra_lines) == 1 and not child_lines:\n                main_str += extra_lines[0]\n            else:\n                main_str += '\\n  ' + '\\n  '.join(lines) + '\\n'\n\n        main_str += ')'\n        return main_str\n\n    def __dir__(self):\n        module_attrs = dir(self.__class__)\n        attrs = list(self.__dict__.keys())\n        parameters = list(self._parameters.keys())\n        modules = list(self._modules.keys())\n        buffers = list(self._buffers.keys())\n        keys = module_attrs + attrs + parameters + modules + buffers\n\n        # Eliminate attrs that are not legal Python variable names\n        keys = [key for key in keys if not key[0].isdigit()]\n\n        return sorted(keys)\n\n    def _replicate_for_data_parallel(self):\n        replica = self.__new__(type(self))\n        replica.__dict__ = self.__dict__.copy()\n\n        # replicas do not have parameters themselves, the replicas reference the original\n        # module.\n        replica._parameters = OrderedDict()\n        replica._buffers = replica._buffers.copy()\n        replica._modules = replica._modules.copy()\n        replica._is_replica = True  # type: ignore[assignment]\n\n        return replica\n\n    def compile(self, *args, **kwargs):\n        \"\"\"\n        Compile this Module's forward using :func:`torch.compile`.\n\n        This Module's `__call__` method is compiled and all arguments are passed as-is\n        to :func:`torch.compile`.\n\n        See :func:`torch.compile` for details on the arguments for this function.\n        \"\"\"\n        self._compiled_call_impl = torch.compile(self._call_impl, *args, **kwargs)\n", 2541], "C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py": ["import math\nimport warnings\n\nimport torch\nfrom torch import Tensor\nfrom torch.nn.parameter import Parameter, UninitializedParameter\nfrom .. import functional as F\nfrom .. import init\nfrom .lazy import LazyModuleMixin\nfrom .module import Module\nfrom .utils import _single, _pair, _triple, _reverse_repeat_tuple\nfrom torch._torch_docs import reproducibility_notes\n\nfrom ..common_types import _size_1_t, _size_2_t, _size_3_t\nfrom typing import Optional, List, Tuple, Union\n\n__all__ = ['Conv1d', 'Conv2d', 'Conv3d', 'ConvTranspose1d', 'ConvTranspose2d', 'ConvTranspose3d',\n           'LazyConv1d', 'LazyConv2d', 'LazyConv3d', 'LazyConvTranspose1d', 'LazyConvTranspose2d',\n           'LazyConvTranspose3d']\n\nconvolution_notes = \\\n    {\"groups_note\": r\"\"\"* :attr:`groups` controls the connections between inputs and outputs.\n      :attr:`in_channels` and :attr:`out_channels` must both be divisible by\n      :attr:`groups`. For example,\n\n        * At groups=1, all inputs are convolved to all outputs.\n        * At groups=2, the operation becomes equivalent to having two conv\n          layers side by side, each seeing half the input channels\n          and producing half the output channels, and both subsequently\n          concatenated.\n        * At groups= :attr:`in_channels`, each input channel is convolved with\n          its own set of filters (of size\n          :math:`\\frac{\\text{out\\_channels}}{\\text{in\\_channels}}`).\"\"\",\n\n        \"depthwise_separable_note\": r\"\"\"When `groups == in_channels` and `out_channels == K * in_channels`,\n        where `K` is a positive integer, this operation is also known as a \"depthwise convolution\".\n\n        In other words, for an input of size :math:`(N, C_{in}, L_{in})`,\n        a depthwise convolution with a depthwise multiplier `K` can be performed with the arguments\n        :math:`(C_\\text{in}=C_\\text{in}, C_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in})`.\"\"\"}  # noqa: B950\n\n\n\n\n\nclass _ConvNd(Module):\n\n    __constants__ = ['stride', 'padding', 'dilation', 'groups',\n                     'padding_mode', 'output_padding', 'in_channels',\n                     'out_channels', 'kernel_size']\n    __annotations__ = {'bias': Optional[torch.Tensor]}\n\n    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]) -> Tensor:  # type: ignore[empty-body]\n        ...\n\n    in_channels: int\n    _reversed_padding_repeated_twice: List[int]\n    out_channels: int\n    kernel_size: Tuple[int, ...]\n    stride: Tuple[int, ...]\n    padding: Union[str, Tuple[int, ...]]\n    dilation: Tuple[int, ...]\n    transposed: bool\n    output_padding: Tuple[int, ...]\n    groups: int\n    padding_mode: str\n    weight: Tensor\n    bias: Optional[Tensor]\n\n    def __init__(self,\n                 in_channels: int,\n                 out_channels: int,\n                 kernel_size: Tuple[int, ...],\n                 stride: Tuple[int, ...],\n                 padding: Tuple[int, ...],\n                 dilation: Tuple[int, ...],\n                 transposed: bool,\n                 output_padding: Tuple[int, ...],\n                 groups: int,\n                 bias: bool,\n                 padding_mode: str,\n                 device=None,\n                 dtype=None) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__()\n        if groups <= 0:\n            raise ValueError('groups must be a positive integer')\n        if in_channels % groups != 0:\n            raise ValueError('in_channels must be divisible by groups')\n        if out_channels % groups != 0:\n            raise ValueError('out_channels must be divisible by groups')\n        valid_padding_strings = {'same', 'valid'}\n        if isinstance(padding, str):\n            if padding not in valid_padding_strings:\n                raise ValueError(\n                    f\"Invalid padding string {padding!r}, should be one of {valid_padding_strings}\")\n            if padding == 'same' and any(s != 1 for s in stride):\n                raise ValueError(\"padding='same' is not supported for strided convolutions\")\n\n        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n        if padding_mode not in valid_padding_modes:\n            raise ValueError(f\"padding_mode must be one of {valid_padding_modes}, but got padding_mode='{padding_mode}'\")\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.transposed = transposed\n        self.output_padding = output_padding\n        self.groups = groups\n        self.padding_mode = padding_mode\n        # `_reversed_padding_repeated_twice` is the padding to be passed to\n        # `F.pad` if needed (e.g., for non-zero padding types that are\n        # implemented as two ops: padding + conv). `F.pad` accepts paddings in\n        # reverse order than the dimension.\n        if isinstance(self.padding, str):\n            self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size)\n            if padding == 'same':\n                for d, k, i in zip(dilation, kernel_size,\n                                   range(len(kernel_size) - 1, -1, -1)):\n                    total_padding = d * (k - 1)\n                    left_pad = total_padding // 2\n                    self._reversed_padding_repeated_twice[2 * i] = left_pad\n                    self._reversed_padding_repeated_twice[2 * i + 1] = (\n                        total_padding - left_pad)\n        else:\n            self._reversed_padding_repeated_twice = _reverse_repeat_tuple(self.padding, 2)\n\n        if transposed:\n            self.weight = Parameter(torch.empty(\n                (in_channels, out_channels // groups, *kernel_size), **factory_kwargs))\n        else:\n            self.weight = Parameter(torch.empty(\n                (out_channels, in_channels // groups, *kernel_size), **factory_kwargs))\n        if bias:\n            self.bias = Parameter(torch.empty(out_channels, **factory_kwargs))\n        else:\n            self.register_parameter('bias', None)\n\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n        # uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)\n        # For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\n        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n            if fan_in != 0:\n                bound = 1 / math.sqrt(fan_in)\n                init.uniform_(self.bias, -bound, bound)\n\n    def extra_repr(self):\n        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n             ', stride={stride}')\n        if self.padding != (0,) * len(self.padding):\n            s += ', padding={padding}'\n        if self.dilation != (1,) * len(self.dilation):\n            s += ', dilation={dilation}'\n        if self.output_padding != (0,) * len(self.output_padding):\n            s += ', output_padding={output_padding}'\n        if self.groups != 1:\n            s += ', groups={groups}'\n        if self.bias is None:\n            s += ', bias=False'\n        if self.padding_mode != 'zeros':\n            s += ', padding_mode={padding_mode}'\n        return s.format(**self.__dict__)\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n        if not hasattr(self, 'padding_mode'):\n            self.padding_mode = 'zeros'\n\n\nclass Conv1d(_ConvNd):\n    __doc__ = r\"\"\"Applies a 1D convolution over an input signal composed of several input\n    planes.\n\n    In the simplest case, the output value of the layer with input size\n    :math:`(N, C_{\\text{in}}, L)` and output :math:`(N, C_{\\text{out}}, L_{\\text{out}})` can be\n    precisely described as:\n\n    .. math::\n        \\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) +\n        \\sum_{k = 0}^{C_{in} - 1} \\text{weight}(C_{\\text{out}_j}, k)\n        \\star \\text{input}(N_i, k)\n\n    where :math:`\\star` is the valid `cross-correlation`_ operator,\n    :math:`N` is a batch size, :math:`C` denotes a number of channels,\n    :math:`L` is a length of signal sequence.\n    \"\"\" + r\"\"\"\n\n    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n\n    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n\n    * :attr:`stride` controls the stride for the cross-correlation, a single\n      number or a one-element tuple.\n\n    * :attr:`padding` controls the amount of padding applied to the input. It\n      can be either a string {{'valid', 'same'}} or a tuple of ints giving the\n      amount of implicit padding applied on both sides.\n\n    * :attr:`dilation` controls the spacing between the kernel points; also\n      known as the \u00e0 trous algorithm. It is harder to describe, but this `link`_\n      has a nice visualization of what :attr:`dilation` does.\n\n    {groups_note}\n\n    Note:\n        {depthwise_separable_note}\n    Note:\n        {cudnn_reproducibility_note}\n\n    Note:\n        ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n        the input so the output has the shape as the input. However, this mode\n        doesn't support any stride values other than 1.\n\n    Note:\n        This module supports complex data types i.e. ``complex32, complex64, complex128``.\n\n    Args:\n        in_channels (int): Number of channels in the input image\n        out_channels (int): Number of channels produced by the convolution\n        kernel_size (int or tuple): Size of the convolving kernel\n        stride (int or tuple, optional): Stride of the convolution. Default: 1\n        padding (int, tuple or str, optional): Padding added to both sides of\n            the input. Default: 0\n        padding_mode (str, optional): ``'zeros'``, ``'reflect'``,\n            ``'replicate'`` or ``'circular'``. Default: ``'zeros'``\n        dilation (int or tuple, optional): Spacing between kernel\n            elements. Default: 1\n        groups (int, optional): Number of blocked connections from input\n            channels to output channels. Default: 1\n        bias (bool, optional): If ``True``, adds a learnable bias to the\n            output. Default: ``True``\n\n    \"\"\".format(**reproducibility_notes, **convolution_notes) + r\"\"\"\n\n    Shape:\n        - Input: :math:`(N, C_{in}, L_{in})` or :math:`(C_{in}, L_{in})`\n        - Output: :math:`(N, C_{out}, L_{out})` or :math:`(C_{out}, L_{out})`, where\n\n          .. math::\n              L_{out} = \\left\\lfloor\\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation}\n                        \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor\n\n    Attributes:\n        weight (Tensor): the learnable weights of the module of shape\n            :math:`(\\text{out\\_channels},\n            \\frac{\\text{in\\_channels}}{\\text{groups}}, \\text{kernel\\_size})`.\n            The values of these weights are sampled from\n            :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n            :math:`k = \\frac{groups}{C_\\text{in} * \\text{kernel\\_size}}`\n        bias (Tensor):   the learnable bias of the module of shape\n            (out_channels). If :attr:`bias` is ``True``, then the values of these weights are\n            sampled from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n            :math:`k = \\frac{groups}{C_\\text{in} * \\text{kernel\\_size}}`\n\n    Examples::\n\n        >>> m = nn.Conv1d(16, 33, 3, stride=2)\n        >>> input = torch.randn(20, 16, 50)\n        >>> output = m(input)\n\n    .. _cross-correlation:\n        https://en.wikipedia.org/wiki/Cross-correlation\n\n    .. _link:\n        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: _size_1_t,\n        stride: _size_1_t = 1,\n        padding: Union[str, _size_1_t] = 0,\n        dilation: _size_1_t = 1,\n        groups: int = 1,\n        bias: bool = True,\n        padding_mode: str = 'zeros',  # TODO: refine this type\n        device=None,\n        dtype=None\n    ) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        # we create new variables below to make mypy happy since kernel_size has\n        # type Union[int, Tuple[int]] and kernel_size_ has type Tuple[int]\n        kernel_size_ = _single(kernel_size)\n        stride_ = _single(stride)\n        padding_ = padding if isinstance(padding, str) else _single(padding)\n        dilation_ = _single(dilation)\n        super().__init__(\n            in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,\n            False, _single(0), groups, bias, padding_mode, **factory_kwargs)\n\n    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]):\n        if self.padding_mode != 'zeros':\n            return F.conv1d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n                            weight, bias, self.stride,\n                            _single(0), self.dilation, self.groups)\n        return F.conv1d(input, weight, bias, self.stride,\n                        self.padding, self.dilation, self.groups)\n\n    def forward(self, input: Tensor) -> Tensor:\n        return self._conv_forward(input, self.weight, self.bias)\n\n\nclass Conv2d(_ConvNd):\n    __doc__ = r\"\"\"Applies a 2D convolution over an input signal composed of several input\n    planes.\n\n    In the simplest case, the output value of the layer with input size\n    :math:`(N, C_{\\text{in}}, H, W)` and output :math:`(N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})`\n    can be precisely described as:\n\n    .. math::\n        \\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) +\n        \\sum_{k = 0}^{C_{\\text{in}} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input}(N_i, k)\n\n\n    where :math:`\\star` is the valid 2D `cross-correlation`_ operator,\n    :math:`N` is a batch size, :math:`C` denotes a number of channels,\n    :math:`H` is a height of input planes in pixels, and :math:`W` is\n    width in pixels.\n    \"\"\" + r\"\"\"\n\n    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n\n    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n\n    * :attr:`stride` controls the stride for the cross-correlation, a single\n      number or a tuple.\n\n    * :attr:`padding` controls the amount of padding applied to the input. It\n      can be either a string {{'valid', 'same'}} or an int / a tuple of ints giving the\n      amount of implicit padding applied on both sides.\n\n    * :attr:`dilation` controls the spacing between the kernel points; also\n      known as the \u00e0 trous algorithm. It is harder to describe, but this `link`_\n      has a nice visualization of what :attr:`dilation` does.\n\n    {groups_note}\n\n    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:\n\n        - a single ``int`` -- in which case the same value is used for the height and width dimension\n        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,\n          and the second `int` for the width dimension\n\n    Note:\n        {depthwise_separable_note}\n\n    Note:\n        {cudnn_reproducibility_note}\n\n    Note:\n        ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n        the input so the output has the shape as the input. However, this mode\n        doesn't support any stride values other than 1.\n\n    Note:\n        This module supports complex data types i.e. ``complex32, complex64, complex128``.\n\n    Args:\n        in_channels (int): Number of channels in the input image\n        out_channels (int): Number of channels produced by the convolution\n        kernel_size (int or tuple): Size of the convolving kernel\n        stride (int or tuple, optional): Stride of the convolution. Default: 1\n        padding (int, tuple or str, optional): Padding added to all four sides of\n            the input. Default: 0\n        padding_mode (str, optional): ``'zeros'``, ``'reflect'``,\n            ``'replicate'`` or ``'circular'``. Default: ``'zeros'``\n        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n        groups (int, optional): Number of blocked connections from input\n            channels to output channels. Default: 1\n        bias (bool, optional): If ``True``, adds a learnable bias to the\n            output. Default: ``True``\n    \"\"\".format(**reproducibility_notes, **convolution_notes) + r\"\"\"\n\n    Shape:\n        - Input: :math:`(N, C_{in}, H_{in}, W_{in})` or :math:`(C_{in}, H_{in}, W_{in})`\n        - Output: :math:`(N, C_{out}, H_{out}, W_{out})` or :math:`(C_{out}, H_{out}, W_{out})`, where\n\n          .. math::\n              H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n                        \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n\n          .. math::\n              W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n                        \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n\n    Attributes:\n        weight (Tensor): the learnable weights of the module of shape\n            :math:`(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},`\n            :math:`\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]})`.\n            The values of these weights are sampled from\n            :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n            :math:`k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n        bias (Tensor):   the learnable bias of the module of shape\n            (out_channels). If :attr:`bias` is ``True``,\n            then the values of these weights are\n            sampled from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n            :math:`k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n\n    Examples:\n\n        >>> # With square kernels and equal stride\n        >>> m = nn.Conv2d(16, 33, 3, stride=2)\n        >>> # non-square kernels and unequal stride and with padding\n        >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n        >>> # non-square kernels and unequal stride and with padding and dilation\n        >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n        >>> input = torch.randn(20, 16, 50, 100)\n        >>> output = m(input)\n\n    .. _cross-correlation:\n        https://en.wikipedia.org/wiki/Cross-correlation\n\n    .. _link:\n        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: _size_2_t,\n        stride: _size_2_t = 1,\n        padding: Union[str, _size_2_t] = 0,\n        dilation: _size_2_t = 1,\n        groups: int = 1,\n        bias: bool = True,\n        padding_mode: str = 'zeros',  # TODO: refine this type\n        device=None,\n        dtype=None\n    ) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        kernel_size_ = _pair(kernel_size)\n        stride_ = _pair(stride)\n        padding_ = padding if isinstance(padding, str) else _pair(padding)\n        dilation_ = _pair(dilation)\n        super().__init__(\n            in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,\n            False, _pair(0), groups, bias, padding_mode, **factory_kwargs)\n\n    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]):\n        if self.padding_mode != 'zeros':\n            return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n                            weight, bias, self.stride,\n                            _pair(0), self.dilation, self.groups)\n        return F.conv2d(input, weight, bias, self.stride,\n                        self.padding, self.dilation, self.groups)\n\n    def forward(self, input: Tensor) -> Tensor:\n        return self._conv_forward(input, self.weight, self.bias)\n\nclass Conv3d(_ConvNd):\n    __doc__ = r\"\"\"Applies a 3D convolution over an input signal composed of several input\n    planes.\n\n    In the simplest case, the output value of the layer with input size :math:`(N, C_{in}, D, H, W)`\n    and output :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` can be precisely described as:\n\n    .. math::\n        out(N_i, C_{out_j}) = bias(C_{out_j}) +\n                                \\sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \\star input(N_i, k)\n\n    where :math:`\\star` is the valid 3D `cross-correlation`_ operator\n    \"\"\" + r\"\"\"\n\n    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n\n    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n\n    * :attr:`stride` controls the stride for the cross-correlation.\n\n    * :attr:`padding` controls the amount of padding applied to the input. It\n      can be either a string {{'valid', 'same'}} or a tuple of ints giving the\n      amount of implicit padding applied on both sides.\n\n    * :attr:`dilation` controls the spacing between the kernel points; also known as the \u00e0 trous algorithm.\n      It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.\n\n    {groups_note}\n\n    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:\n\n        - a single ``int`` -- in which case the same value is used for the depth, height and width dimension\n        - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension,\n          the second `int` for the height dimension and the third `int` for the width dimension\n\n    Note:\n        {depthwise_separable_note}\n\n    Note:\n        {cudnn_reproducibility_note}\n\n    Note:\n        ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n        the input so the output has the shape as the input. However, this mode\n        doesn't support any stride values other than 1.\n\n    Note:\n        This module supports complex data types i.e. ``complex32, complex64, complex128``.\n\n    Args:\n        in_channels (int): Number of channels in the input image\n        out_channels (int): Number of channels produced by the convolution\n        kernel_size (int or tuple): Size of the convolving kernel\n        stride (int or tuple, optional): Stride of the convolution. Default: 1\n        padding (int, tuple or str, optional): Padding added to all six sides of\n            the input. Default: 0\n        padding_mode (str, optional): ``'zeros'``, ``'reflect'``, ``'replicate'`` or ``'circular'``. Default: ``'zeros'``\n        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n    \"\"\".format(**reproducibility_notes, **convolution_notes) + r\"\"\"\n\n    Shape:\n        - Input: :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})` or :math:`(C_{in}, D_{in}, H_{in}, W_{in})`\n        - Output: :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` or :math:`(C_{out}, D_{out}, H_{out}, W_{out})`,\n          where\n\n          .. math::\n              D_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n                    \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n\n          .. math::\n              H_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n                    \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n\n          .. math::\n              W_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] - \\text{dilation}[2]\n                    \\times (\\text{kernel\\_size}[2] - 1) - 1}{\\text{stride}[2]} + 1\\right\\rfloor\n\n    Attributes:\n        weight (Tensor): the learnable weights of the module of shape\n                         :math:`(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},`\n                         :math:`\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]}, \\text{kernel\\_size[2]})`.\n                         The values of these weights are sampled from\n                         :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n                         :math:`k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]}`\n        bias (Tensor):   the learnable bias of the module of shape (out_channels). If :attr:`bias` is ``True``,\n                         then the values of these weights are\n                         sampled from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n                         :math:`k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]}`\n\n    Examples::\n\n        >>> # With square kernels and equal stride\n        >>> m = nn.Conv3d(16, 33, 3, stride=2)\n        >>> # non-square kernels and unequal stride and with padding\n        >>> m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))\n        >>> input = torch.randn(20, 16, 10, 50, 100)\n        >>> output = m(input)\n\n    .. _cross-correlation:\n        https://en.wikipedia.org/wiki/Cross-correlation\n\n    .. _link:\n        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: _size_3_t,\n        stride: _size_3_t = 1,\n        padding: Union[str, _size_3_t] = 0,\n        dilation: _size_3_t = 1,\n        groups: int = 1,\n        bias: bool = True,\n        padding_mode: str = 'zeros',\n        device=None,\n        dtype=None\n    ) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        kernel_size_ = _triple(kernel_size)\n        stride_ = _triple(stride)\n        padding_ = padding if isinstance(padding, str) else _triple(padding)\n        dilation_ = _triple(dilation)\n        super().__init__(\n            in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,\n            False, _triple(0), groups, bias, padding_mode, **factory_kwargs)\n\n    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]):\n        if self.padding_mode != \"zeros\":\n            return F.conv3d(\n                F.pad(\n                    input, self._reversed_padding_repeated_twice, mode=self.padding_mode\n                ),\n                weight,\n                bias,\n                self.stride,\n                _triple(0),\n                self.dilation,\n                self.groups,\n            )\n        return F.conv3d(\n            input, weight, bias, self.stride, self.padding, self.dilation, self.groups\n        )\n\n    def forward(self, input: Tensor) -> Tensor:\n        return self._conv_forward(input, self.weight, self.bias)\n\n\n\nclass _ConvTransposeNd(_ConvNd):\n    def __init__(self, in_channels, out_channels, kernel_size, stride,\n                 padding, dilation, transposed, output_padding,\n                 groups, bias, padding_mode, device=None, dtype=None) -> None:\n        if padding_mode != 'zeros':\n            raise ValueError(f'Only \"zeros\" padding mode is supported for {self.__class__.__name__}')\n\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(\n            in_channels, out_channels, kernel_size, stride,\n            padding, dilation, transposed, output_padding,\n            groups, bias, padding_mode, **factory_kwargs)\n\n    # dilation being an optional parameter is for backwards\n    # compatibility\n    def _output_padding(self, input: Tensor, output_size: Optional[List[int]],\n                        stride: List[int], padding: List[int], kernel_size: List[int],\n                        num_spatial_dims: int, dilation: Optional[List[int]] = None) -> List[int]:\n        if output_size is None:\n            ret = _single(self.output_padding)  # converting to list if was not already\n        else:\n            has_batch_dim = input.dim() == num_spatial_dims + 2\n            num_non_spatial_dims = 2 if has_batch_dim else 1\n            if len(output_size) == num_non_spatial_dims + num_spatial_dims:\n                output_size = output_size[num_non_spatial_dims:]\n            if len(output_size) != num_spatial_dims:\n                raise ValueError(\n                    \"ConvTranspose{}D: for {}D input, output_size must have {} or {} elements (got {})\"\n                    .format(num_spatial_dims, input.dim(), num_spatial_dims,\n                            num_non_spatial_dims + num_spatial_dims, len(output_size)))\n\n            min_sizes = torch.jit.annotate(List[int], [])\n            max_sizes = torch.jit.annotate(List[int], [])\n            for d in range(num_spatial_dims):\n                dim_size = ((input.size(d + num_non_spatial_dims) - 1) * stride[d] -\n                            2 * padding[d] +\n                            (dilation[d] if dilation is not None else 1) * (kernel_size[d] - 1) + 1)\n                min_sizes.append(dim_size)\n                max_sizes.append(min_sizes[d] + stride[d] - 1)\n\n            for i in range(len(output_size)):\n                size = output_size[i]\n                min_size = min_sizes[i]\n                max_size = max_sizes[i]\n                if size < min_size or size > max_size:\n                    raise ValueError(\n                        f\"requested an output size of {output_size}, but valid sizes range \"\n                        f\"from {min_sizes} to {max_sizes} (for an input of {input.size()[2:]})\")\n\n            res = torch.jit.annotate(List[int], [])\n            for d in range(num_spatial_dims):\n                res.append(output_size[d] - min_sizes[d])\n\n            ret = res\n        return ret\n\n\nclass ConvTranspose1d(_ConvTransposeNd):\n    __doc__ = r\"\"\"Applies a 1D transposed convolution operator over an input image\n    composed of several input planes.\n\n    This module can be seen as the gradient of Conv1d with respect to its input.\n    It is also known as a fractionally-strided convolution or\n    a deconvolution (although it is not an actual deconvolution operation as it does\n    not compute a true inverse of convolution). For more information, see the visualizations\n    `here`_ and the `Deconvolutional Networks`_ paper.\n\n    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n\n    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n\n    * :attr:`stride` controls the stride for the cross-correlation.\n\n    * :attr:`padding` controls the amount of implicit zero padding on both\n      sides for ``dilation * (kernel_size - 1) - padding`` number of points. See note\n      below for details.\n\n    * :attr:`output_padding` controls the additional size added to one side\n      of the output shape. See note below for details.\n\n    * :attr:`dilation` controls the spacing between the kernel points; also known as the \u00e0 trous algorithm.\n      It is harder to describe, but the link `here`_ has a nice visualization of what :attr:`dilation` does.\n\n    {groups_note}\n\n    Note:\n        The :attr:`padding` argument effectively adds ``dilation * (kernel_size - 1) - padding``\n        amount of zero padding to both sizes of the input. This is set so that\n        when a :class:`~torch.nn.Conv1d` and a :class:`~torch.nn.ConvTranspose1d`\n        are initialized with same parameters, they are inverses of each other in\n        regard to the input and output shapes. However, when ``stride > 1``,\n        :class:`~torch.nn.Conv1d` maps multiple input shapes to the same output\n        shape. :attr:`output_padding` is provided to resolve this ambiguity by\n        effectively increasing the calculated output shape on one side. Note\n        that :attr:`output_padding` is only used to find output shape, but does\n        not actually add zero-padding to output.\n\n    Note:\n        In some circumstances when using the CUDA backend with CuDNN, this operator\n        may select a nondeterministic algorithm to increase performance. If this is\n        undesirable, you can try to make the operation deterministic (potentially at\n        a performance cost) by setting ``torch.backends.cudnn.deterministic =\n        True``.\n        Please see the notes on :doc:`/notes/randomness` for background.\n\n\n    Args:\n        in_channels (int): Number of channels in the input image\n        out_channels (int): Number of channels produced by the convolution\n        kernel_size (int or tuple): Size of the convolving kernel\n        stride (int or tuple, optional): Stride of the convolution. Default: 1\n        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding\n            will be added to both sides of the input. Default: 0\n        output_padding (int or tuple, optional): Additional size added to one side\n            of the output shape. Default: 0\n        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n    \"\"\".format(**reproducibility_notes, **convolution_notes) + r\"\"\"\n\n    Shape:\n        - Input: :math:`(N, C_{in}, L_{in})` or :math:`(C_{in}, L_{in})`\n        - Output: :math:`(N, C_{out}, L_{out})` or :math:`(C_{out}, L_{out})`, where\n\n          .. math::\n              L_{out} = (L_{in} - 1) \\times \\text{stride} - 2 \\times \\text{padding} + \\text{dilation}\n                        \\times (\\text{kernel\\_size} - 1) + \\text{output\\_padding} + 1\n\n    Attributes:\n        weight (Tensor): the learnable weights of the module of shape\n                         :math:`(\\text{in\\_channels}, \\frac{\\text{out\\_channels}}{\\text{groups}},`\n                         :math:`\\text{kernel\\_size})`.\n                         The values of these weights are sampled from\n                         :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n                         :math:`k = \\frac{groups}{C_\\text{out} * \\text{kernel\\_size}}`\n        bias (Tensor):   the learnable bias of the module of shape (out_channels).\n                         If :attr:`bias` is ``True``, then the values of these weights are\n                         sampled from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n                         :math:`k = \\frac{groups}{C_\\text{out} * \\text{kernel\\_size}}`\n\n    .. _`here`:\n        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n\n    .. _`Deconvolutional Networks`:\n        https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: _size_1_t,\n        stride: _size_1_t = 1,\n        padding: _size_1_t = 0,\n        output_padding: _size_1_t = 0,\n        groups: int = 1,\n        bias: bool = True,\n        dilation: _size_1_t = 1,\n        padding_mode: str = 'zeros',\n        device=None,\n        dtype=None\n    ) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        kernel_size = _single(kernel_size)\n        stride = _single(stride)\n        padding = _single(padding)\n        dilation = _single(dilation)\n        output_padding = _single(output_padding)\n        super().__init__(\n            in_channels, out_channels, kernel_size, stride, padding, dilation,\n            True, output_padding, groups, bias, padding_mode, **factory_kwargs)\n\n    def forward(self, input: Tensor, output_size: Optional[List[int]] = None) -> Tensor:\n        if self.padding_mode != 'zeros':\n            raise ValueError('Only `zeros` padding mode is supported for ConvTranspose1d')\n\n        assert isinstance(self.padding, tuple)\n        # One cannot replace List by Tuple or Sequence in \"_output_padding\" because\n        # TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.\n        num_spatial_dims = 1\n        output_padding = self._output_padding(\n            input, output_size, self.stride, self.padding, self.kernel_size,  # type: ignore[arg-type]\n            num_spatial_dims, self.dilation)  # type: ignore[arg-type]\n        return F.conv_transpose1d(\n            input, self.weight, self.bias, self.stride, self.padding,\n            output_padding, self.groups, self.dilation)\n\n\nclass ConvTranspose2d(_ConvTransposeNd):\n    __doc__ = r\"\"\"Applies a 2D transposed convolution operator over an input image\n    composed of several input planes.\n\n    This module can be seen as the gradient of Conv2d with respect to its input.\n    It is also known as a fractionally-strided convolution or\n    a deconvolution (although it is not an actual deconvolution operation as it does\n    not compute a true inverse of convolution). For more information, see the visualizations\n    `here`_ and the `Deconvolutional Networks`_ paper.\n\n    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n\n    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n\n    * :attr:`stride` controls the stride for the cross-correlation.\n\n    * :attr:`padding` controls the amount of implicit zero padding on both\n      sides for ``dilation * (kernel_size - 1) - padding`` number of points. See note\n      below for details.\n\n    * :attr:`output_padding` controls the additional size added to one side\n      of the output shape. See note below for details.\n\n    * :attr:`dilation` controls the spacing between the kernel points; also known as the \u00e0 trous algorithm.\n      It is harder to describe, but the link `here`_ has a nice visualization of what :attr:`dilation` does.\n\n    {groups_note}\n\n    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`output_padding`\n    can either be:\n\n        - a single ``int`` -- in which case the same value is used for the height and width dimensions\n        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,\n          and the second `int` for the width dimension\n\n    Note:\n        The :attr:`padding` argument effectively adds ``dilation * (kernel_size - 1) - padding``\n        amount of zero padding to both sizes of the input. This is set so that\n        when a :class:`~torch.nn.Conv2d` and a :class:`~torch.nn.ConvTranspose2d`\n        are initialized with same parameters, they are inverses of each other in\n        regard to the input and output shapes. However, when ``stride > 1``,\n        :class:`~torch.nn.Conv2d` maps multiple input shapes to the same output\n        shape. :attr:`output_padding` is provided to resolve this ambiguity by\n        effectively increasing the calculated output shape on one side. Note\n        that :attr:`output_padding` is only used to find output shape, but does\n        not actually add zero-padding to output.\n\n    Note:\n        {cudnn_reproducibility_note}\n\n    Args:\n        in_channels (int): Number of channels in the input image\n        out_channels (int): Number of channels produced by the convolution\n        kernel_size (int or tuple): Size of the convolving kernel\n        stride (int or tuple, optional): Stride of the convolution. Default: 1\n        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding\n            will be added to both sides of each dimension in the input. Default: 0\n        output_padding (int or tuple, optional): Additional size added to one side\n            of each dimension in the output shape. Default: 0\n        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n    \"\"\".format(**reproducibility_notes, **convolution_notes) + r\"\"\"\n\n    Shape:\n        - Input: :math:`(N, C_{in}, H_{in}, W_{in})` or :math:`(C_{in}, H_{in}, W_{in})`\n        - Output: :math:`(N, C_{out}, H_{out}, W_{out})` or :math:`(C_{out}, H_{out}, W_{out})`, where\n\n        .. math::\n              H_{out} = (H_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{dilation}[0]\n                        \\times (\\text{kernel\\_size}[0] - 1) + \\text{output\\_padding}[0] + 1\n        .. math::\n              W_{out} = (W_{in} - 1) \\times \\text{stride}[1] - 2 \\times \\text{padding}[1] + \\text{dilation}[1]\n                        \\times (\\text{kernel\\_size}[1] - 1) + \\text{output\\_padding}[1] + 1\n\n    Attributes:\n        weight (Tensor): the learnable weights of the module of shape\n                         :math:`(\\text{in\\_channels}, \\frac{\\text{out\\_channels}}{\\text{groups}},`\n                         :math:`\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]})`.\n                         The values of these weights are sampled from\n                         :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n                         :math:`k = \\frac{groups}{C_\\text{out} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n        bias (Tensor):   the learnable bias of the module of shape (out_channels)\n                         If :attr:`bias` is ``True``, then the values of these weights are\n                         sampled from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n                         :math:`k = \\frac{groups}{C_\\text{out} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n\n    Examples::\n\n        >>> # With square kernels and equal stride\n        >>> m = nn.ConvTranspose2d(16, 33, 3, stride=2)\n        >>> # non-square kernels and unequal stride and with padding\n        >>> m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n        >>> input = torch.randn(20, 16, 50, 100)\n        >>> output = m(input)\n        >>> # exact output size can be also specified as an argument\n        >>> input = torch.randn(1, 16, 12, 12)\n        >>> downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n        >>> upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)\n        >>> h = downsample(input)\n        >>> h.size()\n        torch.Size([1, 16, 6, 6])\n        >>> output = upsample(h, output_size=input.size())\n        >>> output.size()\n        torch.Size([1, 16, 12, 12])\n\n    .. _`here`:\n        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n\n    .. _`Deconvolutional Networks`:\n        https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: _size_2_t,\n        stride: _size_2_t = 1,\n        padding: _size_2_t = 0,\n        output_padding: _size_2_t = 0,\n        groups: int = 1,\n        bias: bool = True,\n        dilation: _size_2_t = 1,\n        padding_mode: str = 'zeros',\n        device=None,\n        dtype=None\n    ) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        kernel_size = _pair(kernel_size)\n        stride = _pair(stride)\n        padding = _pair(padding)\n        dilation = _pair(dilation)\n        output_padding = _pair(output_padding)\n        super().__init__(\n            in_channels, out_channels, kernel_size, stride, padding, dilation,\n            True, output_padding, groups, bias, padding_mode, **factory_kwargs)\n\n    def forward(self, input: Tensor, output_size: Optional[List[int]] = None) -> Tensor:\n        if self.padding_mode != 'zeros':\n            raise ValueError('Only `zeros` padding mode is supported for ConvTranspose2d')\n\n        assert isinstance(self.padding, tuple)\n        # One cannot replace List by Tuple or Sequence in \"_output_padding\" because\n        # TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.\n        num_spatial_dims = 2\n        output_padding = self._output_padding(\n            input, output_size, self.stride, self.padding, self.kernel_size,  # type: ignore[arg-type]\n            num_spatial_dims, self.dilation)  # type: ignore[arg-type]\n\n        return F.conv_transpose2d(\n            input, self.weight, self.bias, self.stride, self.padding,\n            output_padding, self.groups, self.dilation)\n\n\nclass ConvTranspose3d(_ConvTransposeNd):\n    __doc__ = r\"\"\"Applies a 3D transposed convolution operator over an input image composed of several input\n    planes.\n    The transposed convolution operator multiplies each input value element-wise by a learnable kernel,\n    and sums over the outputs from all input feature planes.\n\n    This module can be seen as the gradient of Conv3d with respect to its input.\n    It is also known as a fractionally-strided convolution or\n    a deconvolution (although it is not an actual deconvolution operation as it does\n    not compute a true inverse of convolution). For more information, see the visualizations\n    `here`_ and the `Deconvolutional Networks`_ paper.\n\n    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n\n    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n\n    * :attr:`stride` controls the stride for the cross-correlation.\n\n    * :attr:`padding` controls the amount of implicit zero padding on both\n      sides for ``dilation * (kernel_size - 1) - padding`` number of points. See note\n      below for details.\n\n    * :attr:`output_padding` controls the additional size added to one side\n      of the output shape. See note below for details.\n\n    * :attr:`dilation` controls the spacing between the kernel points; also known as the \u00e0 trous algorithm.\n      It is harder to describe, but the link `here`_ has a nice visualization of what :attr:`dilation` does.\n\n    {groups_note}\n\n    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`output_padding`\n    can either be:\n\n        - a single ``int`` -- in which case the same value is used for the depth, height and width dimensions\n        - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension,\n          the second `int` for the height dimension and the third `int` for the width dimension\n\n    Note:\n        The :attr:`padding` argument effectively adds ``dilation * (kernel_size - 1) - padding``\n        amount of zero padding to both sizes of the input. This is set so that\n        when a :class:`~torch.nn.Conv3d` and a :class:`~torch.nn.ConvTranspose3d`\n        are initialized with same parameters, they are inverses of each other in\n        regard to the input and output shapes. However, when ``stride > 1``,\n        :class:`~torch.nn.Conv3d` maps multiple input shapes to the same output\n        shape. :attr:`output_padding` is provided to resolve this ambiguity by\n        effectively increasing the calculated output shape on one side. Note\n        that :attr:`output_padding` is only used to find output shape, but does\n        not actually add zero-padding to output.\n\n    Note:\n        {cudnn_reproducibility_note}\n\n    Args:\n        in_channels (int): Number of channels in the input image\n        out_channels (int): Number of channels produced by the convolution\n        kernel_size (int or tuple): Size of the convolving kernel\n        stride (int or tuple, optional): Stride of the convolution. Default: 1\n        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding\n            will be added to both sides of each dimension in the input. Default: 0\n        output_padding (int or tuple, optional): Additional size added to one side\n            of each dimension in the output shape. Default: 0\n        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n    \"\"\".format(**reproducibility_notes, **convolution_notes) + r\"\"\"\n\n    Shape:\n        - Input: :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})` or :math:`(C_{in}, D_{in}, H_{in}, W_{in})`\n        - Output: :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` or\n          :math:`(C_{out}, D_{out}, H_{out}, W_{out})`, where\n\n        .. math::\n              D_{out} = (D_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{dilation}[0]\n                        \\times (\\text{kernel\\_size}[0] - 1) + \\text{output\\_padding}[0] + 1\n        .. math::\n              H_{out} = (H_{in} - 1) \\times \\text{stride}[1] - 2 \\times \\text{padding}[1] + \\text{dilation}[1]\n                        \\times (\\text{kernel\\_size}[1] - 1) + \\text{output\\_padding}[1] + 1\n        .. math::\n              W_{out} = (W_{in} - 1) \\times \\text{stride}[2] - 2 \\times \\text{padding}[2] + \\text{dilation}[2]\n                        \\times (\\text{kernel\\_size}[2] - 1) + \\text{output\\_padding}[2] + 1\n\n\n    Attributes:\n        weight (Tensor): the learnable weights of the module of shape\n                         :math:`(\\text{in\\_channels}, \\frac{\\text{out\\_channels}}{\\text{groups}},`\n                         :math:`\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]}, \\text{kernel\\_size[2]})`.\n                         The values of these weights are sampled from\n                         :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n                         :math:`k = \\frac{groups}{C_\\text{out} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]}`\n        bias (Tensor):   the learnable bias of the module of shape (out_channels)\n                         If :attr:`bias` is ``True``, then the values of these weights are\n                         sampled from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n                         :math:`k = \\frac{groups}{C_\\text{out} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]}`\n\n    Examples::\n\n        >>> # With square kernels and equal stride\n        >>> m = nn.ConvTranspose3d(16, 33, 3, stride=2)\n        >>> # non-square kernels and unequal stride and with padding\n        >>> m = nn.ConvTranspose3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(0, 4, 2))\n        >>> input = torch.randn(20, 16, 10, 50, 100)\n        >>> output = m(input)\n\n    .. _`here`:\n        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n\n    .. _`Deconvolutional Networks`:\n        https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: _size_3_t,\n        stride: _size_3_t = 1,\n        padding: _size_3_t = 0,\n        output_padding: _size_3_t = 0,\n        groups: int = 1,\n        bias: bool = True,\n        dilation: _size_3_t = 1,\n        padding_mode: str = 'zeros',\n        device=None,\n        dtype=None\n    ) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        kernel_size = _triple(kernel_size)\n        stride = _triple(stride)\n        padding = _triple(padding)\n        dilation = _triple(dilation)\n        output_padding = _triple(output_padding)\n        super().__init__(\n            in_channels, out_channels, kernel_size, stride, padding, dilation,\n            True, output_padding, groups, bias, padding_mode, **factory_kwargs)\n\n    def forward(self, input: Tensor, output_size: Optional[List[int]] = None) -> Tensor:\n        if self.padding_mode != 'zeros':\n            raise ValueError('Only `zeros` padding mode is supported for ConvTranspose3d')\n\n        assert isinstance(self.padding, tuple)\n        # One cannot replace List by Tuple or Sequence in \"_output_padding\" because\n        # TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.\n        num_spatial_dims = 3\n        output_padding = self._output_padding(\n            input, output_size, self.stride, self.padding, self.kernel_size,  # type: ignore[arg-type]\n            num_spatial_dims, self.dilation)  # type: ignore[arg-type]\n\n        return F.conv_transpose3d(\n            input, self.weight, self.bias, self.stride, self.padding,\n            output_padding, self.groups, self.dilation)\n\n\n# TODO: Deprecate and remove the following alias `_ConvTransposeMixin`.\n#\n# `_ConvTransposeMixin` was a mixin that was removed.  It is meant to be used\n# with `_ConvNd` to construct actual module classes that implements conv\n# transpose ops:\n#\n#   class MyConvTranspose(_ConvNd, _ConvTransposeMixin):\n#       ...\n#\n# In PyTorch, it has been replaced by `_ConvTransposeNd`, which is a proper\n# subclass of `_ConvNd`.  However, some user code in the wild still (incorrectly)\n# use the internal class `_ConvTransposeMixin`.  Hence, we provide this alias\n# for BC, because it is cheap and easy for us to do so, even though that\n# `_ConvTransposeNd` is really not a mixin anymore (but multiple inheritance as\n# above would still work).\nclass _ConvTransposeMixin(_ConvTransposeNd):\n    def __init__(self, *args, **kwargs):\n        warnings.warn(\n            \"_ConvTransposeMixin is a deprecated internal class. \"\n            \"Please consider using public APIs.\")\n        super().__init__(*args, **kwargs)\n\n\n# TODO: Conv2dLocal\n# TODO: Conv2dMap\n# TODO: ConvTranspose2dMap\n\n\nclass _LazyConvXdMixin(LazyModuleMixin):\n    groups: int\n    transposed: bool\n    in_channels: int\n    out_channels: int\n    kernel_size: Tuple[int, ...]\n    weight: UninitializedParameter\n    bias: UninitializedParameter\n\n    def reset_parameters(self) -> None:\n        # has_uninitialized_params is defined in parent class and it is using a protocol on self\n        if not self.has_uninitialized_params() and self.in_channels != 0:  # type: ignore[misc]\n            # \"type:ignore[..]\" is required because mypy thinks that \"reset_parameters\" is undefined\n            # in super class. Turns out that it is defined in _ConvND which is inherited by any class\n            # that also inherits _LazyConvXdMixin\n            super().reset_parameters()  # type: ignore[misc]\n\n    # Signature of \"initialize_parameters\" is incompatible with the definition in supertype LazyModuleMixin\n    def initialize_parameters(self, input) -> None:  # type: ignore[override]\n        # defined by parent class but using a protocol\n        if self.has_uninitialized_params():  # type: ignore[misc]\n            self.in_channels = self._get_in_channels(input)\n            if self.in_channels % self.groups != 0:\n                raise ValueError('in_channels must be divisible by groups')\n            assert isinstance(self.weight, UninitializedParameter)\n            if self.transposed:\n                self.weight.materialize((\n                    self.in_channels, self.out_channels // self.groups, *self.kernel_size))\n            else:\n                self.weight.materialize((\n                    self.out_channels, self.in_channels // self.groups, *self.kernel_size))\n            if self.bias is not None:\n                assert isinstance(self.bias, UninitializedParameter)\n                self.bias.materialize((self.out_channels,))\n            self.reset_parameters()\n\n    # Function to extract in_channels from first input.\n    def _get_in_channels(self, input: Tensor) -> int:\n        num_spatial_dims = self._get_num_spatial_dims()\n        num_dims_no_batch = num_spatial_dims + 1  # +1 for channels dim\n        num_dims_batch = num_dims_no_batch + 1\n        if input.dim() not in (num_dims_no_batch, num_dims_batch):\n            raise RuntimeError(\"Expected {}D (unbatched) or {}D (batched) input to {}, but \"\n                               \"got input of size: {}\".format(num_dims_no_batch, num_dims_batch,\n                                                              self.__class__.__name__, input.shape))\n        return input.shape[1] if input.dim() == num_dims_batch else input.shape[0]\n\n    # Function to return the number of spatial dims expected for inputs to the module.\n    # This is expected to be implemented by subclasses.\n    def _get_num_spatial_dims(self) -> int:\n        raise NotImplementedError()\n\n\n# LazyConv1d defines weight as a Tensor but derived class defines it as UnitializeParameter\nclass LazyConv1d(_LazyConvXdMixin, Conv1d):  # type: ignore[misc]\n    r\"\"\"A :class:`torch.nn.Conv1d` module with lazy initialization of the ``in_channels`` argument.\n\n    The ``in_channels`` argument of the :class:`Conv1d` is inferred from the ``input.size(1)``.\n    The attributes that will be lazily initialized are `weight` and `bias`.\n\n    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation\n    on lazy modules and their limitations.\n\n    Args:\n        out_channels (int): Number of channels produced by the convolution\n        kernel_size (int or tuple): Size of the convolving kernel\n        stride (int or tuple, optional): Stride of the convolution. Default: 1\n        padding (int or tuple, optional): Zero-padding added to both sides of\n            the input. Default: 0\n        padding_mode (str, optional): ``'zeros'``, ``'reflect'``,\n            ``'replicate'`` or ``'circular'``. Default: ``'zeros'``\n        dilation (int or tuple, optional): Spacing between kernel\n            elements. Default: 1\n        groups (int, optional): Number of blocked connections from input\n            channels to output channels. Default: 1\n        bias (bool, optional): If ``True``, adds a learnable bias to the\n            output. Default: ``True``\n\n    .. seealso:: :class:`torch.nn.Conv1d` and :class:`torch.nn.modules.lazy.LazyModuleMixin`\n    \"\"\"\n\n    # super class define this variable as None. \"type: ignore[..] is required\n    # since we are redefining the variable.\n    cls_to_become = Conv1d  # type: ignore[assignment]\n\n    def __init__(\n        self,\n        out_channels: int,\n        kernel_size: _size_1_t,\n        stride: _size_1_t = 1,\n        padding: _size_1_t = 0,\n        dilation: _size_1_t = 1,\n        groups: int = 1,\n        bias: bool = True,\n        padding_mode: str = 'zeros',\n        device=None,\n        dtype=None\n    ) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(\n            0,\n            0,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            groups,\n            # bias is hardcoded to False to avoid creating tensor\n            # that will soon be overwritten.\n            False,\n            padding_mode,\n            **factory_kwargs\n        )\n        self.weight = UninitializedParameter(**factory_kwargs)\n        self.out_channels = out_channels\n        if bias:\n            self.bias = UninitializedParameter(**factory_kwargs)\n\n    def _get_num_spatial_dims(self) -> int:\n        return 1\n\n\n# LazyConv2d defines weight as a Tensor but derived class defines it as UnitializeParameter\nclass LazyConv2d(_LazyConvXdMixin, Conv2d):  # type: ignore[misc]\n    r\"\"\"A :class:`torch.nn.Conv2d` module with lazy initialization of the ``in_channels`` argument.\n\n    The ``in_channels`` argument of the :class:`Conv2d` that is inferred from the ``input.size(1)``.\n    The attributes that will be lazily initialized are `weight` and `bias`.\n\n    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation\n    on lazy modules and their limitations.\n\n    Args:\n        out_channels (int): Number of channels produced by the convolution\n        kernel_size (int or tuple): Size of the convolving kernel\n        stride (int or tuple, optional): Stride of the convolution. Default: 1\n        padding (int or tuple, optional): Zero-padding added to both sides of\n            the input. Default: 0\n        padding_mode (str, optional): ``'zeros'``, ``'reflect'``,\n            ``'replicate'`` or ``'circular'``. Default: ``'zeros'``\n        dilation (int or tuple, optional): Spacing between kernel\n            elements. Default: 1\n        groups (int, optional): Number of blocked connections from input\n            channels to output channels. Default: 1\n        bias (bool, optional): If ``True``, adds a learnable bias to the\n            output. Default: ``True``\n\n    .. seealso:: :class:`torch.nn.Conv2d` and :class:`torch.nn.modules.lazy.LazyModuleMixin`\n    \"\"\"\n\n    # super class define this variable as None. \"type: ignore[..] is required\n    # since we are redefining the variable.\n    cls_to_become = Conv2d  # type: ignore[assignment]\n\n    def __init__(\n        self,\n        out_channels: int,\n        kernel_size: _size_2_t,\n        stride: _size_2_t = 1,\n        padding: _size_2_t = 0,\n        dilation: _size_2_t = 1,\n        groups: int = 1,\n        bias: bool = True,\n        padding_mode: str = 'zeros',  # TODO: refine this type\n        device=None,\n        dtype=None\n    ) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(\n            0,\n            0,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            groups,\n            # bias is hardcoded to False to avoid creating tensor\n            # that will soon be overwritten.\n            False,\n            padding_mode,\n            **factory_kwargs\n        )\n        self.weight = UninitializedParameter(**factory_kwargs)\n        self.out_channels = out_channels\n        if bias:\n            self.bias = UninitializedParameter(**factory_kwargs)\n\n    def _get_num_spatial_dims(self) -> int:\n        return 2\n\n\n# LazyConv3d defines weight as a Tensor but derived class defines it as UnitializeParameter\nclass LazyConv3d(_LazyConvXdMixin, Conv3d):  # type: ignore[misc]\n    r\"\"\"A :class:`torch.nn.Conv3d` module with lazy initialization of the ``in_channels`` argument.\n\n    The ``in_channels`` argument of the :class:`Conv3d` that is inferred from\n    the ``input.size(1)``.\n    The attributes that will be lazily initialized are `weight` and `bias`.\n\n    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation\n    on lazy modules and their limitations.\n\n    Args:\n        out_channels (int): Number of channels produced by the convolution\n        kernel_size (int or tuple): Size of the convolving kernel\n        stride (int or tuple, optional): Stride of the convolution. Default: 1\n        padding (int or tuple, optional): Zero-padding added to both sides of\n            the input. Default: 0\n        padding_mode (str, optional): ``'zeros'``, ``'reflect'``,\n            ``'replicate'`` or ``'circular'``. Default: ``'zeros'``\n        dilation (int or tuple, optional): Spacing between kernel\n            elements. Default: 1\n        groups (int, optional): Number of blocked connections from input\n            channels to output channels. Default: 1\n        bias (bool, optional): If ``True``, adds a learnable bias to the\n            output. Default: ``True``\n\n    .. seealso:: :class:`torch.nn.Conv3d` and :class:`torch.nn.modules.lazy.LazyModuleMixin`\n    \"\"\"\n\n    # super class define this variable as None. \"type: ignore[..] is required\n    # since we are redefining the variable.\n    cls_to_become = Conv3d  # type: ignore[assignment]\n\n    def __init__(\n        self,\n        out_channels: int,\n        kernel_size: _size_3_t,\n        stride: _size_3_t = 1,\n        padding: _size_3_t = 0,\n        dilation: _size_3_t = 1,\n        groups: int = 1,\n        bias: bool = True,\n        padding_mode: str = 'zeros',\n        device=None,\n        dtype=None\n    ) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(\n            0,\n            0,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            groups,\n            # bias is hardcoded to False to avoid creating tensor\n            # that will soon be overwritten.\n            False,\n            padding_mode,\n            **factory_kwargs\n        )\n        self.weight = UninitializedParameter(**factory_kwargs)\n        self.out_channels = out_channels\n        if bias:\n            self.bias = UninitializedParameter(**factory_kwargs)\n\n    def _get_num_spatial_dims(self) -> int:\n        return 3\n\n\n# LazyConvTranspose1d defines weight as a Tensor but derived class defines it as UnitializeParameter\nclass LazyConvTranspose1d(_LazyConvXdMixin, ConvTranspose1d):  # type: ignore[misc]\n    r\"\"\"A :class:`torch.nn.ConvTranspose1d` module with lazy initialization of the ``in_channels`` argument.\n\n    The ``in_channels`` argument of the :class:`ConvTranspose1d` that is inferred from\n    the ``input.size(1)``.\n    The attributes that will be lazily initialized are `weight` and `bias`.\n\n    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation\n    on lazy modules and their limitations.\n\n    Args:\n        out_channels (int): Number of channels produced by the convolution\n        kernel_size (int or tuple): Size of the convolving kernel\n        stride (int or tuple, optional): Stride of the convolution. Default: 1\n        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding\n            will be added to both sides of the input. Default: 0\n        output_padding (int or tuple, optional): Additional size added to one side\n            of the output shape. Default: 0\n        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n\n    .. seealso:: :class:`torch.nn.ConvTranspose1d` and :class:`torch.nn.modules.lazy.LazyModuleMixin`\n    \"\"\"\n\n    # super class define this variable as None. \"type: ignore[..] is required\n    # since we are redefining the variable.\n    cls_to_become = ConvTranspose1d  # type: ignore[assignment]\n\n    def __init__(\n        self,\n        out_channels: int,\n        kernel_size: _size_1_t,\n        stride: _size_1_t = 1,\n        padding: _size_1_t = 0,\n        output_padding: _size_1_t = 0,\n        groups: int = 1,\n        bias: bool = True,\n        dilation: _size_1_t = 1,\n        padding_mode: str = 'zeros',\n        device=None,\n        dtype=None\n    ) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(\n            0,\n            0,\n            kernel_size,\n            stride,\n            padding,\n            output_padding,\n            groups,\n            # bias is hardcoded to False to avoid creating tensor\n            # that will soon be overwritten.\n            False,\n            dilation,\n            padding_mode,\n            **factory_kwargs\n        )\n        self.weight = UninitializedParameter(**factory_kwargs)\n        self.out_channels = out_channels\n        if bias:\n            self.bias = UninitializedParameter(**factory_kwargs)\n\n    def _get_num_spatial_dims(self) -> int:\n        return 1\n\n\n# LazyConvTranspose2d defines weight as a Tensor but derived class defines it as UnitializeParameter\nclass LazyConvTranspose2d(_LazyConvXdMixin, ConvTranspose2d):  # type: ignore[misc]\n    r\"\"\"A :class:`torch.nn.ConvTranspose2d` module with lazy initialization of the ``in_channels`` argument.\n\n    The ``in_channels`` argument of the :class:`ConvTranspose2d` is inferred from\n    the ``input.size(1)``.\n    The attributes that will be lazily initialized are `weight` and `bias`.\n\n    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation\n    on lazy modules and their limitations.\n\n    Args:\n        out_channels (int): Number of channels produced by the convolution\n        kernel_size (int or tuple): Size of the convolving kernel\n        stride (int or tuple, optional): Stride of the convolution. Default: 1\n        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding\n            will be added to both sides of each dimension in the input. Default: 0\n        output_padding (int or tuple, optional): Additional size added to one side\n            of each dimension in the output shape. Default: 0\n        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n\n    .. seealso:: :class:`torch.nn.ConvTranspose2d` and :class:`torch.nn.modules.lazy.LazyModuleMixin`\n    \"\"\"\n\n    # super class define this variable as None. \"type: ignore[..] is required\n    # since we are redefining the variable.\n    cls_to_become = ConvTranspose2d  # type: ignore[assignment]\n\n    def __init__(\n        self,\n        out_channels: int,\n        kernel_size: _size_2_t,\n        stride: _size_2_t = 1,\n        padding: _size_2_t = 0,\n        output_padding: _size_2_t = 0,\n        groups: int = 1,\n        bias: bool = True,\n        dilation: int = 1,\n        padding_mode: str = 'zeros',\n        device=None,\n        dtype=None\n    ) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(\n            0,\n            0,\n            kernel_size,\n            stride,\n            padding,\n            output_padding,\n            groups,\n            # bias is hardcoded to False to avoid creating tensor\n            # that will soon be overwritten.\n            False,\n            dilation,\n            padding_mode,\n            **factory_kwargs\n        )\n        self.weight = UninitializedParameter(**factory_kwargs)\n        self.out_channels = out_channels\n        if bias:\n            self.bias = UninitializedParameter(**factory_kwargs)\n\n    def _get_num_spatial_dims(self) -> int:\n        return 2\n\n\n# LazyConvTranspose3d defines weight as a Tensor but derived class defines it as UnitializeParameter\nclass LazyConvTranspose3d(_LazyConvXdMixin, ConvTranspose3d):  # type: ignore[misc]\n    r\"\"\"A :class:`torch.nn.ConvTranspose3d` module with lazy initialization of the ``in_channels`` argument.\n\n    The ``in_channels`` argument of the :class:`ConvTranspose3d` is inferred from\n    the ``input.size(1)``.\n    The attributes that will be lazily initialized are `weight` and `bias`.\n\n    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation\n    on lazy modules and their limitations.\n\n    Args:\n        out_channels (int): Number of channels produced by the convolution\n        kernel_size (int or tuple): Size of the convolving kernel\n        stride (int or tuple, optional): Stride of the convolution. Default: 1\n        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding\n            will be added to both sides of each dimension in the input. Default: 0\n        output_padding (int or tuple, optional): Additional size added to one side\n            of each dimension in the output shape. Default: 0\n        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n\n    .. seealso:: :class:`torch.nn.ConvTranspose3d` and :class:`torch.nn.modules.lazy.LazyModuleMixin`\n    \"\"\"\n\n    # super class define this variable as None. \"type: ignore[..] is required\n    # since we are redefining the variable.\n    cls_to_become = ConvTranspose3d  # type: ignore[assignment]\n\n    def __init__(\n        self,\n        out_channels: int,\n        kernel_size: _size_3_t,\n        stride: _size_3_t = 1,\n        padding: _size_3_t = 0,\n        output_padding: _size_3_t = 0,\n        groups: int = 1,\n        bias: bool = True,\n        dilation: _size_3_t = 1,\n        padding_mode: str = 'zeros',\n        device=None,\n        dtype=None\n    ) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(\n            0,\n            0,\n            kernel_size,\n            stride,\n            padding,\n            output_padding,\n            groups,\n            # bias is hardcoded to False to avoid creating tensor\n            # that will soon be overwritten.\n            False,\n            dilation,\n            padding_mode,\n            **factory_kwargs\n        )\n        self.weight = UninitializedParameter(**factory_kwargs)\n        self.out_channels = out_channels\n        if bias:\n            self.bias = UninitializedParameter(**factory_kwargs)\n\n    def _get_num_spatial_dims(self) -> int:\n        return 3\n", 1602], "C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py": ["from typing import Optional, Any\n\nimport torch\nfrom torch import Tensor\nfrom torch.nn.parameter import Parameter, UninitializedParameter, UninitializedBuffer\n\nfrom .. import functional as F\nfrom .. import init\nfrom ._functions import SyncBatchNorm as sync_batch_norm\nfrom .lazy import LazyModuleMixin\nfrom .module import Module\n\n__all__ = ['BatchNorm1d', 'LazyBatchNorm1d', 'BatchNorm2d', 'LazyBatchNorm2d', 'BatchNorm3d',\n           'LazyBatchNorm3d', 'SyncBatchNorm']\n\n\nclass _NormBase(Module):\n    \"\"\"Common base of _InstanceNorm and _BatchNorm\"\"\"\n\n    _version = 2\n    __constants__ = [\"track_running_stats\", \"momentum\", \"eps\", \"num_features\", \"affine\"]\n    num_features: int\n    eps: float\n    momentum: float\n    affine: bool\n    track_running_stats: bool\n    # WARNING: weight and bias purposely not defined here.\n    # See https://github.com/pytorch/pytorch/issues/39670\n\n    def __init__(\n        self,\n        num_features: int,\n        eps: float = 1e-5,\n        momentum: float = 0.1,\n        affine: bool = True,\n        track_running_stats: bool = True,\n        device=None,\n        dtype=None\n    ) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n        self.affine = affine\n        self.track_running_stats = track_running_stats\n        if self.affine:\n            self.weight = Parameter(torch.empty(num_features, **factory_kwargs))\n            self.bias = Parameter(torch.empty(num_features, **factory_kwargs))\n        else:\n            self.register_parameter(\"weight\", None)\n            self.register_parameter(\"bias\", None)\n        if self.track_running_stats:\n            self.register_buffer('running_mean', torch.zeros(num_features, **factory_kwargs))\n            self.register_buffer('running_var', torch.ones(num_features, **factory_kwargs))\n            self.running_mean: Optional[Tensor]\n            self.running_var: Optional[Tensor]\n            self.register_buffer('num_batches_tracked',\n                                 torch.tensor(0, dtype=torch.long,\n                                              **{k: v for k, v in factory_kwargs.items() if k != 'dtype'}))\n            self.num_batches_tracked: Optional[Tensor]\n        else:\n            self.register_buffer(\"running_mean\", None)\n            self.register_buffer(\"running_var\", None)\n            self.register_buffer(\"num_batches_tracked\", None)\n        self.reset_parameters()\n\n    def reset_running_stats(self) -> None:\n        if self.track_running_stats:\n            # running_mean/running_var/num_batches... are registered at runtime depending\n            # if self.track_running_stats is on\n            self.running_mean.zero_()  # type: ignore[union-attr]\n            self.running_var.fill_(1)  # type: ignore[union-attr]\n            self.num_batches_tracked.zero_()  # type: ignore[union-attr,operator]\n\n    def reset_parameters(self) -> None:\n        self.reset_running_stats()\n        if self.affine:\n            init.ones_(self.weight)\n            init.zeros_(self.bias)\n\n    def _check_input_dim(self, input):\n        raise NotImplementedError\n\n    def extra_repr(self):\n        return (\n            \"{num_features}, eps={eps}, momentum={momentum}, affine={affine}, \"\n            \"track_running_stats={track_running_stats}\".format(**self.__dict__)\n        )\n\n    def _load_from_state_dict(\n        self,\n        state_dict,\n        prefix,\n        local_metadata,\n        strict,\n        missing_keys,\n        unexpected_keys,\n        error_msgs,\n    ):\n        version = local_metadata.get(\"version\", None)\n\n        if (version is None or version < 2) and self.track_running_stats:\n            # at version 2: added num_batches_tracked buffer\n            #               this should have a default value of 0\n            num_batches_tracked_key = prefix + \"num_batches_tracked\"\n            if num_batches_tracked_key not in state_dict:\n                state_dict[num_batches_tracked_key] = (\n                    self.num_batches_tracked\n                    if self.num_batches_tracked is not None\n                    else torch.tensor(0, dtype=torch.long)\n                )\n\n        super()._load_from_state_dict(\n            state_dict,\n            prefix,\n            local_metadata,\n            strict,\n            missing_keys,\n            unexpected_keys,\n            error_msgs,\n        )\n\n\nclass _BatchNorm(_NormBase):\n    def __init__(\n        self,\n        num_features: int,\n        eps: float = 1e-5,\n        momentum: float = 0.1,\n        affine: bool = True,\n        track_running_stats: bool = True,\n        device=None,\n        dtype=None\n    ) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(\n            num_features, eps, momentum, affine, track_running_stats, **factory_kwargs\n        )\n\n    def forward(self, input: Tensor) -> Tensor:\n        self._check_input_dim(input)\n\n        # exponential_average_factor is set to self.momentum\n        # (when it is available) only so that it gets updated\n        # in ONNX graph when this node is exported to ONNX.\n        if self.momentum is None:\n            exponential_average_factor = 0.0\n        else:\n            exponential_average_factor = self.momentum\n\n        if self.training and self.track_running_stats:\n            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n                if self.momentum is None:  # use cumulative moving average\n                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n                else:  # use exponential moving average\n                    exponential_average_factor = self.momentum\n\n        r\"\"\"\n        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n        \"\"\"\n        if self.training:\n            bn_training = True\n        else:\n            bn_training = (self.running_mean is None) and (self.running_var is None)\n\n        r\"\"\"\n        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n        used for normalization (i.e. in eval mode when buffers are not None).\n        \"\"\"\n        return F.batch_norm(\n            input,\n            # If buffers are not to be tracked, ensure that they won't be updated\n            self.running_mean\n            if not self.training or self.track_running_stats\n            else None,\n            self.running_var if not self.training or self.track_running_stats else None,\n            self.weight,\n            self.bias,\n            bn_training,\n            exponential_average_factor,\n            self.eps,\n        )\n\n\nclass _LazyNormBase(LazyModuleMixin, _NormBase):\n\n    weight: UninitializedParameter  # type: ignore[assignment]\n    bias: UninitializedParameter  # type: ignore[assignment]\n\n    def __init__(self, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True,\n                 device=None, dtype=None) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(\n            # affine and track_running_stats are hardcoded to False to\n            # avoid creating tensors that will soon be overwritten.\n            0,\n            eps,\n            momentum,\n            False,\n            False,\n            **factory_kwargs,\n        )\n        self.affine = affine\n        self.track_running_stats = track_running_stats\n        if self.affine:\n            self.weight = UninitializedParameter(**factory_kwargs)\n            self.bias = UninitializedParameter(**factory_kwargs)\n        if self.track_running_stats:\n            self.running_mean = UninitializedBuffer(**factory_kwargs)\n            self.running_var = UninitializedBuffer(**factory_kwargs)\n            self.num_batches_tracked = torch.tensor(\n                0, dtype=torch.long, **{k: v for k, v in factory_kwargs.items() if k != 'dtype'})\n\n    def reset_parameters(self) -> None:\n        if not self.has_uninitialized_params() and self.num_features != 0:\n            super().reset_parameters()\n\n    def initialize_parameters(self, input) -> None:  # type: ignore[override]\n        if self.has_uninitialized_params():\n            self.num_features = input.shape[1]\n            if self.affine:\n                assert isinstance(self.weight, UninitializedParameter)\n                assert isinstance(self.bias, UninitializedParameter)\n                self.weight.materialize((self.num_features,))\n                self.bias.materialize((self.num_features,))\n            if self.track_running_stats:\n                self.running_mean.materialize((self.num_features,))  # type:ignore[union-attr]\n                self.running_var.materialize((self.num_features,))  # type:ignore[union-attr]\n            self.reset_parameters()\n\n\nclass BatchNorm1d(_BatchNorm):\n    r\"\"\"Applies Batch Normalization over a 2D or 3D input as described in the paper\n    `Batch Normalization: Accelerating Deep Network Training by Reducing\n    Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .\n\n    .. math::\n\n        y = \\frac{x - \\mathrm{E}[x]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\n\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and :math:`\\gamma` and :math:`\\beta` are learnable parameter vectors\n    of size `C` (where `C` is the number of features or channels of the input). By default, the\n    elements of :math:`\\gamma` are set to 1 and the elements of :math:`\\beta` are set to 0.\n    At train time in the forward pass, the standard-deviation is calculated via the biased estimator,\n    equivalent to ``torch.var(input, unbiased=False)``. However, the value stored in the\n    moving average of the standard-deviation is calculated via the unbiased  estimator, equivalent to\n    ``torch.var(input, unbiased=True)``.\n\n    Also by default, during training this layer keeps running estimates of its\n    computed mean and variance, which are then used for normalization during\n    evaluation. The running estimates are kept with a default :attr:`momentum`\n    of 0.1.\n\n    If :attr:`track_running_stats` is set to ``False``, this layer then does not\n    keep running estimates, and batch statistics are instead used during\n    evaluation time as well.\n\n    .. note::\n        This :attr:`momentum` argument is different from one used in optimizer\n        classes and the conventional notion of momentum. Mathematically, the\n        update rule for running statistics here is\n        :math:`\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t`,\n        where :math:`\\hat{x}` is the estimated statistic and :math:`x_t` is the\n        new observed value.\n\n    Because the Batch Normalization is done over the `C` dimension, computing statistics\n    on `(N, L)` slices, it's common terminology to call this Temporal Batch Normalization.\n\n    Args:\n        num_features: number of features or channels :math:`C` of the input\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Can be set to ``None`` for cumulative moving average\n            (i.e. simple average). Default: 0.1\n        affine: a boolean value that when set to ``True``, this module has\n            learnable affine parameters. Default: ``True``\n        track_running_stats: a boolean value that when set to ``True``, this\n            module tracks the running mean and variance, and when set to ``False``,\n            this module does not track such statistics, and initializes statistics\n            buffers :attr:`running_mean` and :attr:`running_var` as ``None``.\n            When these buffers are ``None``, this module always uses batch statistics.\n            in both training and eval modes. Default: ``True``\n\n    Shape:\n        - Input: :math:`(N, C)` or :math:`(N, C, L)`, where :math:`N` is the batch size,\n          :math:`C` is the number of features or channels, and :math:`L` is the sequence length\n        - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)\n\n    Examples::\n\n        >>> # With Learnable Parameters\n        >>> m = nn.BatchNorm1d(100)\n        >>> # Without Learnable Parameters\n        >>> m = nn.BatchNorm1d(100, affine=False)\n        >>> input = torch.randn(20, 100)\n        >>> output = m(input)\n    \"\"\"\n\n    def _check_input_dim(self, input):\n        if input.dim() != 2 and input.dim() != 3:\n            raise ValueError(\n                f\"expected 2D or 3D input (got {input.dim()}D input)\"\n            )\n\n\nclass LazyBatchNorm1d(_LazyNormBase, _BatchNorm):\n    r\"\"\"A :class:`torch.nn.BatchNorm1d` module with lazy initialization of\n    the ``num_features`` argument of the :class:`BatchNorm1d` that is inferred\n    from the ``input.size(1)``.\n    The attributes that will be lazily initialized are `weight`, `bias`,\n    `running_mean` and `running_var`.\n\n    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation\n    on lazy modules and their limitations.\n\n    Args:\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Can be set to ``None`` for cumulative moving average\n            (i.e. simple average). Default: 0.1\n        affine: a boolean value that when set to ``True``, this module has\n            learnable affine parameters. Default: ``True``\n        track_running_stats: a boolean value that when set to ``True``, this\n            module tracks the running mean and variance, and when set to ``False``,\n            this module does not track such statistics, and initializes statistics\n            buffers :attr:`running_mean` and :attr:`running_var` as ``None``.\n            When these buffers are ``None``, this module always uses batch statistics.\n            in both training and eval modes. Default: ``True``\n    \"\"\"\n\n    cls_to_become = BatchNorm1d  # type: ignore[assignment]\n\n    def _check_input_dim(self, input):\n        if input.dim() != 2 and input.dim() != 3:\n            raise ValueError(\n                f\"expected 2D or 3D input (got {input.dim()}D input)\"\n            )\n\n\nclass BatchNorm2d(_BatchNorm):\n    r\"\"\"Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs\n    with additional channel dimension) as described in the paper\n    `Batch Normalization: Accelerating Deep Network Training by Reducing\n    Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .\n\n    .. math::\n\n        y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\n\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and :math:`\\gamma` and :math:`\\beta` are learnable parameter vectors\n    of size `C` (where `C` is the input size). By default, the elements of :math:`\\gamma` are set\n    to 1 and the elements of :math:`\\beta` are set to 0. At train time in the forward pass, the\n    standard-deviation is calculated via the biased estimator, equivalent to\n    ``torch.var(input, unbiased=False)``. However, the value stored in the moving average of the\n    standard-deviation is calculated via the unbiased  estimator, equivalent to\n    ``torch.var(input, unbiased=True)``.\n\n    Also by default, during training this layer keeps running estimates of its\n    computed mean and variance, which are then used for normalization during\n    evaluation. The running estimates are kept with a default :attr:`momentum`\n    of 0.1.\n\n    If :attr:`track_running_stats` is set to ``False``, this layer then does not\n    keep running estimates, and batch statistics are instead used during\n    evaluation time as well.\n\n    .. note::\n        This :attr:`momentum` argument is different from one used in optimizer\n        classes and the conventional notion of momentum. Mathematically, the\n        update rule for running statistics here is\n        :math:`\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t`,\n        where :math:`\\hat{x}` is the estimated statistic and :math:`x_t` is the\n        new observed value.\n\n    Because the Batch Normalization is done over the `C` dimension, computing statistics\n    on `(N, H, W)` slices, it's common terminology to call this Spatial Batch Normalization.\n\n    Args:\n        num_features: :math:`C` from an expected input of size\n            :math:`(N, C, H, W)`\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Can be set to ``None`` for cumulative moving average\n            (i.e. simple average). Default: 0.1\n        affine: a boolean value that when set to ``True``, this module has\n            learnable affine parameters. Default: ``True``\n        track_running_stats: a boolean value that when set to ``True``, this\n            module tracks the running mean and variance, and when set to ``False``,\n            this module does not track such statistics, and initializes statistics\n            buffers :attr:`running_mean` and :attr:`running_var` as ``None``.\n            When these buffers are ``None``, this module always uses batch statistics.\n            in both training and eval modes. Default: ``True``\n\n    Shape:\n        - Input: :math:`(N, C, H, W)`\n        - Output: :math:`(N, C, H, W)` (same shape as input)\n\n    Examples::\n\n        >>> # With Learnable Parameters\n        >>> m = nn.BatchNorm2d(100)\n        >>> # Without Learnable Parameters\n        >>> m = nn.BatchNorm2d(100, affine=False)\n        >>> input = torch.randn(20, 100, 35, 45)\n        >>> output = m(input)\n    \"\"\"\n\n    def _check_input_dim(self, input):\n        if input.dim() != 4:\n            raise ValueError(f\"expected 4D input (got {input.dim()}D input)\")\n\n\nclass LazyBatchNorm2d(_LazyNormBase, _BatchNorm):\n    r\"\"\"A :class:`torch.nn.BatchNorm2d` module with lazy initialization of\n    the ``num_features`` argument of the :class:`BatchNorm2d` that is inferred\n    from the ``input.size(1)``.\n    The attributes that will be lazily initialized are `weight`, `bias`,\n    `running_mean` and `running_var`.\n\n    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation\n    on lazy modules and their limitations.\n\n    Args:\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Can be set to ``None`` for cumulative moving average\n            (i.e. simple average). Default: 0.1\n        affine: a boolean value that when set to ``True``, this module has\n            learnable affine parameters. Default: ``True``\n        track_running_stats: a boolean value that when set to ``True``, this\n            module tracks the running mean and variance, and when set to ``False``,\n            this module does not track such statistics, and initializes statistics\n            buffers :attr:`running_mean` and :attr:`running_var` as ``None``.\n            When these buffers are ``None``, this module always uses batch statistics.\n            in both training and eval modes. Default: ``True``\n    \"\"\"\n\n    cls_to_become = BatchNorm2d  # type: ignore[assignment]\n\n    def _check_input_dim(self, input):\n        if input.dim() != 4:\n            raise ValueError(f\"expected 4D input (got {input.dim()}D input)\")\n\n\nclass BatchNorm3d(_BatchNorm):\n    r\"\"\"Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs\n    with additional channel dimension) as described in the paper\n    `Batch Normalization: Accelerating Deep Network Training by Reducing\n    Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .\n\n    .. math::\n\n        y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\n\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and :math:`\\gamma` and :math:`\\beta` are learnable parameter vectors\n    of size `C` (where `C` is the input size). By default, the elements of :math:`\\gamma` are set\n    to 1 and the elements of :math:`\\beta` are set to 0. At train time in the forward pass, the\n    standard-deviation is calculated via the biased estimator, equivalent to\n    ``torch.var(input, unbiased=False)``. However, the value stored in the moving average of the\n    standard-deviation is calculated via the unbiased  estimator, equivalent to\n    ``torch.var(input, unbiased=True)``.\n\n    Also by default, during training this layer keeps running estimates of its\n    computed mean and variance, which are then used for normalization during\n    evaluation. The running estimates are kept with a default :attr:`momentum`\n    of 0.1.\n\n    If :attr:`track_running_stats` is set to ``False``, this layer then does not\n    keep running estimates, and batch statistics are instead used during\n    evaluation time as well.\n\n    .. note::\n        This :attr:`momentum` argument is different from one used in optimizer\n        classes and the conventional notion of momentum. Mathematically, the\n        update rule for running statistics here is\n        :math:`\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t`,\n        where :math:`\\hat{x}` is the estimated statistic and :math:`x_t` is the\n        new observed value.\n\n    Because the Batch Normalization is done over the `C` dimension, computing statistics\n    on `(N, D, H, W)` slices, it's common terminology to call this Volumetric Batch Normalization\n    or Spatio-temporal Batch Normalization.\n\n    Args:\n        num_features: :math:`C` from an expected input of size\n            :math:`(N, C, D, H, W)`\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Can be set to ``None`` for cumulative moving average\n            (i.e. simple average). Default: 0.1\n        affine: a boolean value that when set to ``True``, this module has\n            learnable affine parameters. Default: ``True``\n        track_running_stats: a boolean value that when set to ``True``, this\n            module tracks the running mean and variance, and when set to ``False``,\n            this module does not track such statistics, and initializes statistics\n            buffers :attr:`running_mean` and :attr:`running_var` as ``None``.\n            When these buffers are ``None``, this module always uses batch statistics.\n            in both training and eval modes. Default: ``True``\n\n    Shape:\n        - Input: :math:`(N, C, D, H, W)`\n        - Output: :math:`(N, C, D, H, W)` (same shape as input)\n\n    Examples::\n\n        >>> # With Learnable Parameters\n        >>> m = nn.BatchNorm3d(100)\n        >>> # Without Learnable Parameters\n        >>> m = nn.BatchNorm3d(100, affine=False)\n        >>> input = torch.randn(20, 100, 35, 45, 10)\n        >>> output = m(input)\n    \"\"\"\n\n    def _check_input_dim(self, input):\n        if input.dim() != 5:\n            raise ValueError(f\"expected 5D input (got {input.dim()}D input)\")\n\n\nclass LazyBatchNorm3d(_LazyNormBase, _BatchNorm):\n    r\"\"\"A :class:`torch.nn.BatchNorm3d` module with lazy initialization of\n    the ``num_features`` argument of the :class:`BatchNorm3d` that is inferred\n    from the ``input.size(1)``.\n    The attributes that will be lazily initialized are `weight`, `bias`,\n    `running_mean` and `running_var`.\n\n    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation\n    on lazy modules and their limitations.\n\n    Args:\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Can be set to ``None`` for cumulative moving average\n            (i.e. simple average). Default: 0.1\n        affine: a boolean value that when set to ``True``, this module has\n            learnable affine parameters. Default: ``True``\n        track_running_stats: a boolean value that when set to ``True``, this\n            module tracks the running mean and variance, and when set to ``False``,\n            this module does not track such statistics, and initializes statistics\n            buffers :attr:`running_mean` and :attr:`running_var` as ``None``.\n            When these buffers are ``None``, this module always uses batch statistics.\n            in both training and eval modes. Default: ``True``\n    \"\"\"\n\n    cls_to_become = BatchNorm3d  # type: ignore[assignment]\n\n    def _check_input_dim(self, input):\n        if input.dim() != 5:\n            raise ValueError(f\"expected 5D input (got {input.dim()}D input)\")\n\n\nclass SyncBatchNorm(_BatchNorm):\n    r\"\"\"Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs\n    with additional channel dimension) as described in the paper\n    `Batch Normalization: Accelerating Deep Network Training by Reducing\n    Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .\n\n    .. math::\n\n        y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\n\n    The mean and standard-deviation are calculated per-dimension over all\n    mini-batches of the same process groups. :math:`\\gamma` and :math:`\\beta`\n    are learnable parameter vectors of size `C` (where `C` is the input size).\n    By default, the elements of :math:`\\gamma` are sampled from\n    :math:`\\mathcal{U}(0, 1)` and the elements of :math:`\\beta` are set to 0.\n    The standard-deviation is calculated via the biased estimator, equivalent to\n    `torch.var(input, unbiased=False)`.\n\n    Also by default, during training this layer keeps running estimates of its\n    computed mean and variance, which are then used for normalization during\n    evaluation. The running estimates are kept with a default :attr:`momentum`\n    of 0.1.\n\n    If :attr:`track_running_stats` is set to ``False``, this layer then does not\n    keep running estimates, and batch statistics are instead used during\n    evaluation time as well.\n\n    .. note::\n        This :attr:`momentum` argument is different from one used in optimizer\n        classes and the conventional notion of momentum. Mathematically, the\n        update rule for running statistics here is\n        :math:`\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t`,\n        where :math:`\\hat{x}` is the estimated statistic and :math:`x_t` is the\n        new observed value.\n\n    Because the Batch Normalization is done for each channel in the ``C`` dimension, computing\n    statistics on ``(N, +)`` slices, it's common terminology to call this Volumetric Batch\n    Normalization or Spatio-temporal Batch Normalization.\n\n    Currently :class:`SyncBatchNorm` only supports\n    :class:`~torch.nn.DistributedDataParallel` (DDP) with single GPU per process. Use\n    :meth:`torch.nn.SyncBatchNorm.convert_sync_batchnorm()` to convert\n    :attr:`BatchNorm*D` layer to :class:`SyncBatchNorm` before wrapping\n    Network with DDP.\n\n    Args:\n        num_features: :math:`C` from an expected input of size\n            :math:`(N, C, +)`\n        eps: a value added to the denominator for numerical stability.\n            Default: ``1e-5``\n        momentum: the value used for the running_mean and running_var\n            computation. Can be set to ``None`` for cumulative moving average\n            (i.e. simple average). Default: 0.1\n        affine: a boolean value that when set to ``True``, this module has\n            learnable affine parameters. Default: ``True``\n        track_running_stats: a boolean value that when set to ``True``, this\n            module tracks the running mean and variance, and when set to ``False``,\n            this module does not track such statistics, and initializes statistics\n            buffers :attr:`running_mean` and :attr:`running_var` as ``None``.\n            When these buffers are ``None``, this module always uses batch statistics.\n            in both training and eval modes. Default: ``True``\n        process_group: synchronization of stats happen within each process group\n            individually. Default behavior is synchronization across the whole\n            world\n\n    Shape:\n        - Input: :math:`(N, C, +)`\n        - Output: :math:`(N, C, +)` (same shape as input)\n\n    .. note::\n        Synchronization of batchnorm statistics occurs only while training, i.e.\n        synchronization is disabled when ``model.eval()`` is set or if\n        ``self.training`` is otherwise ``False``.\n\n    Examples::\n\n        >>> # xdoctest: +SKIP\n        >>> # With Learnable Parameters\n        >>> m = nn.SyncBatchNorm(100)\n        >>> # creating process group (optional)\n        >>> # ranks is a list of int identifying rank ids.\n        >>> ranks = list(range(8))\n        >>> r1, r2 = ranks[:4], ranks[4:]\n        >>> # Note: every rank calls into new_group for every\n        >>> # process group created, even if that rank is not\n        >>> # part of the group.\n        >>> process_groups = [torch.distributed.new_group(pids) for pids in [r1, r2]]\n        >>> process_group = process_groups[0 if dist.get_rank() <= 3 else 1]\n        >>> # Without Learnable Parameters\n        >>> m = nn.BatchNorm3d(100, affine=False, process_group=process_group)\n        >>> input = torch.randn(20, 100, 35, 45, 10)\n        >>> output = m(input)\n\n        >>> # network is nn.BatchNorm layer\n        >>> sync_bn_network = nn.SyncBatchNorm.convert_sync_batchnorm(network, process_group)\n        >>> # only single gpu per process is currently supported\n        >>> ddp_sync_bn_network = torch.nn.parallel.DistributedDataParallel(\n        >>>                         sync_bn_network,\n        >>>                         device_ids=[args.local_rank],\n        >>>                         output_device=args.local_rank)\n    \"\"\"\n\n    def __init__(\n        self,\n        num_features: int,\n        eps: float = 1e-5,\n        momentum: float = 0.1,\n        affine: bool = True,\n        track_running_stats: bool = True,\n        process_group: Optional[Any] = None,\n        device=None,\n        dtype=None\n    ) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(\n            num_features, eps, momentum, affine, track_running_stats, **factory_kwargs\n        )\n        self.process_group = process_group\n\n    def _check_input_dim(self, input):\n        if input.dim() < 2:\n            raise ValueError(\n                f\"expected at least 2D input (got {input.dim()}D input)\"\n            )\n\n    def _check_non_zero_input_channels(self, input):\n        if input.size(1) == 0:\n            raise ValueError(\n                \"SyncBatchNorm number of input channels should be non-zero\"\n            )\n\n    def forward(self, input: Tensor) -> Tensor:\n        self._check_input_dim(input)\n        self._check_non_zero_input_channels(input)\n\n        # exponential_average_factor is set to self.momentum\n        # (when it is available) only so that it gets updated\n        # in ONNX graph when this node is exported to ONNX.\n        if self.momentum is None:\n            exponential_average_factor = 0.0\n        else:\n            exponential_average_factor = self.momentum\n\n        if self.training and self.track_running_stats:\n            assert self.num_batches_tracked is not None\n            self.num_batches_tracked.add_(1)\n            if self.momentum is None:  # use cumulative moving average\n                exponential_average_factor = 1.0 / self.num_batches_tracked.item()\n            else:  # use exponential moving average\n                exponential_average_factor = self.momentum\n\n        r\"\"\"\n        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n        \"\"\"\n        if self.training:\n            bn_training = True\n        else:\n            bn_training = (self.running_mean is None) and (self.running_var is None)\n\n        r\"\"\"\n        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n        used for normalization (i.e. in eval mode when buffers are not None).\n        \"\"\"\n        # If buffers are not to be tracked, ensure that they won't be updated\n        running_mean = (\n            self.running_mean if not self.training or self.track_running_stats else None\n        )\n        running_var = (\n            self.running_var if not self.training or self.track_running_stats else None\n        )\n\n        # Don't sync batchnorm stats in inference mode (model.eval()).\n        need_sync = (bn_training and self.training and\n                     torch.distributed.is_available() and torch.distributed.is_initialized())\n        if need_sync:\n            # currently only GPU/PrivateUse1 input is supported\n            if input.device.type not in [\"cuda\", torch._C._get_privateuse1_backend_name()]:\n                raise ValueError(\"SyncBatchNorm expected input tensor to be on GPU or \"\n                                 f\"{torch._C._get_privateuse1_backend_name()}\")\n\n            process_group = torch.distributed.group.WORLD\n            if self.process_group:\n                process_group = self.process_group\n            world_size = torch.distributed.get_world_size(process_group)\n            need_sync = world_size > 1\n\n        # fallback to framework BN when synchronization is not necessary\n        if not need_sync:\n            return F.batch_norm(\n                input,\n                running_mean,\n                running_var,\n                self.weight,\n                self.bias,\n                bn_training,\n                exponential_average_factor,\n                self.eps,\n            )\n        else:\n            assert bn_training\n            return sync_batch_norm.apply(\n                input,\n                self.weight,\n                self.bias,\n                running_mean,\n                running_var,\n                self.eps,\n                exponential_average_factor,\n                process_group,\n                world_size,\n            )\n\n    @classmethod\n    def convert_sync_batchnorm(cls, module, process_group=None):\n        r\"\"\"Helper function to convert all :attr:`BatchNorm*D` layers in the model to\n        :class:`torch.nn.SyncBatchNorm` layers.\n\n        Args:\n            module (nn.Module): module containing one or more :attr:`BatchNorm*D` layers\n            process_group (optional): process group to scope synchronization,\n                default is the whole world\n\n        Returns:\n            The original :attr:`module` with the converted :class:`torch.nn.SyncBatchNorm`\n            layers. If the original :attr:`module` is a :attr:`BatchNorm*D` layer,\n            a new :class:`torch.nn.SyncBatchNorm` layer object will be returned\n            instead.\n\n        Example::\n\n            >>> # Network with nn.BatchNorm layer\n            >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)\n            >>> module = torch.nn.Sequential(\n            >>>            torch.nn.Linear(20, 100),\n            >>>            torch.nn.BatchNorm1d(100),\n            >>>          ).cuda()\n            >>> # creating process group (optional)\n            >>> # ranks is a list of int identifying rank ids.\n            >>> ranks = list(range(8))\n            >>> r1, r2 = ranks[:4], ranks[4:]\n            >>> # Note: every rank calls into new_group for every\n            >>> # process group created, even if that rank is not\n            >>> # part of the group.\n            >>> # xdoctest: +SKIP(\"distributed\")\n            >>> process_groups = [torch.distributed.new_group(pids) for pids in [r1, r2]]\n            >>> process_group = process_groups[0 if dist.get_rank() <= 3 else 1]\n            >>> sync_bn_module = torch.nn.SyncBatchNorm.convert_sync_batchnorm(module, process_group)\n\n        \"\"\"\n        module_output = module\n        if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):\n            module_output = torch.nn.SyncBatchNorm(\n                module.num_features,\n                module.eps,\n                module.momentum,\n                module.affine,\n                module.track_running_stats,\n                process_group,\n            )\n            if module.affine:\n                with torch.no_grad():\n                    module_output.weight = module.weight\n                    module_output.bias = module.bias\n            module_output.running_mean = module.running_mean\n            module_output.running_var = module.running_var\n            module_output.num_batches_tracked = module.num_batches_tracked\n            module_output.training = module.training\n            if hasattr(module, \"qconfig\"):\n                module_output.qconfig = module.qconfig\n        for name, child in module.named_children():\n            module_output.add_module(\n                name, cls.convert_sync_batchnorm(child, process_group)\n            )\n        del module\n        return module_output\n", 841], "C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py": ["import types\nfrom contextlib import contextmanager\n\n# The idea for this parameter is that we forbid bare assignment\n# to torch.backends.<cudnn|mkldnn>.enabled and friends when running our\n# test suite, where it's very easy to forget to undo the change\n# later.\n__allow_nonbracketed_mutation_flag = True\n\n\ndef disable_global_flags():\n    global __allow_nonbracketed_mutation_flag\n    __allow_nonbracketed_mutation_flag = False\n\n\ndef flags_frozen():\n    return not __allow_nonbracketed_mutation_flag\n\n\n@contextmanager\ndef __allow_nonbracketed_mutation():\n    global __allow_nonbracketed_mutation_flag\n    old = __allow_nonbracketed_mutation_flag\n    __allow_nonbracketed_mutation_flag = True\n    try:\n        yield\n    finally:\n        __allow_nonbracketed_mutation_flag = old\n\n\nclass ContextProp:\n    def __init__(self, getter, setter):\n        self.getter = getter\n        self.setter = setter\n\n    def __get__(self, obj, objtype):\n        return self.getter()\n\n    def __set__(self, obj, val):\n        if not flags_frozen():\n            self.setter(val)\n        else:\n            raise RuntimeError(\n                \"not allowed to set %s flags \"\n                \"after disable_global_flags; please use flags() context manager instead\"\n                % obj.__name__\n            )\n\n\nclass PropModule(types.ModuleType):\n    def __init__(self, m, name):\n        super().__init__(name)\n        self.m = m\n\n    def __getattr__(self, attr):\n        return self.m.__getattribute__(attr)\n\n\nfrom torch.backends import (\n    cpu as cpu,\n    cuda as cuda,\n    cudnn as cudnn,\n    mkl as mkl,\n    mkldnn as mkldnn,\n    mps as mps,\n    openmp as openmp,\n    quantized as quantized,\n)\n", 68], "C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py": ["\"\"\"Functional interface.\"\"\"\nfrom typing import Callable, List, Optional, Tuple, Union\nimport math\nimport warnings\nimport importlib\n\ntry:\n    import numpy as np\nexcept ModuleNotFoundError:\n    np = None\n\nimport torch\nfrom torch import _VF\nfrom torch import sym_int as _sym_int\nfrom torch._C import _infer_size, _add_docstr\nfrom torch._torch_docs import reproducibility_notes, tf32_notes, sparse_support_notes\n# A workaround to support both TorchScript and MyPy:\nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from torch.types import _dtype as DType\nelse:\n    # The JIT doesn't understand Union, nor torch.dtype here\n    DType = int\n\nfrom .._jit_internal import boolean_dispatch, _overload, BroadcastingList1, BroadcastingList2, BroadcastingList3\nfrom ..overrides import (\n    has_torch_function, has_torch_function_unary, has_torch_function_variadic,\n    handle_torch_function)\nfrom . import _reduction as _Reduction\nfrom . import grad  # noqa: F401\nfrom .modules import utils\nfrom .modules.utils import _single, _pair, _triple, _list_with_default\n\nTensor = torch.Tensor\n\nconv1d = _add_docstr(\n    torch.conv1d,\n    r\"\"\"\nconv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n\nApplies a 1D convolution over an input signal composed of several input\nplanes.\n\n{tf32_note}\n\nSee :class:`~torch.nn.Conv1d` for details and output shape.\n\nNote:\n    {cudnn_reproducibility_note}\n\nNote:\n    This operator supports complex data types i.e. ``complex32, complex64, complex128``.\n\"\"\".format(\n        **reproducibility_notes, **tf32_notes\n    )\n    + r\"\"\"\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`\n    weight: filters of shape :math:`(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kW)`\n    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: ``None``\n    stride: the stride of the convolving kernel. Can be a single number or\n      a one-element tuple `(sW,)`. Default: 1\n    padding: implicit paddings on both sides of the input. Can be a string {'valid', 'same'},\n      single number or a one-element tuple `(padW,)`. Default: 0\n      ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n      the input so the output has the same shape as the input. However, this mode\n      doesn't support any stride values other than 1.\n\n      .. warning::\n          For ``padding='same'``, if the ``weight`` is even-length and\n          ``dilation`` is odd in any dimension, a full :func:`pad` operation\n          may be needed internally. Lowering performance.\n    dilation: the spacing between kernel elements. Can be a single number or\n      a one-element tuple `(dW,)`. Default: 1\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by\n      the number of groups. Default: 1\n\nExamples::\n\n    >>> inputs = torch.randn(33, 16, 30)\n    >>> filters = torch.randn(20, 16, 5)\n    >>> F.conv1d(inputs, filters)\n\"\"\",\n)\n\nconv2d = _add_docstr(\n    torch.conv2d,\n    r\"\"\"\nconv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n\nApplies a 2D convolution over an input image composed of several input\nplanes.\n\n{tf32_note}\n\nSee :class:`~torch.nn.Conv2d` for details and output shape.\n\nNote:\n    {cudnn_reproducibility_note}\n\nNote:\n    This operator supports complex data types i.e. ``complex32, complex64, complex128``.\n\"\"\".format(\n        **reproducibility_notes, **tf32_notes\n    )\n    + r\"\"\"\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`\n    weight: filters of shape :math:`(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kH , kW)`\n    bias: optional bias tensor of shape :math:`(\\text{out\\_channels})`. Default: ``None``\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple `(sH, sW)`. Default: 1\n    padding: implicit paddings on both sides of the input. Can be a string {'valid', 'same'},\n      single number or a tuple `(padH, padW)`. Default: 0\n      ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n      the input so the output has the same shape as the input. However, this mode\n      doesn't support any stride values other than 1.\n\n      .. warning::\n          For ``padding='same'``, if the ``weight`` is even-length and\n          ``dilation`` is odd in any dimension, a full :func:`pad` operation\n          may be needed internally. Lowering performance.\n\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple `(dH, dW)`. Default: 1\n    groups: split input into groups, both :math:`\\text{in\\_channels}` and :math:`\\text{out\\_channels}`\n      should be divisible by the number of groups. Default: 1\n\nExamples::\n\n    >>> # With square kernels and equal stride\n    >>> filters = torch.randn(8, 4, 3, 3)\n    >>> inputs = torch.randn(1, 4, 5, 5)\n    >>> F.conv2d(inputs, filters, padding=1)\n\"\"\",\n)  # noqa: E501\n\nconv3d = _add_docstr(\n    torch.conv3d,\n    r\"\"\"\nconv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n\nApplies a 3D convolution over an input image composed of several input\nplanes.\n\n{tf32_note}\n\nSee :class:`~torch.nn.Conv3d` for details and output shape.\n\nNote:\n    {cudnn_reproducibility_note}\n\nNote:\n    This operator supports complex data types i.e. ``complex32, complex64, complex128``.\n\"\"\".format(\n        **reproducibility_notes, **tf32_notes\n    )\n    + r\"\"\"\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iT , iH , iW)`\n    weight: filters of shape :math:`(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kT , kH , kW)`\n    bias: optional bias tensor of shape :math:`(\\text{out\\_channels})`. Default: None\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple `(sT, sH, sW)`. Default: 1\n    padding: implicit paddings on both sides of the input. Can be a string {'valid', 'same'},\n      single number or a tuple `(padT, padH, padW)`. Default: 0\n      ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n      the input so the output has the same shape as the input. However, this mode\n      doesn't support any stride values other than 1.\n\n      .. warning::\n          For ``padding='same'``, if the ``weight`` is even-length and\n          ``dilation`` is odd in any dimension, a full :func:`pad` operation\n          may be needed internally. Lowering performance.\n\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple `(dT, dH, dW)`. Default: 1\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by\n      the number of groups. Default: 1\n\nExamples::\n\n    >>> filters = torch.randn(33, 16, 3, 3, 3)\n    >>> inputs = torch.randn(20, 16, 50, 10, 20)\n    >>> F.conv3d(inputs, filters)\n\"\"\",\n)  # noqa: E501\n\nconv_transpose1d = _add_docstr(\n    torch.conv_transpose1d,\n    r\"\"\"\nconv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n\nApplies a 1D transposed convolution operator over an input signal\ncomposed of several input planes, sometimes also called \"deconvolution\".\n\n{tf32_note}\n\nSee :class:`~torch.nn.ConvTranspose1d` for details and output shape.\n\nNote:\n    {cudnn_reproducibility_note}\n\"\"\".format(\n        **reproducibility_notes, **tf32_notes\n    )\n    + r\"\"\"\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`\n    weight: filters of shape :math:`(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kW)`\n    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: None\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple ``(sW,)``. Default: 1\n    padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both\n      sides of each dimension in the input. Can be a single number or a tuple\n      ``(padW,)``. Default: 0\n    output_padding: additional size added to one side of each dimension in the\n      output shape. Can be a single number or a tuple ``(out_padW)``. Default: 0\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n      number of groups. Default: 1\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple ``(dW,)``. Default: 1\n\nExamples::\n\n    >>> inputs = torch.randn(20, 16, 50)\n    >>> weights = torch.randn(16, 33, 5)\n    >>> F.conv_transpose1d(inputs, weights)\n\"\"\",\n)\n\nconv_transpose2d = _add_docstr(\n    torch.conv_transpose2d,\n    r\"\"\"\nconv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n\nApplies a 2D transposed convolution operator over an input image\ncomposed of several input planes, sometimes also called \"deconvolution\".\n\n{tf32_note}\n\nSee :class:`~torch.nn.ConvTranspose2d` for details and output shape.\n\nNote:\n    {cudnn_reproducibility_note}\n\"\"\".format(\n        **reproducibility_notes, **tf32_notes\n    )\n    + r\"\"\"\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`\n    weight: filters of shape :math:`(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kH , kW)`\n    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: None\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple ``(sH, sW)``. Default: 1\n    padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both\n      sides of each dimension in the input. Can be a single number or a tuple\n      ``(padH, padW)``. Default: 0\n    output_padding: additional size added to one side of each dimension in the\n      output shape. Can be a single number or a tuple ``(out_padH, out_padW)``.\n      Default: 0\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n      number of groups. Default: 1\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple ``(dH, dW)``. Default: 1\n\nExamples::\n\n    >>> # With square kernels and equal stride\n    >>> inputs = torch.randn(1, 4, 5, 5)\n    >>> weights = torch.randn(4, 8, 3, 3)\n    >>> F.conv_transpose2d(inputs, weights, padding=1)\n\"\"\",\n)  # noqa: E501\n\nconv_transpose3d = _add_docstr(\n    torch.conv_transpose3d,\n    r\"\"\"\nconv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n\nApplies a 3D transposed convolution operator over an input image\ncomposed of several input planes, sometimes also called \"deconvolution\"\n\n{tf32_note}\n\nSee :class:`~torch.nn.ConvTranspose3d` for details and output shape.\n\nNote:\n    {cudnn_reproducibility_note}\n\"\"\".format(\n        **reproducibility_notes, **tf32_notes\n    )\n    + r\"\"\"\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iT , iH , iW)`\n    weight: filters of shape :math:`(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kT , kH , kW)`\n    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: None\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple ``(sT, sH, sW)``. Default: 1\n    padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both\n      sides of each dimension in the input. Can be a single number or a tuple\n      ``(padT, padH, padW)``. Default: 0\n    output_padding: additional size added to one side of each dimension in the\n      output shape. Can be a single number or a tuple\n      ``(out_padT, out_padH, out_padW)``. Default: 0\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n      number of groups. Default: 1\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple `(dT, dH, dW)`. Default: 1\n\nExamples::\n\n    >>> inputs = torch.randn(20, 16, 50, 10, 20)\n    >>> weights = torch.randn(16, 33, 3, 3, 3)\n    >>> F.conv_transpose3d(inputs, weights)\n\"\"\",\n)  # noqa: E501\n\nconv_tbc = _add_docstr(\n    torch.conv_tbc,\n    r\"\"\"\nApplies a 1-dimensional sequence convolution over an input sequence.\nInput and output dimensions are (Time, Batch, Channels) - hence TBC.\n\nArgs:\n    input: input tensor of shape :math:`(\\text{sequence length} \\times batch \\times \\text{in\\_channels})`\n    weight: filter of shape (:math:`\\text{kernel width} \\times \\text{in\\_channels} \\times \\text{out\\_channels}`)\n    bias: bias of shape (:math:`\\text{out\\_channels}`)\n    pad: number of timesteps to pad. Default: 0\n\"\"\",\n)\n\n\n# Pooling\navg_pool1d = _add_docstr(\n    torch.avg_pool1d,\n    r\"\"\"\navg_pool1d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) -> Tensor\n\nApplies a 1D average pooling over an input signal composed of several\ninput planes.\n\nSee :class:`~torch.nn.AvgPool1d` for details and output shape.\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`\n    kernel_size: the size of the window. Can be a single number or a\n      tuple `(kW,)`\n    stride: the stride of the window. Can be a single number or a tuple\n      `(sW,)`. Default: :attr:`kernel_size`\n    padding: implicit zero paddings on both sides of the input. Can be a\n      single number or a tuple `(padW,)`. Default: 0\n    ceil_mode: when True, will use `ceil` instead of `floor` to compute the\n        output shape. Default: ``False``\n    count_include_pad: when True, will include the zero-padding in the\n        averaging calculation. Default: ``True``\n\nExamples::\n\n    >>> # pool of square window of size=3, stride=2\n    >>> input = torch.tensor([[[1, 2, 3, 4, 5, 6, 7]]], dtype=torch.float32)\n    >>> F.avg_pool1d(input, kernel_size=3, stride=2)\n    tensor([[[ 2.,  4.,  6.]]])\n\n\"\"\",\n)\n\n\navg_pool2d = _add_docstr(\n    torch._C._nn.avg_pool2d,\n    r\"\"\"\navg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) -> Tensor\n\nApplies 2D average-pooling operation in :math:`kH \\times kW` regions by step size\n:math:`sH \\times sW` steps. The number of output features is equal to the number of\ninput planes.\n\nSee :class:`~torch.nn.AvgPool2d` for details and output shape.\n\nArgs:\n    input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`\n    kernel_size: size of the pooling region. Can be a single number or a\n      tuple `(kH, kW)`\n    stride: stride of the pooling operation. Can be a single number or a\n      tuple `(sH, sW)`. Default: :attr:`kernel_size`\n    padding: implicit zero paddings on both sides of the input. Can be a\n      single number or a tuple `(padH, padW)`. Default: 0\n    ceil_mode: when True, will use `ceil` instead of `floor` in the formula\n        to compute the output shape. Default: ``False``\n    count_include_pad: when True, will include the zero-padding in the\n        averaging calculation. Default: ``True``\n    divisor_override: if specified, it will be used as divisor, otherwise\n         size of the pooling region will be used. Default: None\n\"\"\",\n)\n\navg_pool3d = _add_docstr(\n    torch._C._nn.avg_pool3d,\n    r\"\"\"\navg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) -> Tensor\n\nApplies 3D average-pooling operation in :math:`kT \\times kH \\times kW` regions by step\nsize :math:`sT \\times sH \\times sW` steps. The number of output features is equal to\n:math:`\\lfloor\\frac{\\text{input planes}}{sT}\\rfloor`.\n\nSee :class:`~torch.nn.AvgPool3d` for details and output shape.\n\nArgs:\n    input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iT \\times iH , iW)`\n    kernel_size: size of the pooling region. Can be a single number or a\n      tuple `(kT, kH, kW)`\n    stride: stride of the pooling operation. Can be a single number or a\n      tuple `(sT, sH, sW)`. Default: :attr:`kernel_size`\n    padding: implicit zero paddings on both sides of the input. Can be a\n      single number or a tuple `(padT, padH, padW)`, Default: 0\n    ceil_mode: when True, will use `ceil` instead of `floor` in the formula\n        to compute the output shape\n    count_include_pad: when True, will include the zero-padding in the\n        averaging calculation\n    divisor_override: if specified, it will be used as divisor, otherwise\n        size of the pooling region will be used. Default: None\n\"\"\",\n)\n\n\ndef fractional_max_pool2d_with_indices(\n    input: Tensor, kernel_size: BroadcastingList2[int],\n    output_size: Optional[BroadcastingList2[int]] = None,\n    output_ratio: Optional[BroadcastingList2[float]] = None,\n    return_indices: bool = False,\n    _random_samples: Optional[Tensor] = None\n) -> Tuple[Tensor, Tensor]:  # noqa: D400\n    r\"\"\"\n    fractional_max_pool2d(input, kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None)\n\n    Applies 2D fractional max pooling over an input signal composed of several input planes.\n\n    Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham\n\n    The max-pooling operation is applied in :math:`kH \\times kW` regions by a stochastic\n    step size determined by the target output size.\n    The number of output features is equal to the number of input planes.\n\n    Args:\n        kernel_size: the size of the window to take a max over.\n                     Can be a single number :math:`k` (for a square kernel of :math:`k \\times k`)\n                     or a tuple `(kH, kW)`\n        output_size: the target output size of the image of the form :math:`oH \\times oW`.\n                     Can be a tuple `(oH, oW)` or a single number :math:`oH` for a square image :math:`oH \\times oH`\n        output_ratio: If one wants to have an output size as a ratio of the input size, this option can be given.\n                      This has to be a number or tuple in the range (0, 1)\n        return_indices: if ``True``, will return the indices along with the outputs.\n                        Useful to pass to :func:`~torch.nn.functional.max_unpool2d`.\n\n    Examples::\n        >>> input = torch.randn(20, 16, 50, 32)\n        >>> # pool of square window of size=3, and target output size 13x12\n        >>> F.fractional_max_pool2d(input, 3, output_size=(13, 12))\n        >>> # pool of square window and target output size being half of input image size\n        >>> F.fractional_max_pool2d(input, 3, output_ratio=(0.5, 0.5))\n\n    .. _Fractional MaxPooling:\n        http://arxiv.org/abs/1412.6071\n    \"\"\"\n    if has_torch_function_variadic(input, _random_samples):\n        return handle_torch_function(\n            fractional_max_pool2d_with_indices,\n            (input, _random_samples),\n            input,\n            kernel_size,\n            output_size=output_size,\n            output_ratio=output_ratio,\n            return_indices=return_indices,\n            _random_samples=_random_samples,\n        )\n    if output_size is None and output_ratio is None:\n        raise ValueError(\"fractional_max_pool2d requires specifying either an output_size or an output_ratio\")\n    if output_size is None:\n        assert output_ratio is not None\n        if len(output_ratio) > 2:\n            raise ValueError(\"fractional_max_pool2d requires output_ratio to either be a single Int or tuple of Ints.\")\n        _output_ratio = _pair(output_ratio)\n        output_size = [int(input.size(-2) * _output_ratio[0]), int(input.size(-1) * _output_ratio[1])]\n\n    if _random_samples is None:\n        n_batch = 1 if input.dim() == 3 else input.size(0)\n        _random_samples = torch.rand(n_batch, input.size(-3), 2, dtype=input.dtype, device=input.device)\n    return torch._C._nn.fractional_max_pool2d(input, kernel_size, output_size, _random_samples)\n\n\ndef _fractional_max_pool2d(\n    input: Tensor, kernel_size: BroadcastingList2[int],\n    output_size: Optional[BroadcastingList2[int]] = None,\n    output_ratio: Optional[BroadcastingList2[float]] = None,\n    return_indices: bool = False,\n    _random_samples: Optional[Tensor] = None\n) -> Tensor:\n    if has_torch_function_variadic(input, _random_samples):\n        return handle_torch_function(\n            fractional_max_pool2d,\n            (input, _random_samples),\n            input,\n            kernel_size,\n            output_size=output_size,\n            output_ratio=output_ratio,\n            return_indices=return_indices,\n            _random_samples=_random_samples,\n        )\n    return fractional_max_pool2d_with_indices(\n        input, kernel_size, output_size, output_ratio, return_indices, _random_samples\n    )[0]\n\n\nfractional_max_pool2d = boolean_dispatch(\n    arg_name=\"return_indices\",\n    arg_index=4,\n    default=False,\n    if_true=fractional_max_pool2d_with_indices,\n    if_false=_fractional_max_pool2d,\n    module_name=__name__,\n    func_name=\"fractional_max_pool2d\",\n)\n\n\ndef fractional_max_pool3d_with_indices(\n    input: Tensor, kernel_size: BroadcastingList3[int],\n    output_size: Optional[BroadcastingList3[int]] = None,\n    output_ratio: Optional[BroadcastingList3[float]] = None,\n    return_indices: bool = False,\n    _random_samples: Optional[Tensor] = None\n) -> Tuple[Tensor, Tensor]:  # noqa: D400\n    r\"\"\"\n    fractional_max_pool3d(input, kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None)\n\n    Applies 3D fractional max pooling over an input signal composed of several input planes.\n\n    Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham\n\n    The max-pooling operation is applied in :math:`kT \\times kH \\times kW` regions by a stochastic\n    step size determined by the target output size.\n    The number of output features is equal to the number of input planes.\n\n    Args:\n        kernel_size: the size of the window to take a max over.\n                     Can be a single number :math:`k` (for a square kernel of :math:`k \\times k \\times k`)\n                     or a tuple `(kT, kH, kW)`\n        output_size: the target output size of the form :math:`oT \\times oH \\times oW`.\n                     Can be a tuple `(oT, oH, oW)` or a single number :math:`oH` for a cubic output\n                     :math:`oH \\times oH \\times oH`\n        output_ratio: If one wants to have an output size as a ratio of the input size, this option can be given.\n                      This has to be a number or tuple in the range (0, 1)\n        return_indices: if ``True``, will return the indices along with the outputs.\n                        Useful to pass to :func:`~torch.nn.functional.max_unpool3d`.\n\n    Shape:\n        - Input: :math:`(N, C, T_{in}, H_{in}, W_{in})` or :math:`(C, T_{in}, H_{in}, W_{in})`.\n        - Output: :math:`(N, C, T_{out}, H_{out}, W_{out})` or :math:`(C, T_{out}, H_{out}, W_{out})`, where\n          :math:`(T_{out}, H_{out}, W_{out})=\\text{output\\_size}` or\n          :math:`(T_{out}, H_{out}, W_{out})=\\text{output\\_ratio} \\times (T_{in}, H_{in}, W_{in})`\n\n    Examples::\n        >>> input = torch.randn(20, 16, 50, 32, 16)\n        >>> # pool of cubic window of size=3, and target output size 13x12x11\n        >>> F.fractional_max_pool3d(input, 3, output_size=(13, 12, 11))\n        >>> # pool of cubic window and target output size being half of input size\n        >>> F.fractional_max_pool3d(input, 3, output_ratio=(0.5, 0.5, 0.5))\n\n    .. _Fractional MaxPooling:\n        http://arxiv.org/abs/1412.6071\n    \"\"\"\n    if has_torch_function_variadic(input, _random_samples):\n        return handle_torch_function(\n            fractional_max_pool3d_with_indices,\n            (input, _random_samples),\n            input,\n            kernel_size,\n            output_size=output_size,\n            output_ratio=output_ratio,\n            return_indices=return_indices,\n            _random_samples=_random_samples,\n        )\n    if output_size is None and output_ratio is None:\n        raise ValueError(\"fractional_max_pool3d requires specifying either an output_size or an output_ratio\")\n    if output_size is None:\n        assert output_ratio is not None\n        _output_ratio = _triple(output_ratio)\n        output_size = [\n            int(input.size(-3) * _output_ratio[0]),\n            int(input.size(-2) * _output_ratio[1]),\n            int(input.size(-1) * _output_ratio[2]),\n        ]\n\n    if _random_samples is None:\n        n_batch = 1 if input.dim() == 4 else input.size(0)\n        _random_samples = torch.rand(n_batch, input.size(-4), 3, dtype=input.dtype, device=input.device)\n    return torch._C._nn.fractional_max_pool3d(input, kernel_size, output_size, _random_samples)\n\n\ndef _fractional_max_pool3d(\n    input: Tensor, kernel_size: BroadcastingList3[int],\n    output_size: Optional[BroadcastingList3[int]] = None,\n    output_ratio: Optional[BroadcastingList3[float]] = None,\n    return_indices: bool = False,\n    _random_samples: Optional[Tensor] = None\n) -> Tensor:\n    if has_torch_function_variadic(input, _random_samples):\n        return handle_torch_function(\n            fractional_max_pool3d,\n            (input, _random_samples),\n            input,\n            kernel_size,\n            output_size=output_size,\n            output_ratio=output_ratio,\n            return_indices=return_indices,\n            _random_samples=_random_samples,\n        )\n    return fractional_max_pool3d_with_indices(\n        input, kernel_size, output_size, output_ratio, return_indices, _random_samples\n    )[0]\n\n\nfractional_max_pool3d = boolean_dispatch(\n    arg_name=\"return_indices\",\n    arg_index=4,\n    default=False,\n    if_true=fractional_max_pool3d_with_indices,\n    if_false=_fractional_max_pool3d,\n    module_name=__name__,\n    func_name=\"fractional_max_pool3d\",\n)\n\n\ndef max_pool1d_with_indices(\n    input: Tensor, kernel_size: BroadcastingList1[int],\n    stride: Optional[BroadcastingList1[int]] = None,\n    padding: BroadcastingList1[int] = 0,\n    dilation: BroadcastingList1[int] = 1,\n    ceil_mode: bool = False,\n    return_indices: bool = False\n) -> Tuple[Tensor, Tensor]:  # noqa: D400\n    r\"\"\"\n    max_pool1d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)\n\n    Applies a 1D max pooling over an input signal composed of several input\n    planes.\n\n    .. note::\n        The order of :attr:`ceil_mode` and :attr:`return_indices` is different from\n        what seen in :class:`~torch.nn.MaxPool1d`, and will change in a future release.\n\n    See :class:`~torch.nn.MaxPool1d` for details.\n\n    Args:\n        input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`, minibatch dim optional.\n        kernel_size: the size of the window. Can be a single number or a\n            tuple `(kW,)`\n        stride: the stride of the window. Can be a single number or a tuple\n            `(sW,)`. Default: :attr:`kernel_size`\n        padding: Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.\n        dilation: The stride between elements within a sliding window, must be > 0.\n        ceil_mode: If ``True``, will use `ceil` instead of `floor` to compute the output shape. This\n                   ensures that every element in the input tensor is covered by a sliding window.\n        return_indices: If ``True``, will return the argmax along with the max values.\n                        Useful for :class:`torch.nn.functional.max_unpool1d` later\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            max_pool1d_with_indices,\n            (input,),\n            input,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            ceil_mode=ceil_mode,\n            return_indices=return_indices,\n        )\n    if stride is None:\n        stride = torch.jit.annotate(List[int], [])\n    return torch.max_pool1d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)\n\n\ndef _max_pool1d(\n    input: Tensor, kernel_size: BroadcastingList1[int],\n    stride: Optional[BroadcastingList1[int]] = None,\n    padding: BroadcastingList1[int] = 0,\n    dilation: BroadcastingList1[int] = 1,\n    ceil_mode: bool = False,\n    return_indices: bool = False\n) -> Tensor:\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            max_pool1d,\n            (input,),\n            input,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            ceil_mode=ceil_mode,\n            return_indices=return_indices,\n        )\n    if stride is None:\n        stride = torch.jit.annotate(List[int], [])\n    return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n\n\nmax_pool1d = boolean_dispatch(\n    arg_name=\"return_indices\",\n    arg_index=6,\n    default=False,\n    if_true=max_pool1d_with_indices,\n    if_false=_max_pool1d,\n    module_name=__name__,\n    func_name=\"max_pool1d\",\n)\n\n\ndef max_pool2d_with_indices(\n    input: Tensor, kernel_size: BroadcastingList2[int],\n    stride: Optional[BroadcastingList2[int]] = None,\n    padding: BroadcastingList2[int] = 0,\n    dilation: BroadcastingList2[int] = 1,\n    ceil_mode: bool = False,\n    return_indices: bool = False\n) -> Tuple[Tensor, Tensor]:  # noqa: D400\n    r\"\"\"\n    max_pool2d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)\n\n    Applies a 2D max pooling over an input signal composed of several input\n    planes.\n\n    .. note::\n        The order of :attr:`ceil_mode` and :attr:`return_indices` is different from\n        what seen in :class:`~torch.nn.MaxPool2d`, and will change in a future release.\n\n    See :class:`~torch.nn.MaxPool2d` for details.\n\n    Args:\n        input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`, minibatch dim optional.\n        kernel_size: size of the pooling region. Can be a single number or a\n            tuple `(kH, kW)`\n        stride: stride of the pooling operation. Can be a single number or a\n            tuple `(sH, sW)`. Default: :attr:`kernel_size`\n        padding: Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.\n        dilation: The stride between elements within a sliding window, must be > 0.\n        ceil_mode: If ``True``, will use `ceil` instead of `floor` to compute the output shape. This\n                   ensures that every element in the input tensor is covered by a sliding window.\n        return_indices: If ``True``, will return the argmax along with the max values.\n                        Useful for :class:`torch.nn.functional.max_unpool2d` later\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            max_pool2d_with_indices,\n            (input,),\n            input,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            ceil_mode=ceil_mode,\n            return_indices=return_indices,\n        )\n    if stride is None:\n        stride = torch.jit.annotate(List[int], [])\n    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)\n\n\ndef _max_pool2d(\n    input: Tensor, kernel_size: BroadcastingList2[int],\n    stride: Optional[BroadcastingList2[int]] = None,\n    padding: BroadcastingList2[int] = 0,\n    dilation: BroadcastingList2[int] = 1,\n    ceil_mode: bool = False,\n    return_indices: bool = False\n) -> Tensor:\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            max_pool2d,\n            (input,),\n            input,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            ceil_mode=ceil_mode,\n            return_indices=return_indices,\n        )\n    if stride is None:\n        stride = torch.jit.annotate(List[int], [])\n    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n\n\nmax_pool2d = boolean_dispatch(\n    arg_name=\"return_indices\",\n    arg_index=6,\n    default=False,\n    if_true=max_pool2d_with_indices,\n    if_false=_max_pool2d,\n    module_name=__name__,\n    func_name=\"max_pool2d\",\n)\n\n\ndef max_pool3d_with_indices(\n    input: Tensor, kernel_size: BroadcastingList3[int],\n    stride: Optional[BroadcastingList3[int]] = None,\n    padding: BroadcastingList3[int] = 0,\n    dilation: BroadcastingList3[int] = 1,\n    ceil_mode: bool = False,\n    return_indices: bool = False\n) -> Tuple[Tensor, Tensor]:  # noqa: D400\n    r\"\"\"\n    max_pool3d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)\n\n    Applies a 3D max pooling over an input signal composed of several input\n    planes.\n\n    .. note::\n        The order of :attr:`ceil_mode` and :attr:`return_indices` is different from\n        what seen in :class:`~torch.nn.MaxPool3d`, and will change in a future release.\n\n    See :class:`~torch.nn.MaxPool3d` for details.\n\n    Args:\n        input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iD, iH , iW)`, minibatch dim optional.\n        kernel_size: size of the pooling region. Can be a single number or a\n                     tuple `(kT, kH, kW)`\n        stride: stride of the pooling operation. Can be a single number or a\n                tuple `(sT, sH, sW)`. Default: :attr:`kernel_size`\n        padding: Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.\n        dilation: The stride between elements within a sliding window, must be > 0.\n        ceil_mode: If ``True``, will use `ceil` instead of `floor` to compute the output shape. This\n                   ensures that every element in the input tensor is covered by a sliding window.\n        return_indices: If ``True``, will return the argmax along with the max values.\n                        Useful for :class:`torch.nn.functional.max_unpool3d` later\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            max_pool3d_with_indices,\n            (input,),\n            input,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            ceil_mode=ceil_mode,\n            return_indices=return_indices,\n        )\n    if stride is None:\n        stride = torch.jit.annotate(List[int], [])\n    return torch._C._nn.max_pool3d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)\n\n\ndef _max_pool3d(\n    input: Tensor, kernel_size: BroadcastingList3[int],\n    stride: Optional[BroadcastingList3[int]] = None,\n    padding: BroadcastingList3[int] = 0,\n    dilation: BroadcastingList3[int] = 1,\n    ceil_mode: bool = False,\n    return_indices: bool = False\n) -> Tensor:\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            max_pool3d,\n            (input,),\n            input,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            ceil_mode=ceil_mode,\n            return_indices=return_indices,\n        )\n    if stride is None:\n        stride = torch.jit.annotate(List[int], [])\n    return torch.max_pool3d(input, kernel_size, stride, padding, dilation, ceil_mode)\n\n\nmax_pool3d = boolean_dispatch(\n    arg_name=\"return_indices\",\n    arg_index=6,\n    default=False,\n    if_true=max_pool3d_with_indices,\n    if_false=_max_pool3d,\n    module_name=__name__,\n    func_name=\"max_pool3d\",\n)\n\n\ndef _unpool_output_size(\n    input: Tensor, kernel_size: List[int], stride: List[int], padding: List[int], output_size: Optional[List[int]]\n) -> List[int]:\n    input_size = input.size()\n    default_size = torch.jit.annotate(List[int], [])\n    for d in range(len(kernel_size)):\n        default_size.append((input_size[-len(kernel_size) + d] - 1) * stride[d] + kernel_size[d] - 2 * padding[d])\n    if output_size is None:\n        ret = default_size\n    else:\n        if len(output_size) == len(kernel_size) + 2:\n            output_size = output_size[2:]\n        if len(output_size) != len(kernel_size):\n            raise ValueError(\n                \"output_size should be a sequence containing \"\n                f\"{len(kernel_size)} or {len(kernel_size) + 2} elements, but it has a length of '{len(output_size)}'\"\n            )\n        for d in range(len(kernel_size)):\n            min_size = default_size[d] - stride[d]\n            max_size = default_size[d] + stride[d]\n            if not (min_size < output_size[d] < max_size):\n                raise ValueError(\n                    f'invalid output_size \"{output_size}\" (dim {d} must be between {min_size} and {max_size})'\n                )\n\n        ret = output_size\n    return ret\n\n\ndef max_unpool1d(\n    input: Tensor, indices: Tensor,\n    kernel_size: BroadcastingList1[int],\n    stride: Optional[BroadcastingList1[int]] = None,\n    padding: BroadcastingList1[int] = 0,\n    output_size: Optional[BroadcastingList1[int]] = None\n) -> Tensor:\n    r\"\"\"Compute a partial inverse of :class:`MaxPool1d`.\n\n    See :class:`~torch.nn.MaxUnpool1d` for details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            max_unpool1d,\n            (input,),\n            input,\n            indices,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            output_size=output_size,\n        )\n    kernel_size = _single(kernel_size)\n    if stride is not None:\n        _stride = _single(stride)\n    else:\n        _stride = kernel_size\n    padding = _single(padding)\n    output_size = _unpool_output_size(input, kernel_size, _stride, padding, output_size)\n    if isinstance(output_size, list):\n        output_size = output_size + [1]\n    else:\n        output_size = output_size + (1,)\n    return torch._C._nn.max_unpool2d(input.unsqueeze(-1), indices.unsqueeze(-1), output_size).squeeze(-1)\n\n\ndef max_unpool2d(\n    input: Tensor, indices: Tensor,\n    kernel_size: BroadcastingList2[int],\n    stride: Optional[BroadcastingList2[int]] = None,\n    padding: BroadcastingList2[int] = 0,\n    output_size: Optional[BroadcastingList2[int]] = None\n) -> Tensor:\n    r\"\"\"Compute a partial inverse of :class:`MaxPool2d`.\n\n    See :class:`~torch.nn.MaxUnpool2d` for details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            max_unpool2d,\n            (input,),\n            input,\n            indices,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            output_size=output_size,\n        )\n    kernel_size = _pair(kernel_size)\n    if stride is not None:\n        _stride = _pair(stride)\n    else:\n        _stride = kernel_size\n    padding = _pair(padding)\n    output_size = _unpool_output_size(input, kernel_size, _stride, padding, output_size)\n    return torch._C._nn.max_unpool2d(input, indices, output_size)\n\n\ndef max_unpool3d(\n    input: Tensor, indices: Tensor,\n    kernel_size: BroadcastingList3[int],\n    stride: Optional[BroadcastingList3[int]] = None,\n    padding: BroadcastingList3[int] = 0,\n    output_size: Optional[BroadcastingList3[int]] = None\n) -> Tensor:\n    r\"\"\"Compute a partial inverse of :class:`MaxPool3d`.\n\n    See :class:`~torch.nn.MaxUnpool3d` for details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            max_unpool3d,\n            (input,),\n            input,\n            indices,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            output_size=output_size,\n        )\n    kernel_size = _triple(kernel_size)\n    if stride is not None:\n        _stride = _triple(stride)\n    else:\n        _stride = kernel_size\n    padding = _triple(padding)\n    output_size = _unpool_output_size(input, kernel_size, _stride, padding, output_size)\n    return torch._C._nn.max_unpool3d(input, indices, output_size, _stride, padding)\n\n\ndef lp_pool2d(\n    input: Tensor, norm_type: Union[int, float],\n    kernel_size: BroadcastingList2[int],\n    stride: Optional[BroadcastingList2[int]] = None,\n    ceil_mode: bool = False\n) -> Tensor:\n    r\"\"\"\n    Apply a 2D power-average pooling over an input signal composed of several input planes.\n\n    If the sum of all inputs to the power of `p` is\n    zero, the gradient is set to zero as well.\n\n    See :class:`~torch.nn.LPPool2d` for details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            lp_pool2d, (input,), input, norm_type, kernel_size, stride=stride, ceil_mode=ceil_mode\n        )\n    kw, kh = utils._pair(kernel_size)\n    if stride is not None:\n        out = avg_pool2d(input.pow(norm_type), kernel_size, stride, 0, ceil_mode)\n    else:\n        out = avg_pool2d(input.pow(norm_type), kernel_size, padding=0, ceil_mode=ceil_mode)\n\n    return (torch.sign(out) * relu(torch.abs(out))).mul(kw * kh).pow(1.0 / norm_type)\n\n\ndef lp_pool1d(\n    input: Tensor, norm_type: Union[int, float],\n    kernel_size: int,\n    stride: Optional[BroadcastingList1[int]] = None,\n    ceil_mode: bool = False\n) -> Tensor:\n    r\"\"\"Apply a 1D power-average pooling over an input signal composed of several input planes.\n\n    If the sum of all inputs to the power of `p` is\n    zero, the gradient is set to zero as well.\n\n    See :class:`~torch.nn.LPPool1d` for details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            lp_pool1d, (input,), input, norm_type, kernel_size, stride=stride, ceil_mode=ceil_mode\n        )\n    if stride is not None:\n        out = avg_pool1d(input.pow(norm_type), kernel_size, stride, 0, ceil_mode)\n    else:\n        out = avg_pool1d(input.pow(norm_type), kernel_size, padding=0, ceil_mode=ceil_mode)\n\n    return (torch.sign(out) * relu(torch.abs(out))).mul(kernel_size).pow(1.0 / norm_type)\n\n\ndef adaptive_max_pool1d_with_indices(\n    input: Tensor, output_size: BroadcastingList1[int], return_indices: bool = False\n) -> Tuple[Tensor, Tensor]:  # noqa: D400\n    r\"\"\"\n    adaptive_max_pool1d(input, output_size, return_indices=False)\n\n    Applies a 1D adaptive max pooling over an input signal composed of\n    several input planes.\n\n    See :class:`~torch.nn.AdaptiveMaxPool1d` for details and output shape.\n\n    Args:\n        output_size: the target output size (single integer)\n        return_indices: whether to return pooling indices. Default: ``False``\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            adaptive_max_pool1d_with_indices, (input,), input, output_size, return_indices=return_indices\n        )\n    return torch.adaptive_max_pool1d(input, output_size)\n\n\ndef _adaptive_max_pool1d(input: Tensor, output_size: BroadcastingList1[int], return_indices: bool = False) -> Tensor:\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            adaptive_max_pool1d, (input,), input, output_size, return_indices=return_indices\n        )\n    return adaptive_max_pool1d_with_indices(input, output_size)[0]\n\n\nadaptive_max_pool1d = boolean_dispatch(\n    arg_name=\"return_indices\",\n    arg_index=2,\n    default=False,\n    if_true=adaptive_max_pool1d_with_indices,\n    if_false=_adaptive_max_pool1d,\n    module_name=__name__,\n    func_name=\"adaptive_max_pool1d\",\n)\n\n\ndef adaptive_max_pool2d_with_indices(\n    input: Tensor, output_size: BroadcastingList2[int],\n    return_indices: bool = False\n) -> Tuple[Tensor, Tensor]:  # noqa: D400\n    r\"\"\"adaptive_max_pool2d(input, output_size, return_indices=False)\n\n    Applies a 2D adaptive max pooling over an input signal composed of\n    several input planes.\n\n    See :class:`~torch.nn.AdaptiveMaxPool2d` for details and output shape.\n\n    Args:\n        output_size: the target output size (single integer or\n            double-integer tuple)\n        return_indices: whether to return pooling indices. Default: ``False``\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            adaptive_max_pool2d_with_indices, (input,), input, output_size, return_indices=return_indices\n        )\n    output_size = _list_with_default(output_size, input.size())\n    return torch._C._nn.adaptive_max_pool2d(input, output_size)\n\n\ndef _adaptive_max_pool2d(input: Tensor, output_size: BroadcastingList2[int], return_indices: bool = False) -> Tensor:\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            adaptive_max_pool2d, (input,), input, output_size, return_indices=return_indices\n        )\n    return adaptive_max_pool2d_with_indices(input, output_size)[0]\n\n\nadaptive_max_pool2d = boolean_dispatch(\n    arg_name=\"return_indices\",\n    arg_index=2,\n    default=False,\n    if_true=adaptive_max_pool2d_with_indices,\n    if_false=_adaptive_max_pool2d,\n    module_name=__name__,\n    func_name=\"adaptive_max_pool2d\",\n)\n\n\ndef adaptive_max_pool3d_with_indices(\n    input: Tensor, output_size: BroadcastingList3[int],\n    return_indices: bool = False\n) -> Tuple[Tensor, Tensor]:  # noqa: D400\n    r\"\"\"\n    adaptive_max_pool3d(input, output_size, return_indices=False)\n\n    Applies a 3D adaptive max pooling over an input signal composed of\n    several input planes.\n\n    See :class:`~torch.nn.AdaptiveMaxPool3d` for details and output shape.\n\n    Args:\n        output_size: the target output size (single integer or\n            triple-integer tuple)\n        return_indices: whether to return pooling indices. Default: ``False``\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            adaptive_max_pool3d_with_indices, (input,), input, output_size, return_indices=return_indices\n        )\n    output_size = _list_with_default(output_size, input.size())\n    return torch._C._nn.adaptive_max_pool3d(input, output_size)\n\n\ndef _adaptive_max_pool3d(input: Tensor, output_size: BroadcastingList3[int], return_indices: bool = False) -> Tensor:\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            adaptive_max_pool3d, (input,), input, output_size, return_indices=return_indices\n        )\n    return adaptive_max_pool3d_with_indices(input, output_size)[0]\n\n\nadaptive_max_pool3d = boolean_dispatch(\n    arg_name=\"return_indices\",\n    arg_index=2,\n    default=False,\n    if_true=adaptive_max_pool3d_with_indices,\n    if_false=_adaptive_max_pool3d,\n    module_name=__name__,\n    func_name=\"adaptive_max_pool3d\",\n)\n\n\nadaptive_avg_pool1d = _add_docstr(\n    torch.adaptive_avg_pool1d,\n    r\"\"\"\nadaptive_avg_pool1d(input, output_size) -> Tensor\n\nApplies a 1D adaptive average pooling over an input signal composed of\nseveral input planes.\n\nSee :class:`~torch.nn.AdaptiveAvgPool1d` for details and output shape.\n\nArgs:\n    output_size: the target output size (single integer)\n\"\"\",\n)\n\n\ndef adaptive_avg_pool2d(input: Tensor, output_size: BroadcastingList2[int]) -> Tensor:\n    r\"\"\"Apply a 2D adaptive average pooling over an input signal composed of several input planes.\n\n    See :class:`~torch.nn.AdaptiveAvgPool2d` for details and output shape.\n\n    Args:\n        output_size: the target output size (single integer or\n            double-integer tuple)\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(adaptive_avg_pool2d, (input,), input, output_size)\n    _output_size = _list_with_default(output_size, input.size())\n    return torch._C._nn.adaptive_avg_pool2d(input, _output_size)\n\n\ndef adaptive_avg_pool3d(input: Tensor, output_size: BroadcastingList3[int]) -> Tensor:\n    r\"\"\"Apply a 3D adaptive average pooling over an input signal composed of several input planes.\n\n    See :class:`~torch.nn.AdaptiveAvgPool3d` for details and output shape.\n\n    Args:\n        output_size: the target output size (single integer or\n            triple-integer tuple)\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(adaptive_avg_pool3d, (input,), input, output_size)\n    _output_size = _list_with_default(output_size, input.size())\n    return torch._C._nn.adaptive_avg_pool3d(input, _output_size)\n\n\n# Activation functions\ndef dropout(input: Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> Tensor:\n    r\"\"\"During training, randomly zeroes some elements of the input tensor with probability :attr:`p`.\n\n    Uses samples from a Bernoulli distribution.\n\n    See :class:`~torch.nn.Dropout` for details.\n\n    Args:\n        p: probability of an element to be zeroed. Default: 0.5\n        training: apply dropout if is ``True``. Default: ``True``\n        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(dropout, (input,), input, p=p, training=training, inplace=inplace)\n    if p < 0.0 or p > 1.0:\n        raise ValueError(f\"dropout probability has to be between 0 and 1, but got {p}\")\n    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n\n\ndef alpha_dropout(input: Tensor, p: float = 0.5, training: bool = False, inplace: bool = False) -> Tensor:\n    r\"\"\"Apply alpha dropout to the input.\n\n    See :class:`~torch.nn.AlphaDropout` for details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(alpha_dropout, (input,), input, p=p, training=training, inplace=inplace)\n    if p < 0.0 or p > 1.0:\n        raise ValueError(f\"dropout probability has to be between 0 and 1, but got {p}\")\n    return _VF.alpha_dropout_(input, p, training) if inplace else _VF.alpha_dropout(input, p, training)\n\n\ndef dropout1d(input: Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> Tensor:\n    r\"\"\"Randomly zero out entire channels (a channel is a 1D feature map).\n\n    For example, the :math:`j`-th channel of the :math:`i`-th sample in the\n    batched input is a 1D tensor :math:`\\text{input}[i, j]` of the input tensor.\n    Each channel will be zeroed out independently on every forward call with\n    probability :attr:`p` using samples from a Bernoulli distribution.\n\n    See :class:`~torch.nn.Dropout1d` for details.\n\n    Args:\n        p: probability of a channel to be zeroed. Default: 0.5\n        training: apply dropout if is ``True``. Default: ``True``\n        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(dropout1d, (input,), input, p=p, training=training, inplace=inplace)\n    if p < 0.0 or p > 1.0:\n        raise ValueError(f\"dropout probability has to be between 0 and 1, but got {p}\")\n    inp_dim = input.dim()\n    if inp_dim not in (2, 3):\n        raise RuntimeError(f\"dropout1d: Expected 2D or 3D input, but received a {inp_dim}D input. \"\n                           \"Note that dropout1d exists to provide channel-wise dropout on inputs with 1 \"\n                           \"spatial dimension, a channel dimension, and an optional batch dimension \"\n                           \"(i.e. 2D or 3D inputs).\")\n\n    is_batched = inp_dim == 3\n    if not is_batched:\n        input = input.unsqueeze_(0) if inplace else input.unsqueeze(0)\n\n    result = _VF.feature_dropout_(input, p, training) if inplace else _VF.feature_dropout(input, p, training)\n\n    if not is_batched:\n        result = result.squeeze_(0) if inplace else result.squeeze(0)\n\n    return result\n\n\ndef dropout2d(input: Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> Tensor:\n    r\"\"\"Randomly zero out entire channels (a channel is a 2D feature map).\n\n    For example, the :math:`j`-th channel of the :math:`i`-th sample in the\n    batched input is a 2D tensor :math:`\\text{input}[i, j]` of the input tensor.\n    Each channel will be zeroed out independently on every forward call with\n    probability :attr:`p` using samples from a Bernoulli distribution.\n\n    See :class:`~torch.nn.Dropout2d` for details.\n\n    Args:\n        p: probability of a channel to be zeroed. Default: 0.5\n        training: apply dropout if is ``True``. Default: ``True``\n        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(dropout2d, (input,), input, p=p, training=training, inplace=inplace)\n    if p < 0.0 or p > 1.0:\n        raise ValueError(f\"dropout probability has to be between 0 and 1, but got {p}\")\n    inp_dim = input.dim()\n    if inp_dim not in (3, 4):\n        warn_msg = (f\"dropout2d: Received a {inp_dim}-D input to dropout2d, which is deprecated \"\n                    \"and will result in an error in a future release. To retain the behavior \"\n                    \"and silence this warning, please use dropout instead. Note that dropout2d \"\n                    \"exists to provide channel-wise dropout on inputs with 2 spatial dimensions, \"\n                    \"a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\")\n        warnings.warn(warn_msg)\n\n    # TODO: Properly support no-batch-dim inputs. For now, these are NOT supported; passing\n    # a 3D input will perform dropout1d behavior instead. This was done historically and the\n    # behavior is maintained here for now.\n    # See https://github.com/pytorch/pytorch/issues/77081\n    if inp_dim == 3:\n        warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n                      \"1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C \"\n                      \"is the channel dim. This behavior will change in a future release to interpret the \"\n                      \"input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D \"\n                      \"channel-wise dropout behavior, please switch to using dropout1d instead.\")\n\n    result = _VF.feature_dropout_(input, p, training) if inplace else _VF.feature_dropout(input, p, training)\n\n    return result\n\n\ndef dropout3d(input: Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> Tensor:\n    r\"\"\"Randomly zero out entire channels (a channel is a 3D feature map).\n\n    For example, the :math:`j`-th channel of the :math:`i`-th sample in the\n    batched input is a 3D tensor :math:`\\text{input}[i, j]` of the input tensor.\n    Each channel will be zeroed out independently on every forward call with\n    probability :attr:`p` using samples from a Bernoulli distribution.\n\n    See :class:`~torch.nn.Dropout3d` for details.\n\n    Args:\n        p: probability of a channel to be zeroed. Default: 0.5\n        training: apply dropout if is ``True``. Default: ``True``\n        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(dropout3d, (input,), input, p=p, training=training, inplace=inplace)\n    if p < 0.0 or p > 1.0:\n        raise ValueError(f\"dropout probability has to be between 0 and 1, but got {p}\")\n    inp_dim = input.dim()\n    if inp_dim not in (4, 5):\n        warn_msg = (f\"dropout3d: Received a {inp_dim}-D input to dropout3d, which is deprecated \"\n                    \"and will result in an error in a future release. To retain the behavior \"\n                    \"and silence this warning, please use dropout instead. Note that dropout3d \"\n                    \"exists to provide channel-wise dropout on inputs with 3 spatial dimensions, \"\n                    \"a channel dimension, and an optional batch dimension (i.e. 4D or 5D inputs).\")\n        warnings.warn(warn_msg)\n\n    is_batched = inp_dim == 5\n    if not is_batched:\n        input = input.unsqueeze_(0) if inplace else input.unsqueeze(0)\n\n    result = _VF.feature_dropout_(input, p, training) if inplace else _VF.feature_dropout(input, p, training)\n\n    if not is_batched:\n        result = result.squeeze_(0) if inplace else result.squeeze(0)\n    return result\n\n\ndef feature_alpha_dropout(input: Tensor, p: float = 0.5, training: bool = False, inplace: bool = False) -> Tensor:\n    r\"\"\"Randomly masks out entire channels (a channel is a feature map).\n\n    For example, the :math:`j`-th channel of the :math:`i`-th sample in the batch input\n    is a tensor :math:`\\text{input}[i, j]` of the input tensor. Instead of\n    setting activations to zero, as in regular Dropout, the activations are set\n    to the negative saturation value of the SELU activation function.\n\n    Each element will be masked independently on every forward call with\n    probability :attr:`p` using samples from a Bernoulli distribution.\n    The elements to be masked are randomized on every forward call, and scaled\n    and shifted to maintain zero mean and unit variance.\n\n    See :class:`~torch.nn.FeatureAlphaDropout` for details.\n\n    Args:\n        p: dropout probability of a channel to be zeroed. Default: 0.5\n        training: apply dropout if is ``True``. Default: ``True``\n        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            feature_alpha_dropout, (input,), input, p=p, training=training, inplace=inplace\n        )\n    if p < 0.0 or p > 1.0:\n        raise ValueError(f\"dropout probability has to be between 0 and 1, but got {p}\")\n    return _VF.feature_alpha_dropout_(input, p, training) if inplace else _VF.feature_alpha_dropout(input, p, training)\n\n\ndef _threshold(input: Tensor, threshold: float, value: float, inplace: bool = False) -> Tensor:\n    r\"\"\"Apply a threshold to each element of the input Tensor.\n\n    See :class:`~torch.nn.Threshold` for more details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(_threshold, (input,), input, threshold, value, inplace=inplace)\n    if inplace:\n        result = _VF.threshold_(input, threshold, value)\n    else:\n        result = _VF.threshold(input, threshold, value)\n    return result\n\n\n# We define this function as _threshold because it takes an argument\n# named threshold, which clobbers the recursive reference to the\n# function needed for __torch_function__ support\nthreshold = _threshold\n\nthreshold_ = _add_docstr(\n    _VF.threshold_,\n    r\"\"\"\nthreshold_(input, threshold, value) -> Tensor\n\nIn-place version of :func:`~threshold`.\n\"\"\",\n)\n\n\ndef relu(input: Tensor, inplace: bool = False) -> Tensor:  # noqa: D400,D402\n    r\"\"\"relu(input, inplace=False) -> Tensor\n\n    Applies the rectified linear unit function element-wise. See\n    :class:`~torch.nn.ReLU` for more details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(relu, (input,), input, inplace=inplace)\n    if inplace:\n        result = torch.relu_(input)\n    else:\n        result = torch.relu(input)\n    return result\n\n\nrelu_ = _add_docstr(\n    torch.relu_,\n    r\"\"\"\nrelu_(input) -> Tensor\n\nIn-place version of :func:`~relu`.\n\"\"\",\n)\n\n\ndef glu(input: Tensor, dim: int = -1) -> Tensor:  # noqa: D400,D402\n    r\"\"\"\n    glu(input, dim=-1) -> Tensor\n\n    The gated linear unit. Computes:\n\n    .. math ::\n        \\text{GLU}(a, b) = a \\otimes \\sigma(b)\n\n    where `input` is split in half along `dim` to form `a` and `b`, :math:`\\sigma`\n    is the sigmoid function and :math:`\\otimes` is the element-wise product between matrices.\n\n    See `Language Modeling with Gated Convolutional Networks <https://arxiv.org/abs/1612.08083>`_.\n\n    Args:\n        input (Tensor): input tensor\n        dim (int): dimension on which to split the input. Default: -1\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(glu, (input,), input, dim=dim)\n    if input.dim() == 0:\n        raise RuntimeError(\"glu does not support scalars because halving size must be even\")\n    return torch._C._nn.glu(input, dim)\n\n\ndef hardtanh(input: Tensor, min_val: float = -1., max_val: float = 1., inplace: bool = False) -> Tensor:  # noqa: D400,D402\n    r\"\"\"\n    hardtanh(input, min_val=-1., max_val=1., inplace=False) -> Tensor\n\n    Applies the HardTanh function element-wise. See :class:`~torch.nn.Hardtanh` for more\n    details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(hardtanh, (input,), input, min_val=min_val, max_val=max_val, inplace=inplace)\n    if inplace:\n        result = torch._C._nn.hardtanh_(input, min_val, max_val)\n    else:\n        result = torch._C._nn.hardtanh(input, min_val, max_val)\n    return result\n\n\nhardtanh_ = _add_docstr(\n    torch._C._nn.hardtanh_,\n    r\"\"\"\nhardtanh_(input, min_val=-1., max_val=1.) -> Tensor\n\nIn-place version of :func:`~hardtanh`.\n\"\"\",\n)\n\n\ndef relu6(input: Tensor, inplace: bool = False) -> Tensor:  # noqa: D400,D402\n    r\"\"\"relu6(input, inplace=False) -> Tensor\n\n    Applies the element-wise function :math:`\\text{ReLU6}(x) = \\min(\\max(0,x), 6)`.\n\n    See :class:`~torch.nn.ReLU6` for more details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(relu6, (input,), input, inplace=inplace)\n    if inplace:\n        result = torch._C._nn.relu6_(input)\n    else:\n        result = torch._C._nn.relu6(input)\n    return result\n\n\ndef elu(input: Tensor, alpha: float = 1.0, inplace: bool = False) -> Tensor:\n    r\"\"\"Apply the Exponential Linear Unit (ELU) function element-wise.\n\n    See :class:`~torch.nn.ELU` for more details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(elu, (input,), input, alpha=alpha, inplace=inplace)\n    if inplace:\n        result = torch._C._nn.elu_(input, alpha)\n    else:\n        result = torch._C._nn.elu(input, alpha)\n    return result\n\n\nelu_ = _add_docstr(\n    torch._C._nn.elu_,\n    r\"\"\"\nelu_(input, alpha=1.) -> Tensor\n\nIn-place version of :func:`~elu`.\n\"\"\",\n)\n\n\ndef selu(input: Tensor, inplace: bool = False) -> Tensor:  # noqa: D400,D402\n    r\"\"\"selu(input, inplace=False) -> Tensor\n\n    Applies element-wise,\n    :math:`\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))`,\n    with :math:`\\alpha=1.6732632423543772848170429916717` and\n    :math:`scale=1.0507009873554804934193349852946`.\n\n    See :class:`~torch.nn.SELU` for more details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(selu, (input,), input, inplace=inplace)\n    if inplace:\n        result = torch.selu_(input)\n    else:\n        result = torch.selu(input)\n    return result\n\n\nselu_ = _add_docstr(\n    torch.selu_,\n    r\"\"\"\nselu_(input) -> Tensor\n\nIn-place version of :func:`~selu`.\n\"\"\",\n)\n\n\ndef celu(input: Tensor, alpha: float = 1.0, inplace: bool = False) -> Tensor:  # noqa: D400,D402\n    r\"\"\"celu(input, alpha=1., inplace=False) -> Tensor\n\n    Applies element-wise,\n    :math:`\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))`.\n\n    See :class:`~torch.nn.CELU` for more details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(celu, (input,), input, alpha=alpha, inplace=inplace)\n    if inplace:\n        result = torch.celu_(input, alpha)\n    else:\n        result = torch.celu(input, alpha)\n    return result\n\n\ncelu_ = _add_docstr(\n    torch.celu_,\n    r\"\"\"\ncelu_(input, alpha=1.) -> Tensor\n\nIn-place version of :func:`~celu`.\n\"\"\",\n)\n\n\ndef leaky_relu(input: Tensor, negative_slope: float = 0.01, inplace: bool = False) -> Tensor:  # noqa: D400,D402\n    r\"\"\"\n    leaky_relu(input, negative_slope=0.01, inplace=False) -> Tensor\n\n    Applies element-wise,\n    :math:`\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)`\n\n    See :class:`~torch.nn.LeakyReLU` for more details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(leaky_relu, (input,), input, negative_slope=negative_slope, inplace=inplace)\n    if inplace:\n        result = torch._C._nn.leaky_relu_(input, negative_slope)\n    else:\n        result = torch._C._nn.leaky_relu(input, negative_slope)\n    return result\n\n\nleaky_relu_ = _add_docstr(\n    torch._C._nn.leaky_relu_,\n    r\"\"\"\nleaky_relu_(input, negative_slope=0.01) -> Tensor\n\nIn-place version of :func:`~leaky_relu`.\n\"\"\",\n)\n\n\nprelu = _add_docstr(\n    torch.prelu,\n    r\"\"\"prelu(input, weight) -> Tensor\n\nApplies element-wise the function\n:math:`\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)` where weight is a\nlearnable parameter.\n\n.. note::\n    `weight` is expected to be a scalar or 1-D tensor. If `weight` is 1-D,\n    its size must match the number of input channels, determined by\n    `input.size(1)` when `input.dim() >= 2`, otherwise 1.\n    In the 1-D case, note that when `input` has dim > 2, `weight` can be expanded\n    to the shape of `input` in a way that is not possible using normal\n    :ref:`broadcasting semantics<broadcasting-semantics>`.\n\nSee :class:`~torch.nn.PReLU` for more details.\n\"\"\")\n\n\ndef rrelu(\n    input: Tensor, lower: float = 1.0 / 8, upper: float = 1.0 / 3, training: bool = False, inplace: bool = False\n) -> Tensor:  # noqa: D400,D402\n    r\"\"\"rrelu(input, lower=1./8, upper=1./3, training=False, inplace=False) -> Tensor\n\n    Randomized leaky ReLU.\n\n    See :class:`~torch.nn.RReLU` for more details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            rrelu, (input,), input, lower=lower, upper=upper, training=training, inplace=inplace\n        )\n    if inplace:\n        result = torch.rrelu_(input, lower, upper, training)\n    else:\n        result = torch.rrelu(input, lower, upper, training)\n    return result\n\n\nrrelu_ = _add_docstr(\n    torch.rrelu_,\n    r\"\"\"\nrrelu_(input, lower=1./8, upper=1./3, training=False) -> Tensor\n\nIn-place version of :func:`~rrelu`.\n\"\"\",\n)\n\nlogsigmoid = _add_docstr(\n    torch._C._nn.log_sigmoid,\n    r\"\"\"\nlogsigmoid(input) -> Tensor\n\nApplies element-wise :math:`\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)`\n\nSee :class:`~torch.nn.LogSigmoid` for more details.\n\"\"\",\n)\n\ngelu = _add_docstr(\n    torch._C._nn.gelu,\n    r\"\"\"\ngelu(input, approximate = 'none') -> Tensor\n\nWhen the approximate argument is 'none', it applies element-wise the function\n:math:`\\text{GELU}(x) = x * \\Phi(x)`\n\nwhere :math:`\\Phi(x)` is the Cumulative Distribution Function for Gaussian Distribution.\n\nWhen the approximate argument is 'tanh', Gelu is estimated with\n\n.. math::\n    \\text{GELU}(x) = 0.5 * x * (1 + \\text{Tanh}(\\sqrt{2 / \\pi} * (x + 0.044715 * x^3)))\n\nSee `Gaussian Error Linear Units (GELUs) <https://arxiv.org/abs/1606.08415>`_.\n\"\"\")\n\nhardshrink = _add_docstr(\n    torch.hardshrink,\n    r\"\"\"\nhardshrink(input, lambd=0.5) -> Tensor\n\nApplies the hard shrinkage function element-wise\n\nSee :class:`~torch.nn.Hardshrink` for more details.\n\"\"\")\n\n\ndef tanhshrink(input):  # noqa: D400,D402\n    r\"\"\"tanhshrink(input) -> Tensor\n\n    Applies element-wise, :math:`\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)`\n\n    See :class:`~torch.nn.Tanhshrink` for more details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(tanhshrink, (input,), input)\n    return input - input.tanh()\n\n\ndef softsign(input):  # noqa: D400,D402\n    r\"\"\"softsign(input) -> Tensor\n\n    Applies element-wise, the function :math:`\\text{SoftSign}(x) = \\frac{x}{1 + |x|}`\n\n    See :class:`~torch.nn.Softsign` for more details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(softsign, (input,), input)\n    return input / (input.abs() + 1)\n\n\nsoftplus = _add_docstr(\n    torch._C._nn.softplus,\n    r\"\"\"\nsoftplus(input, beta=1, threshold=20) -> Tensor\n\nApplies element-wise, the function :math:`\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))`.\n\nFor numerical stability the implementation reverts to the linear function\nwhen :math:`input \\times \\beta > threshold`.\n\nSee :class:`~torch.nn.Softplus` for more details.\n\"\"\",\n)\n\n\ndef _get_softmax_dim(name: str, ndim: int, stacklevel: int) -> int:\n    warnings.warn(\n        f\"Implicit dimension choice for {name} has been deprecated. Change the call to include dim=X as an argument.\",\n        stacklevel=stacklevel,\n    )\n    if ndim == 0 or ndim == 1 or ndim == 3:\n        ret = 0\n    else:\n        ret = 1\n    return ret\n\n\ndef softmin(input: Tensor, dim: Optional[int] = None, _stacklevel: int = 3, dtype: Optional[DType] = None) -> Tensor:\n    r\"\"\"Apply a softmin function.\n\n    Note that :math:`\\text{Softmin}(x) = \\text{Softmax}(-x)`. See softmax definition for mathematical formula.\n\n    See :class:`~torch.nn.Softmin` for more details.\n\n    Args:\n        input (Tensor): input\n        dim (int): A dimension along which softmin will be computed (so every slice\n            along dim will sum to 1).\n        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n          If specified, the input tensor is casted to :attr:`dtype` before the operation\n          is performed. This is useful for preventing data type overflows. Default: None.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(softmin, (input,), input, dim=dim, _stacklevel=_stacklevel, dtype=dtype)\n    if dim is None:\n        dim = _get_softmax_dim(\"softmin\", input.dim(), _stacklevel)\n    if dtype is None:\n        ret = (-input).softmax(dim)\n    else:\n        ret = (-input).softmax(dim, dtype=dtype)\n    return ret\n\n\ndef softmax(input: Tensor, dim: Optional[int] = None, _stacklevel: int = 3, dtype: Optional[DType] = None) -> Tensor:\n    r\"\"\"Apply a softmax function.\n\n    Softmax is defined as:\n\n    :math:`\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}`\n\n    It is applied to all slices along dim, and will re-scale them so that the elements\n    lie in the range `[0, 1]` and sum to 1.\n\n    See :class:`~torch.nn.Softmax` for more details.\n\n    Args:\n        input (Tensor): input\n        dim (int): A dimension along which softmax will be computed.\n        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n          If specified, the input tensor is casted to :attr:`dtype` before the operation\n          is performed. This is useful for preventing data type overflows. Default: None.\n\n    .. note::\n        This function doesn't work directly with NLLLoss,\n        which expects the Log to be computed between the Softmax and itself.\n        Use log_softmax instead (it's faster and has better numerical properties).\n\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(softmax, (input,), input, dim=dim, _stacklevel=_stacklevel, dtype=dtype)\n    if dim is None:\n        dim = _get_softmax_dim(\"softmax\", input.dim(), _stacklevel)\n    if dtype is None:\n        ret = input.softmax(dim)\n    else:\n        ret = input.softmax(dim, dtype=dtype)\n    return ret\n\n\ndef gumbel_softmax(logits: Tensor, tau: float = 1, hard: bool = False, eps: float = 1e-10, dim: int = -1) -> Tensor:\n    r\"\"\"\n    Sample from the Gumbel-Softmax distribution (`Link 1`_  `Link 2`_) and optionally discretize.\n\n    Args:\n      logits: `[..., num_features]` unnormalized log probabilities\n      tau: non-negative scalar temperature\n      hard: if ``True``, the returned samples will be discretized as one-hot vectors,\n            but will be differentiated as if it is the soft sample in autograd\n      dim (int): A dimension along which softmax will be computed. Default: -1.\n\n    Returns:\n      Sampled tensor of same shape as `logits` from the Gumbel-Softmax distribution.\n      If ``hard=True``, the returned samples will be one-hot, otherwise they will\n      be probability distributions that sum to 1 across `dim`.\n\n    .. note::\n      This function is here for legacy reasons, may be removed from nn.Functional in the future.\n\n    .. note::\n      The main trick for `hard` is to do  `y_hard - y_soft.detach() + y_soft`\n\n      It achieves two things:\n      - makes the output value exactly one-hot\n      (since we add then subtract y_soft value)\n      - makes the gradient equal to y_soft gradient\n      (since we strip all other gradients)\n\n    Examples::\n        >>> logits = torch.randn(20, 32)\n        >>> # Sample soft categorical using reparametrization trick:\n        >>> F.gumbel_softmax(logits, tau=1, hard=False)\n        >>> # Sample hard categorical using \"Straight-through\" trick:\n        >>> F.gumbel_softmax(logits, tau=1, hard=True)\n\n    .. _Link 1:\n        https://arxiv.org/abs/1611.00712\n    .. _Link 2:\n        https://arxiv.org/abs/1611.01144\n    \"\"\"\n    if has_torch_function_unary(logits):\n        return handle_torch_function(gumbel_softmax, (logits,), logits, tau=tau, hard=hard, eps=eps, dim=dim)\n    if eps != 1e-10:\n        warnings.warn(\"`eps` parameter is deprecated and has no effect.\")\n\n    gumbels = (\n        -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()\n    )  # ~Gumbel(0,1)\n    gumbels = (logits + gumbels) / tau  # ~Gumbel(logits,tau)\n    y_soft = gumbels.softmax(dim)\n\n    if hard:\n        # Straight through.\n        index = y_soft.max(dim, keepdim=True)[1]\n        y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)\n        ret = y_hard - y_soft.detach() + y_soft\n    else:\n        # Reparametrization trick.\n        ret = y_soft\n    return ret\n\n\ndef log_softmax(input: Tensor, dim: Optional[int] = None, _stacklevel: int = 3, dtype: Optional[DType] = None) -> Tensor:\n    r\"\"\"Apply a softmax followed by a logarithm.\n\n    While mathematically equivalent to log(softmax(x)), doing these two\n    operations separately is slower and numerically unstable. This function\n    uses an alternative formulation to compute the output and gradient correctly.\n\n    See :class:`~torch.nn.LogSoftmax` for more details.\n\n    Args:\n        input (Tensor): input\n        dim (int): A dimension along which log_softmax will be computed.\n        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n          If specified, the input tensor is cast to :attr:`dtype` before the operation\n          is performed. This is useful for preventing data type overflows. Default: None.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(log_softmax, (input,), input, dim=dim, _stacklevel=_stacklevel, dtype=dtype)\n    if dim is None:\n        dim = _get_softmax_dim(\"log_softmax\", input.dim(), _stacklevel)\n    if dtype is None:\n        ret = input.log_softmax(dim)\n    else:\n        ret = input.log_softmax(dim, dtype=dtype)\n    return ret\n\n\nsoftshrink = _add_docstr(\n    torch._C._nn.softshrink,\n    r\"\"\"\nsoftshrink(input, lambd=0.5) -> Tensor\n\nApplies the soft shrinkage function elementwise\n\nSee :class:`~torch.nn.Softshrink` for more details.\n\"\"\",\n)\n\n\ndef tanh(input):  # noqa: D400,D402\n    r\"\"\"tanh(input) -> Tensor\n\n    Applies element-wise,\n    :math:`\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}`\n\n    See :class:`~torch.nn.Tanh` for more details.\n    \"\"\"\n    return input.tanh()\n\n\ndef sigmoid(input):  # noqa: D400,D402\n    r\"\"\"sigmoid(input) -> Tensor\n\n    Applies the element-wise function :math:`\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}`\n\n    See :class:`~torch.nn.Sigmoid` for more details.\n    \"\"\"\n    return input.sigmoid()\n\n\ndef hardsigmoid(input: Tensor, inplace: bool = False) -> Tensor:\n    r\"\"\"Apply the Hardsigmoid function element-wise.\n\n    .. math::\n        \\text{Hardsigmoid}(x) = \\begin{cases}\n            0 & \\text{if~} x \\le -3, \\\\\n            1 & \\text{if~} x \\ge +3, \\\\\n            x / 6 + 1 / 2 & \\text{otherwise}\n        \\end{cases}\n\n    Args:\n        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n\n    See :class:`~torch.nn.Hardsigmoid` for more details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(hardsigmoid, (input,), input, inplace=inplace)\n    if inplace:\n        return torch._C._nn.hardsigmoid_(input)\n    return torch._C._nn.hardsigmoid(input)\n\n\nlinear = _add_docstr(\n    torch._C._nn.linear,\n    r\"\"\"\nlinear(input, weight, bias=None) -> Tensor\n\nApplies a linear transformation to the incoming data: :math:`y = xA^T + b`.\n\nThis operation supports 2-D :attr:`weight` with :ref:`sparse layout<sparse-docs>`\n\n{sparse_beta_warning}\n\nThis operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n\nShape:\n\n    - Input: :math:`(*, in\\_features)` where `*` means any number of\n      additional dimensions, including none\n    - Weight: :math:`(out\\_features, in\\_features)` or :math:`(in\\_features)`\n    - Bias: :math:`(out\\_features)` or :math:`()`\n    - Output: :math:`(*, out\\_features)` or :math:`(*)`, based on the shape of the weight\n\"\"\".format(**sparse_support_notes))\n\n\nbilinear = _add_docstr(\n    torch.bilinear,\n    r\"\"\"\nbilinear(input1, input2, weight, bias=None) -> Tensor\n\nApplies a bilinear transformation to the incoming data:\n:math:`y = x_1^T A x_2 + b`\n\nShape:\n\n    - input1: :math:`(N, *, H_{in1})` where :math:`H_{in1}=\\text{in1\\_features}`\n      and :math:`*` means any number of additional dimensions.\n      All but the last dimension of the inputs should be the same.\n    - input2: :math:`(N, *, H_{in2})` where :math:`H_{in2}=\\text{in2\\_features}`\n    - weight: :math:`(\\text{out\\_features}, \\text{in1\\_features},\n      \\text{in2\\_features})`\n    - bias: :math:`(\\text{out\\_features})`\n    - output: :math:`(N, *, H_{out})` where :math:`H_{out}=\\text{out\\_features}`\n      and all but the last dimension are the same shape as the input.\n\"\"\")\n\n\ndef silu(input: Tensor, inplace: bool = False) -> Tensor:\n    r\"\"\"Apply the Sigmoid Linear Unit (SiLU) function, element-wise.\n\n    The SiLU function is also known as the swish function.\n\n    .. math::\n        \\text{silu}(x) = x * \\sigma(x), \\text{where } \\sigma(x) \\text{ is the logistic sigmoid.}\n\n    .. note::\n        See `Gaussian Error Linear Units (GELUs) <https://arxiv.org/abs/1606.08415>`_\n        where the SiLU (Sigmoid Linear Unit) was originally coined, and see\n        `Sigmoid-Weighted Linear Units for Neural Network Function Approximation\n        in Reinforcement Learning <https://arxiv.org/abs/1702.03118>`_ and `Swish:\n        a Self-Gated Activation Function <https://arxiv.org/abs/1710.05941v1>`_\n        where the SiLU was experimented with later.\n\n    See :class:`~torch.nn.SiLU` for more details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(silu, (input,), input, inplace=inplace)\n    if inplace:\n        return torch._C._nn.silu_(input)\n    return torch._C._nn.silu(input)\n\n\ndef mish(input: Tensor, inplace: bool = False) -> Tensor:\n    r\"\"\"Apply the Mish function, element-wise.\n\n    Mish: A Self Regularized Non-Monotonic Neural Activation Function.\n\n    .. math::\n        \\text{Mish}(x) = x * \\text{Tanh}(\\text{Softplus}(x))\n\n    .. note::\n        See `Mish: A Self Regularized Non-Monotonic Neural Activation Function <https://arxiv.org/abs/1908.08681>`_\n\n    See :class:`~torch.nn.Mish` for more details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(mish, (input,), input, inplace=inplace)\n    if inplace:\n        return torch._C._nn.mish_(input)\n    return torch._C._nn.mish(input)\n\n\ndef hardswish(input: Tensor, inplace: bool = False) -> Tensor:\n    r\"\"\"Apply hardswish function, element-wise.\n\n    Follows implementation as described in the paper:\n    `Searching for MobileNetV3`_.\n\n    .. math::\n        \\text{Hardswish}(x) = \\begin{cases}\n            0 & \\text{if~} x \\le -3, \\\\\n            x & \\text{if~} x \\ge +3, \\\\\n            x \\cdot (x + 3) /6 & \\text{otherwise}\n        \\end{cases}\n\n    See :class:`~torch.nn.Hardswish` for more details.\n\n    .. _`Searching for MobileNetV3`:\n        https://arxiv.org/abs/1905.02244\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(hardswish, (input,), input, inplace=inplace)\n    if inplace:\n        return torch._C._nn.hardswish_(input)\n    return torch._C._nn.hardswish(input)\n\n\ndef _no_grad_embedding_renorm_(weight: Tensor, input: Tensor, max_norm: float, norm_type: float) -> Tuple[Tensor, Tensor]:\n    torch.embedding_renorm_(weight.detach(), input, max_norm, norm_type)\n\n\ndef embedding(\n    input: Tensor,\n    weight: Tensor,\n    padding_idx: Optional[int] = None,\n    max_norm: Optional[float] = None,\n    norm_type: float = 2.0,\n    scale_grad_by_freq: bool = False,\n    sparse: bool = False,\n) -> Tensor:\n    r\"\"\"Generate a simple lookup table that looks up embeddings in a fixed dictionary and size.\n\n    This module is often used to retrieve word embeddings using indices.\n    The input to the module is a list of indices, and the embedding matrix,\n    and the output is the corresponding word embeddings.\n\n    See :class:`torch.nn.Embedding` for more details.\n\n    .. note::\n        Note that the analytical gradients of this function with respect to\n        entries in :attr:`weight` at the row specified by :attr:`padding_idx`\n        are expected to differ from the numerical ones.\n\n    .. note::\n        Note that `:class:`torch.nn.Embedding` differs from this function in\n        that it initializes the row of :attr:`weight` specified by\n        :attr:`padding_idx` to all zeros on construction.\n\n    Args:\n        input (LongTensor): Tensor containing indices into the embedding matrix\n        weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,\n            and number of columns equal to the embedding size\n        padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;\n                                     therefore, the embedding vector at :attr:`padding_idx` is not updated during training,\n                                     i.e. it remains as a fixed \"pad\".\n        max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`\n                                    is renormalized to have norm :attr:`max_norm`.\n                                    Note: this will modify :attr:`weight` in-place.\n        norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.\n        scale_grad_by_freq (bool, optional): If given, this will scale gradients by the inverse of frequency of\n                                                the words in the mini-batch. Default ``False``.\n        sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under\n                                 :class:`torch.nn.Embedding` for more details regarding sparse gradients.\n\n    Shape:\n        - Input: LongTensor of arbitrary shape containing the indices to extract\n        - Weight: Embedding matrix of floating point type with shape `(V, embedding_dim)`,\n          where V = maximum index + 1 and embedding_dim = the embedding size\n        - Output: `(*, embedding_dim)`, where `*` is the input shape\n\n    Examples::\n\n        >>> # a batch of 2 samples of 4 indices each\n        >>> input = torch.tensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n        >>> # an embedding matrix containing 10 tensors of size 3\n        >>> embedding_matrix = torch.rand(10, 3)\n        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n        >>> F.embedding(input, embedding_matrix)\n        tensor([[[ 0.8490,  0.9625,  0.6753],\n                 [ 0.9666,  0.7761,  0.6108],\n                 [ 0.6246,  0.9751,  0.3618],\n                 [ 0.4161,  0.2419,  0.7383]],\n\n                [[ 0.6246,  0.9751,  0.3618],\n                 [ 0.0237,  0.7794,  0.0528],\n                 [ 0.9666,  0.7761,  0.6108],\n                 [ 0.3385,  0.8612,  0.1867]]])\n\n        >>> # example with padding_idx\n        >>> weights = torch.rand(10, 3)\n        >>> weights[0, :].zero_()\n        >>> embedding_matrix = weights\n        >>> input = torch.tensor([[0, 2, 0, 5]])\n        >>> F.embedding(input, embedding_matrix, padding_idx=0)\n        tensor([[[ 0.0000,  0.0000,  0.0000],\n                 [ 0.5609,  0.5384,  0.8720],\n                 [ 0.0000,  0.0000,  0.0000],\n                 [ 0.6262,  0.2438,  0.7471]]])\n    \"\"\"\n    if has_torch_function_variadic(input, weight):\n        return handle_torch_function(\n            embedding,\n            (input, weight),\n            input,\n            weight,\n            padding_idx=padding_idx,\n            max_norm=max_norm,\n            norm_type=norm_type,\n            scale_grad_by_freq=scale_grad_by_freq,\n            sparse=sparse,\n        )\n    if padding_idx is not None:\n        if padding_idx > 0:\n            assert padding_idx < weight.size(0), \"Padding_idx must be within num_embeddings\"\n        elif padding_idx < 0:\n            assert padding_idx >= -weight.size(0), \"Padding_idx must be within num_embeddings\"\n            padding_idx = weight.size(0) + padding_idx\n    else:\n        padding_idx = -1\n    if max_norm is not None:\n        # Note [embedding_renorm contiguous]\n        # `embedding_renorm_` will call .contiguous() on input anyways, so we\n        # call it here and take advantage of the improved locality in the\n        # `embedding` call below too.\n        input = input.contiguous()\n        # Note [embedding_renorm set_grad_enabled]\n        # XXX: equivalent to\n        # with torch.no_grad():\n        #   torch.embedding_renorm_\n        # remove once script supports set_grad_enabled\n        _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n\n\ndef embedding_bag(\n    input: Tensor,\n    weight: Tensor,\n    offsets: Optional[Tensor] = None,\n    max_norm: Optional[float] = None,\n    norm_type: float = 2,\n    scale_grad_by_freq: bool = False,\n    mode: str = \"mean\",\n    sparse: bool = False,\n    per_sample_weights: Optional[Tensor] = None,\n    include_last_offset: bool = False,\n    padding_idx: Optional[int] = None,\n) -> Tensor:\n    r\"\"\"Compute sums, means or maxes of `bags` of embeddings.\n\n    Calculation is done without instantiating the intermediate embeddings.\n    See :class:`torch.nn.EmbeddingBag` for more details.\n\n    Note:\n        {backward_reproducibility_note}\n\n    Args:\n        input (LongTensor): Tensor containing bags of indices into the embedding matrix\n        weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,\n            and number of columns equal to the embedding size\n        offsets (LongTensor, optional): Only used when :attr:`input` is 1D. :attr:`offsets` determines\n                             the starting index position of each bag (sequence) in :attr:`input`.\n        max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`\n                                    is renormalized to have norm :attr:`max_norm`.\n                                    Note: this will modify :attr:`weight` in-place.\n        norm_type (float, optional): The ``p`` in the ``p``-norm to compute for the :attr:`max_norm` option.\n                                     Default ``2``.\n        scale_grad_by_freq (bool, optional): if given, this will scale gradients by the inverse of frequency of\n                                                the words in the mini-batch. Default ``False``.\n                                                Note: this option is not supported when ``mode=\"max\"``.\n        mode (str, optional): ``\"sum\"``, ``\"mean\"`` or ``\"max\"``. Specifies the way to reduce the bag.\n                                 Default: ``\"mean\"``\n        sparse (bool, optional): if ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under\n                                 :class:`torch.nn.Embedding` for more details regarding sparse gradients.\n                                 Note: this option is not supported when ``mode=\"max\"``.\n        per_sample_weights (Tensor, optional): a tensor of float / double weights, or None\n            to indicate all weights should be taken to be 1. If specified, :attr:`per_sample_weights`\n            must have exactly the same shape as input and is treated as having the same\n            :attr:`offsets`, if those are not None.\n\n        include_last_offset (bool, optional): if ``True``, the size of offsets is equal to the number of bags + 1.\n            The last element is the size of the input, or the ending index position of the last bag (sequence).\n\n        padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the\n                                     gradient; therefore, the embedding vector at :attr:`padding_idx` is not updated\n                                     during training, i.e. it remains as a fixed \"pad\". Note that the embedding\n                                     vector at :attr:`padding_idx` is excluded from the reduction.\n\n    Shape:\n        - :attr:`input` (LongTensor) and :attr:`offsets` (LongTensor, optional)\n\n          - If :attr:`input` is 2D of shape `(B, N)`, it will be treated as ``B`` bags (sequences)\n            each of fixed length ``N``, and this will return ``B`` values aggregated in a way\n            depending on the :attr:`mode`. :attr:`offsets` is ignored and required to be ``None`` in this case.\n\n          - If :attr:`input` is 1D of shape `(N)`, it will be treated as a concatenation of\n            multiple bags (sequences). :attr:`offsets` is required to be a 1D tensor containing\n            the starting index positions of each bag in :attr:`input`. Therefore, for :attr:`offsets`\n            of shape `(B)`, :attr:`input` will be viewed as having ``B`` bags.\n            Empty bags (i.e., having 0-length) will have returned vectors filled by zeros.\n\n        - :attr:`weight` (Tensor): the learnable weights of the module of shape `(num_embeddings, embedding_dim)`\n\n        - :attr:`per_sample_weights` (Tensor, optional). Has the same shape as :attr:`input`.\n\n        - :attr:`output`: aggregated embedding values of shape `(B, embedding_dim)`\n\n    Examples::\n\n        >>> # an Embedding module containing 10 tensors of size 3\n        >>> embedding_matrix = torch.rand(10, 3)\n        >>> # a batch of 2 samples of 4 indices each\n        >>> input = torch.tensor([1, 2, 4, 5, 4, 3, 2, 9])\n        >>> offsets = torch.tensor([0, 4])\n        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n        >>> F.embedding_bag(input, embedding_matrix, offsets)\n        tensor([[ 0.3397,  0.3552,  0.5545],\n                [ 0.5893,  0.4386,  0.5882]])\n\n        >>> # example with padding_idx\n        >>> embedding_matrix = torch.rand(10, 3)\n        >>> input = torch.tensor([2, 2, 2, 2, 4, 3, 2, 9])\n        >>> offsets = torch.tensor([0, 4])\n        >>> F.embedding_bag(input, embedding_matrix, offsets, padding_idx=2, mode='sum')\n        tensor([[ 0.0000,  0.0000,  0.0000],\n                [-0.7082,  3.2145, -2.6251]])\n    \"\"\"\n    if has_torch_function_variadic(input, weight, offsets, per_sample_weights):\n        return handle_torch_function(\n            embedding_bag,\n            (input, weight, offsets, per_sample_weights),\n            input,\n            weight,\n            offsets=offsets,\n            max_norm=max_norm,\n            norm_type=norm_type,\n            scale_grad_by_freq=scale_grad_by_freq,\n            mode=mode,\n            sparse=sparse,\n            per_sample_weights=per_sample_weights,\n            include_last_offset=include_last_offset,\n            padding_idx=padding_idx,\n        )\n    # Check for backward compatibility.\n    # Used to be embedding_bag(weight, input, ...)\n    # Now is     embedding_bag(input, weight, ...)\n    if weight.dtype == torch.long and input.is_floating_point():\n        warnings.warn(\n            \"Argument order of nn.functional.embedding_bag was changed. \"\n            \"Usage `embedding_bag(weight, input, ...)` is deprecated, \"\n            \"and should now be `embedding_bag(input, weight, ...)`.\"\n        )\n        weight, input = input, weight\n\n    if per_sample_weights is not None and input.size() != per_sample_weights.size():\n        raise ValueError(\n            f\"embedding_bag: If per_sample_weights ({per_sample_weights.shape}) is not None, \"\n            f\"then it must have the same shape as the input ({input.shape})\"\n        )\n\n    if not weight.dim() == 2:\n        raise ValueError(\n            f\"weight has to be a 2D Tensor, but got Tensor of dimension {weight.dim()}\"\n        )\n\n    if input.dim() == 2:\n        if offsets is not None:\n            type_str = \"<unknown>\"\n            # TODO: Remove this once script supports type() calls\n            if not torch.jit.is_scripting():\n                type_str = str(type(offsets))\n            raise ValueError(\n                \"if input is 2D, then offsets has to be None\"\n                \", as input is treated is a mini-batch of\"\n                \" fixed length sequences. However, found \"\n                f\"offsets of type {type_str}\"\n            )\n        offsets = torch.arange(0, input.numel(), input.size(1), dtype=input.dtype, device=input.device)\n\n        input = input.reshape(-1)\n        if per_sample_weights is not None:\n            per_sample_weights = per_sample_weights.reshape(-1)\n    elif input.dim() == 1:\n        if offsets is None:\n            raise ValueError(\"offsets has to be a 1D Tensor but got None\")\n        if offsets.dim() != 1:\n            raise ValueError(\"offsets has to be a 1D Tensor\")\n    else:\n        raise ValueError(f\"input has to be 1D or 2D Tensor, but got Tensor of dimension {input.dim()}\")\n    if mode == \"sum\":\n        mode_enum = 0\n    elif mode == \"mean\":\n        mode_enum = 1\n    elif mode == \"max\":\n        mode_enum = 2\n\n        if scale_grad_by_freq:\n            raise ValueError(\"max mode does not support scaling the gradient by the frequency\")\n\n        if sparse:\n            raise ValueError(\"max mode does not support sparse weights\")\n\n    else:\n        raise ValueError(\"mode has to be one of sum, mean or max\")\n\n    if max_norm is not None:\n        # XXX: equivalent to\n        # with torch.no_grad():\n        #   torch.nembedding_renorm_\n        # remove once script supports set_grad_enabled\n        _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n\n    if per_sample_weights is not None and mode != \"sum\":\n        raise NotImplementedError(\n            \"embedding_bag: per_sample_weights was not None. \"\n            \"per_sample_weights is only supported for mode='sum' \"\n            f\"(got mode='{mode}'). Please open a feature request on GitHub.\"\n        )\n\n    ret, _, _, _ = torch.embedding_bag(\n        weight, input, offsets, scale_grad_by_freq, mode_enum, sparse, per_sample_weights, include_last_offset, padding_idx\n    )\n    return ret\n\n\nif embedding_bag.__doc__:\n    embedding_bag.__doc__ = embedding_bag.__doc__.format(**reproducibility_notes)\n\n\ndef _verify_batch_size(size: List[int]) -> None:\n    # XXX: JIT script does not support the reduce from functools, and mul op is a\n    # builtin, which cannot be used as a value to a func yet, so rewrite this size\n    # check to a simple equivalent for loop\n    #\n    # TODO: make use of reduce like below when JIT is ready with the missing features:\n    # from operator import mul\n    # from functools import reduce\n    #\n    #   if reduce(mul, size[2:], size[0]) == 1\n    size_prods = size[0]\n    for i in range(len(size) - 2):\n        size_prods *= size[i + 2]\n    if size_prods == 1:\n        raise ValueError(f\"Expected more than 1 value per channel when training, got input size {size}\")\n\n\ndef batch_norm(\n    input: Tensor,\n    running_mean: Optional[Tensor],\n    running_var: Optional[Tensor],\n    weight: Optional[Tensor] = None,\n    bias: Optional[Tensor] = None,\n    training: bool = False,\n    momentum: float = 0.1,\n    eps: float = 1e-5,\n) -> Tensor:\n    r\"\"\"Apply Batch Normalization for each channel across a batch of data.\n\n    See :class:`~torch.nn.BatchNorm1d`, :class:`~torch.nn.BatchNorm2d`,\n    :class:`~torch.nn.BatchNorm3d` for details.\n    \"\"\"\n    if has_torch_function_variadic(input, running_mean, running_var, weight, bias):\n        return handle_torch_function(\n            batch_norm,\n            (input, running_mean, running_var, weight, bias),\n            input,\n            running_mean,\n            running_var,\n            weight=weight,\n            bias=bias,\n            training=training,\n            momentum=momentum,\n            eps=eps,\n        )\n    if training:\n        _verify_batch_size(input.size())\n\n    return torch.batch_norm(\n        input, weight, bias, running_mean, running_var, training, momentum, eps, torch.backends.cudnn.enabled\n    )\n\n\ndef _verify_spatial_size(size: List[int]) -> None:\n    # Verify that there is > 1 spatial element for instance norm calculation.\n    size_prods = 1\n    for i in range(2, len(size)):\n        size_prods *= size[i]\n    if size_prods == 1:\n        raise ValueError(f\"Expected more than 1 spatial element when training, got input size {size}\")\n\n\ndef instance_norm(\n    input: Tensor,\n    running_mean: Optional[Tensor] = None,\n    running_var: Optional[Tensor] = None,\n    weight: Optional[Tensor] = None,\n    bias: Optional[Tensor] = None,\n    use_input_stats: bool = True,\n    momentum: float = 0.1,\n    eps: float = 1e-5,\n) -> Tensor:\n    r\"\"\"Apply Instance Normalization independently for each channel in every data sample within a batch.\n\n    See :class:`~torch.nn.InstanceNorm1d`, :class:`~torch.nn.InstanceNorm2d`,\n    :class:`~torch.nn.InstanceNorm3d` for details.\n    \"\"\"\n    if has_torch_function_variadic(input, running_mean, running_var, weight, bias):\n        return handle_torch_function(\n            instance_norm,\n            (input, running_mean, running_var, weight, bias),\n            input,\n            running_mean=running_mean,\n            running_var=running_var,\n            weight=weight,\n            bias=bias,\n            use_input_stats=use_input_stats,\n            momentum=momentum,\n            eps=eps,\n        )\n    if use_input_stats:\n        _verify_spatial_size(input.size())\n    return torch.instance_norm(\n        input, weight, bias, running_mean, running_var, use_input_stats, momentum, eps, torch.backends.cudnn.enabled\n    )\n\n\ndef layer_norm(\n    input: Tensor,\n    normalized_shape: List[int],\n    weight: Optional[Tensor] = None,\n    bias: Optional[Tensor] = None,\n    eps: float = 1e-5,\n) -> Tensor:\n    r\"\"\"Apply Layer Normalization for last certain number of dimensions.\n\n    See :class:`~torch.nn.LayerNorm` for details.\n    \"\"\"\n    if has_torch_function_variadic(input, weight, bias):\n        return handle_torch_function(\n            layer_norm, (input, weight, bias), input, normalized_shape, weight=weight, bias=bias, eps=eps\n        )\n    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)\n\n\ndef group_norm(\n    input: Tensor, num_groups: int, weight: Optional[Tensor] = None, bias: Optional[Tensor] = None, eps: float = 1e-5\n) -> Tensor:\n    r\"\"\"Apply Group Normalization for last certain number of dimensions.\n\n    See :class:`~torch.nn.GroupNorm` for details.\n    \"\"\"\n    if has_torch_function_variadic(input, weight, bias):\n        return handle_torch_function(group_norm, (input, weight, bias,), input, num_groups, weight=weight, bias=bias, eps=eps)\n    if input.dim() < 2:\n        raise RuntimeError(f\"Expected at least 2 dimensions for input tensor but received {input.dim()}\")\n    _verify_batch_size([input.size(0) * input.size(1) // num_groups, num_groups] + list(input.size()[2:]))\n    return torch.group_norm(input, num_groups, weight, bias, eps, torch.backends.cudnn.enabled)\n\n\ndef local_response_norm(input: Tensor, size: int, alpha: float = 1e-4, beta: float = 0.75, k: float = 1.0) -> Tensor:\n    r\"\"\"Apply local response normalization over an input signal.\n\n    The input signal is composed of several input planes, where channels occupy the second dimension.\n    Normalization is applied across channels.\n\n    See :class:`~torch.nn.LocalResponseNorm` for details.\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(local_response_norm, (input,), input, size, alpha=alpha, beta=beta, k=k)\n    dim = input.dim()\n    if dim < 3:\n        raise ValueError(\n            f\"Expected 3D or higher dimensionality                          input (got {dim} dimensions)\"\n        )\n\n    if input.numel() == 0:\n        return input\n\n    div = input.mul(input)\n    if dim == 3:\n        div = div.unsqueeze(1)\n        div = pad(div, (0, 0, size // 2, (size - 1) // 2))\n        div = avg_pool2d(div, (size, 1), stride=1).squeeze(1)\n    else:\n        sizes = input.size()\n        div = div.view(sizes[0], 1, sizes[1], sizes[2], -1)\n        div = pad(div, (0, 0, 0, 0, size // 2, (size - 1) // 2))\n        div = avg_pool3d(div, (size, 1, 1), stride=1).squeeze(1)\n        div = div.view(sizes)\n    div = div.mul(alpha).add(k).pow(beta)\n    return input / div\n\n\n# loss\n\n\ndef ctc_loss(\n    log_probs: Tensor,\n    targets: Tensor,\n    input_lengths: Tensor,\n    target_lengths: Tensor,\n    blank: int = 0,\n    reduction: str = \"mean\",\n    zero_infinity: bool = False,\n) -> Tensor:\n    r\"\"\"Apply the Connectionist Temporal Classification loss.\n\n    See :class:`~torch.nn.CTCLoss` for details.\n\n    Note:\n        {cudnn_reproducibility_note}\n\n    Note:\n        {backward_reproducibility_note}\n\n    Args:\n        log_probs: :math:`(T, N, C)` or :math:`(T, C)` where `C = number of characters in alphabet including blank`,\n            `T = input length`, and `N = batch size`.\n            The logarithmized probabilities of the outputs\n            (e.g. obtained with :func:`torch.nn.functional.log_softmax`).\n        targets: :math:`(N, S)` or `(sum(target_lengths))`.\n            Targets cannot be blank. In the second form, the targets are assumed to be concatenated.\n        input_lengths: :math:`(N)` or :math:`()`.\n            Lengths of the inputs (must each be :math:`\\leq T`)\n        target_lengths: :math:`(N)` or :math:`()`.\n            Lengths of the targets\n        blank (int, optional):\n            Blank label. Default :math:`0`.\n        reduction (str, optional): Specifies the reduction to apply to the output:\n            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n            ``'mean'``: the output losses will be divided by the target lengths and\n            then the mean over the batch is taken, ``'sum'``: the output will be\n            summed. Default: ``'mean'``\n        zero_infinity (bool, optional):\n            Whether to zero infinite losses and the associated gradients.\n            Default: ``False``\n            Infinite losses mainly occur when the inputs are too short\n            to be aligned to the targets.\n\n    Example::\n\n        >>> log_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_()\n        >>> targets = torch.randint(1, 20, (16, 30), dtype=torch.long)\n        >>> input_lengths = torch.full((16,), 50, dtype=torch.long)\n        >>> target_lengths = torch.randint(10, 30, (16,), dtype=torch.long)\n        >>> loss = F.ctc_loss(log_probs, targets, input_lengths, target_lengths)\n        >>> loss.backward()\n    \"\"\"\n    if has_torch_function_variadic(log_probs, targets, input_lengths, target_lengths):\n        return handle_torch_function(\n            ctc_loss,\n            (log_probs, targets, input_lengths, target_lengths),\n            log_probs, targets, input_lengths, target_lengths,\n            blank=blank, reduction=reduction, zero_infinity=zero_infinity\n        )\n    return torch.ctc_loss(\n        log_probs, targets, input_lengths, target_lengths, blank, _Reduction.get_enum(reduction), zero_infinity\n    )\n\n\nif ctc_loss.__doc__:\n    ctc_loss.__doc__ = ctc_loss.__doc__.format(**reproducibility_notes)\n\n\ndef nll_loss(\n    input: Tensor,\n    target: Tensor,\n    weight: Optional[Tensor] = None,\n    size_average: Optional[bool] = None,\n    ignore_index: int = -100,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n) -> Tensor:\n    r\"\"\"Compute the negative log likelihood loss.\n\n    See :class:`~torch.nn.NLLLoss` for details.\n\n    Args:\n        input: :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`\n            in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)` where :math:`K \\geq 1`\n            in the case of K-dimensional loss. `input` is expected to be log-probabilities.\n        target: :math:`(N)` where each value is :math:`0 \\leq \\text{targets}[i] \\leq C-1`,\n            or :math:`(N, d_1, d_2, ..., d_K)` where :math:`K \\geq 1` for\n            K-dimensional loss.\n        weight (Tensor, optional): a manual rescaling weight given to each\n            class. If given, has to be a Tensor of size `C`\n        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n            the losses are averaged over each loss element in the batch. Note that for\n            some losses, there multiple elements per sample. If the field :attr:`size_average`\n            is set to ``False``, the losses are instead summed for each minibatch. Ignored\n            when reduce is ``False``. Default: ``True``\n        ignore_index (int, optional): Specifies a target value that is ignored\n            and does not contribute to the input gradient. When :attr:`size_average` is\n            ``True``, the loss is averaged over non-ignored targets. Default: -100\n        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n            losses are averaged or summed over observations for each minibatch depending\n            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n            batch element instead and ignores :attr:`size_average`. Default: ``True``\n        reduction (str, optional): Specifies the reduction to apply to the output:\n            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n            ``'mean'``: the sum of the output will be divided by the number of\n            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\n    Example::\n\n        >>> # input is of size N x C = 3 x 5\n        >>> input = torch.randn(3, 5, requires_grad=True)\n        >>> # each element in target has to have 0 <= value < C\n        >>> target = torch.tensor([1, 0, 4])\n        >>> output = F.nll_loss(F.log_softmax(input, dim=1), target)\n        >>> output.backward()\n    \"\"\"\n    if has_torch_function_variadic(input, target, weight):\n        return handle_torch_function(\n            nll_loss,\n            (input, target, weight),\n            input,\n            target,\n            weight=weight,\n            size_average=size_average,\n            ignore_index=ignore_index,\n            reduce=reduce,\n            reduction=reduction,\n        )\n    if size_average is not None or reduce is not None:\n        reduction = _Reduction.legacy_get_string(size_average, reduce)\n    return torch._C._nn.nll_loss_nd(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n\n\ndef poisson_nll_loss(\n    input: Tensor,\n    target: Tensor,\n    log_input: bool = True,\n    full: bool = False,\n    size_average: Optional[bool] = None,\n    eps: float = 1e-8,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n) -> Tensor:\n    r\"\"\"Poisson negative log likelihood loss.\n\n    See :class:`~torch.nn.PoissonNLLLoss` for details.\n\n    Args:\n        input: expectation of underlying Poisson distribution.\n        target: random sample :math:`target \\sim \\text{Poisson}(input)`.\n        log_input: if ``True`` the loss is computed as\n            :math:`\\exp(\\text{input}) - \\text{target} * \\text{input}`, if ``False`` then loss is\n            :math:`\\text{input} - \\text{target} * \\log(\\text{input}+\\text{eps})`. Default: ``True``\n        full: whether to compute full loss, i. e. to add the Stirling\n            approximation term. Default: ``False``\n            :math:`\\text{target} * \\log(\\text{target}) - \\text{target} + 0.5 * \\log(2 * \\pi * \\text{target})`.\n        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n            the losses are averaged over each loss element in the batch. Note that for\n            some losses, there multiple elements per sample. If the field :attr:`size_average`\n            is set to ``False``, the losses are instead summed for each minibatch. Ignored\n            when reduce is ``False``. Default: ``True``\n        eps (float, optional): Small value to avoid evaluation of :math:`\\log(0)` when\n            :attr:`log_input`\\ =\\ ``False``. Default: 1e-8\n        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n            losses are averaged or summed over observations for each minibatch depending\n            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n            batch element instead and ignores :attr:`size_average`. Default: ``True``\n        reduction (str, optional): Specifies the reduction to apply to the output:\n            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n            ``'mean'``: the sum of the output will be divided by the number of\n            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\n    \"\"\"\n    if has_torch_function_variadic(input, target):\n        return handle_torch_function(\n            poisson_nll_loss,\n            (input, target),\n            input,\n            target,\n            log_input=log_input,\n            full=full,\n            size_average=size_average,\n            eps=eps,\n            reduce=reduce,\n            reduction=reduction,\n        )\n    if size_average is not None or reduce is not None:\n        reduction = _Reduction.legacy_get_string(size_average, reduce)\n    if reduction != \"none\" and reduction != \"mean\" and reduction != \"sum\":\n        ret = input\n        raise ValueError(reduction + \" is not a valid value for reduction\")\n\n    ret = torch.poisson_nll_loss(input, target, log_input, full, eps, _Reduction.get_enum(reduction))\n    return ret\n\n\ndef gaussian_nll_loss(\n    input: Tensor,\n    target: Tensor,\n    var: Tensor,\n    full: bool = False,\n    eps: float = 1e-6,\n    reduction: str = \"mean\",\n) -> Tensor:\n    r\"\"\"Gaussian negative log likelihood loss.\n\n    See :class:`~torch.nn.GaussianNLLLoss` for details.\n\n    Args:\n        input: expectation of the Gaussian distribution.\n        target: sample from the Gaussian distribution.\n        var: tensor of positive variance(s), one for each of the expectations\n            in the input (heteroscedastic), or a single one (homoscedastic).\n        full (bool, optional): include the constant term in the loss calculation. Default: ``False``.\n        eps (float, optional): value added to var, for stability. Default: 1e-6.\n        reduction (str, optional): specifies the reduction to apply to the output:\n            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n            ``'mean'``: the output is the average of all batch member losses,\n            ``'sum'``: the output is the sum of all batch member losses.\n            Default: ``'mean'``.\n    \"\"\"\n    if has_torch_function_variadic(input, target, var):\n        return handle_torch_function(\n            gaussian_nll_loss,\n            (input, target, var),\n            input,\n            target,\n            var,\n            full=full,\n            eps=eps,\n            reduction=reduction,\n        )\n\n    # Check var size\n    # If var.size == input.size, the case is heteroscedastic and no further checks are needed.\n    # Otherwise:\n    if var.size() != input.size():\n\n        # If var is one dimension short of input, but the sizes match otherwise, then this is a homoscedastic case.\n        # e.g. input.size = (10, 2, 3), var.size = (10, 2)\n        # -> unsqueeze var so that var.shape = (10, 2, 1)\n        # this is done so that broadcasting can happen in the loss calculation\n        if input.size()[:-1] == var.size():\n            var = torch.unsqueeze(var, -1)\n\n        # This checks if the sizes match up to the final dimension, and the final dimension of var is of size 1.\n        # This is also a homoscedastic case.\n        # e.g. input.size = (10, 2, 3), var.size = (10, 2, 1)\n        elif input.size()[:-1] == var.size()[:-1] and var.size(-1) == 1:  # Heteroscedastic case\n            pass\n\n        # If none of the above pass, then the size of var is incorrect.\n        else:\n            raise ValueError(\"var is of incorrect size\")\n\n    # Check validity of reduction mode\n    if reduction != 'none' and reduction != 'mean' and reduction != 'sum':\n        raise ValueError(reduction + \" is not valid\")\n\n    # Entries of var must be non-negative\n    if torch.any(var < 0):\n        raise ValueError(\"var has negative entry/entries\")\n\n    # Clamp for stability\n    var = var.clone()\n    with torch.no_grad():\n        var.clamp_(min=eps)\n\n    # Calculate the loss\n    loss = 0.5 * (torch.log(var) + (input - target)**2 / var)\n    if full:\n        loss += 0.5 * math.log(2 * math.pi)\n\n    if reduction == 'mean':\n        return loss.mean()\n    elif reduction == 'sum':\n        return loss.sum()\n    else:\n        return loss\n\n\ndef kl_div(\n    input: Tensor,\n    target: Tensor,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n    log_target: bool = False,\n) -> Tensor:\n    r\"\"\"Compute the KL Divergence loss.\n\n    Refer - The `Kullback-Leibler divergence Loss\n    <https://en.wikipedia.org/wiki/Kullback-Leibler_divergence>`__\n\n    See :class:`~torch.nn.KLDivLoss` for details.\n\n    Args:\n        input: Tensor of arbitrary shape in log-probabilities.\n        target: Tensor of the same shape as input. See :attr:`log_target` for\n            the target's interpretation.\n        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n            the losses are averaged over each loss element in the batch. Note that for\n            some losses, there multiple elements per sample. If the field :attr:`size_average`\n            is set to ``False``, the losses are instead summed for each minibatch. Ignored\n            when reduce is ``False``. Default: ``True``\n        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n            losses are averaged or summed over observations for each minibatch depending\n            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n            batch element instead and ignores :attr:`size_average`. Default: ``True``\n        reduction (str, optional): Specifies the reduction to apply to the output:\n            ``'none'`` | ``'batchmean'`` | ``'sum'`` | ``'mean'``.\n            ``'none'``: no reduction will be applied\n            ``'batchmean'``: the sum of the output will be divided by the batchsize\n            ``'sum'``: the output will be summed\n            ``'mean'``: the output will be divided by the number of elements in the output\n            Default: ``'mean'``\n        log_target (bool): A flag indicating whether ``target`` is passed in the log space.\n            It is recommended to pass certain distributions (like ``softmax``)\n            in the log space to avoid numerical issues caused by explicit ``log``.\n            Default: ``False``\n\n    .. note::\n        :attr:`size_average` and :attr:`reduce` are in the process of being deprecated,\n        and in the meantime, specifying either of those two args will override :attr:`reduction`.\n\n    .. warning::\n        :attr:`reduction` = ``'mean'`` doesn't return the true kl divergence value, please use\n        :attr:`reduction` = ``'batchmean'`` which aligns with KL math definition.\n    \"\"\"\n    if has_torch_function_variadic(input, target):\n        return handle_torch_function(\n            kl_div,\n            (input, target),\n            input,\n            target,\n            size_average=size_average,\n            reduce=reduce,\n            reduction=reduction,\n            log_target=log_target,\n        )\n    if size_average is not None or reduce is not None:\n        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n    else:\n        if reduction == \"mean\":\n            warnings.warn(\n                \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n                \"'batchmean' divides only by the batch size, and aligns with the KL div math definition.\"\n                \"'mean' will be changed to behave the same as 'batchmean' in the next major release.\"\n            )\n\n        # special case for batchmean\n        if reduction == \"batchmean\":\n            reduction_enum = _Reduction.get_enum(\"sum\")\n        else:\n            reduction_enum = _Reduction.get_enum(reduction)\n\n    reduced = torch.kl_div(input, target, reduction_enum, log_target=log_target)\n\n    if reduction == \"batchmean\" and input.dim() != 0:\n        reduced = reduced / input.size()[0]\n\n    return reduced\n\n\ndef cross_entropy(\n    input: Tensor,\n    target: Tensor,\n    weight: Optional[Tensor] = None,\n    size_average: Optional[bool] = None,\n    ignore_index: int = -100,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n    label_smoothing: float = 0.0,\n) -> Tensor:\n    r\"\"\"Compute the cross entropy loss between input logits and target.\n\n    See :class:`~torch.nn.CrossEntropyLoss` for details.\n\n    Args:\n        input (Tensor) : Predicted unnormalized logits;\n            see Shape section below for supported shapes.\n        target (Tensor) : Ground truth class indices or class probabilities;\n            see Shape section below for supported shapes.\n        weight (Tensor, optional): a manual rescaling weight given to each\n            class. If given, has to be a Tensor of size `C`\n        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n            the losses are averaged over each loss element in the batch. Note that for\n            some losses, there multiple elements per sample. If the field :attr:`size_average`\n            is set to ``False``, the losses are instead summed for each minibatch. Ignored\n            when reduce is ``False``. Default: ``True``\n        ignore_index (int, optional): Specifies a target value that is ignored\n            and does not contribute to the input gradient. When :attr:`size_average` is\n            ``True``, the loss is averaged over non-ignored targets. Note that\n            :attr:`ignore_index` is only applicable when the target contains class indices.\n            Default: -100\n        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n            losses are averaged or summed over observations for each minibatch depending\n            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n            batch element instead and ignores :attr:`size_average`. Default: ``True``\n        reduction (str, optional): Specifies the reduction to apply to the output:\n            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n            ``'mean'``: the sum of the output will be divided by the number of\n            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n        label_smoothing (float, optional): A float in [0.0, 1.0]. Specifies the amount\n            of smoothing when computing the loss, where 0.0 means no smoothing. The targets\n            become a mixture of the original ground truth and a uniform distribution as described in\n            `Rethinking the Inception Architecture for Computer Vision <https://arxiv.org/abs/1512.00567>`__. Default: :math:`0.0`.\n\n    Shape:\n        - Input: Shape :math:`(C)`, :math:`(N, C)` or :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n          in the case of `K`-dimensional loss.\n        - Target: If containing class indices, shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with\n          :math:`K \\geq 1` in the case of K-dimensional loss where each value should be between :math:`[0, C)`.\n          If containing class probabilities, same shape as the input and each value should be between :math:`[0, 1]`.\n\n        where:\n\n        .. math::\n            \\begin{aligned}\n                C ={} & \\text{number of classes} \\\\\n                N ={} & \\text{batch size} \\\\\n            \\end{aligned}\n\n    Examples::\n\n        >>> # Example of target with class indices\n        >>> input = torch.randn(3, 5, requires_grad=True)\n        >>> target = torch.randint(5, (3,), dtype=torch.int64)\n        >>> loss = F.cross_entropy(input, target)\n        >>> loss.backward()\n        >>>\n        >>> # Example of target with class probabilities\n        >>> input = torch.randn(3, 5, requires_grad=True)\n        >>> target = torch.randn(3, 5).softmax(dim=1)\n        >>> loss = F.cross_entropy(input, target)\n        >>> loss.backward()\n    \"\"\"\n    if has_torch_function_variadic(input, target, weight):\n        return handle_torch_function(\n            cross_entropy,\n            (input, target, weight),\n            input,\n            target,\n            weight=weight,\n            size_average=size_average,\n            ignore_index=ignore_index,\n            reduce=reduce,\n            reduction=reduction,\n            label_smoothing=label_smoothing,\n        )\n    if size_average is not None or reduce is not None:\n        reduction = _Reduction.legacy_get_string(size_average, reduce)\n    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n\n\ndef binary_cross_entropy(\n    input: Tensor,\n    target: Tensor,\n    weight: Optional[Tensor] = None,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n) -> Tensor:\n    r\"\"\"Measure Binary Cross Entropy between the target and input probabilities.\n\n    See :class:`~torch.nn.BCELoss` for details.\n\n    Args:\n        input: Tensor of arbitrary shape as probabilities.\n        target: Tensor of the same shape as input with values between 0 and 1.\n        weight (Tensor, optional): a manual rescaling weight\n                if provided it's repeated to match input tensor shape\n        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n            the losses are averaged over each loss element in the batch. Note that for\n            some losses, there multiple elements per sample. If the field :attr:`size_average`\n            is set to ``False``, the losses are instead summed for each minibatch. Ignored\n            when reduce is ``False``. Default: ``True``\n        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n            losses are averaged or summed over observations for each minibatch depending\n            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n            batch element instead and ignores :attr:`size_average`. Default: ``True``\n        reduction (str, optional): Specifies the reduction to apply to the output:\n            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n            ``'mean'``: the sum of the output will be divided by the number of\n            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\n    Examples::\n\n        >>> input = torch.randn(3, 2, requires_grad=True)\n        >>> target = torch.rand(3, 2, requires_grad=False)\n        >>> loss = F.binary_cross_entropy(torch.sigmoid(input), target)\n        >>> loss.backward()\n    \"\"\"\n    if has_torch_function_variadic(input, target, weight):\n        return handle_torch_function(\n            binary_cross_entropy,\n            (input, target, weight),\n            input,\n            target,\n            weight=weight,\n            size_average=size_average,\n            reduce=reduce,\n            reduction=reduction,\n        )\n    if size_average is not None or reduce is not None:\n        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n    else:\n        reduction_enum = _Reduction.get_enum(reduction)\n    if target.size() != input.size():\n        raise ValueError(\n            \"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\n            \"Please ensure they have the same size.\".format(target.size(), input.size())\n        )\n\n    if weight is not None:\n        new_size = _infer_size(target.size(), weight.size())\n        weight = weight.expand(new_size)\n\n    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)\n\n\ndef binary_cross_entropy_with_logits(\n    input: Tensor,\n    target: Tensor,\n    weight: Optional[Tensor] = None,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n    pos_weight: Optional[Tensor] = None,\n) -> Tensor:\n    r\"\"\"Calculate Binary Cross Entropy between target and input logits.\n\n    See :class:`~torch.nn.BCEWithLogitsLoss` for details.\n\n    Args:\n        input: Tensor of arbitrary shape as unnormalized scores (often referred to as logits).\n        target: Tensor of the same shape as input with values between 0 and 1\n        weight (Tensor, optional): a manual rescaling weight\n            if provided it's repeated to match input tensor shape\n        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n            the losses are averaged over each loss element in the batch. Note that for\n            some losses, there multiple elements per sample. If the field :attr:`size_average`\n            is set to ``False``, the losses are instead summed for each minibatch. Ignored\n            when reduce is ``False``. Default: ``True``\n        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n            losses are averaged or summed over observations for each minibatch depending\n            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n            batch element instead and ignores :attr:`size_average`. Default: ``True``\n        reduction (str, optional): Specifies the reduction to apply to the output:\n            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n            ``'mean'``: the sum of the output will be divided by the number of\n            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n        pos_weight (Tensor, optional): a weight of positive examples to be broadcasted with target.\n            Must be a tensor with equal size along the class dimension to the number of classes.\n            Pay close attention to PyTorch's broadcasting semantics in order to achieve the desired\n            operations. For a target of size [B, C, H, W] (where B is batch size) pos_weight of\n            size [B, C, H, W] will apply different pos_weights to each element of the batch or\n            [C, H, W] the same pos_weights across the batch. To apply the same positive weight\n            along all spacial dimensions for a 2D multi-class target [C, H, W] use: [C, 1, 1].\n            Default: ``None``\n\n    Examples::\n\n         >>> input = torch.randn(3, requires_grad=True)\n         >>> target = torch.empty(3).random_(2)\n         >>> loss = F.binary_cross_entropy_with_logits(input, target)\n         >>> loss.backward()\n    \"\"\"\n    if has_torch_function_variadic(input, target, weight, pos_weight):\n        return handle_torch_function(\n            binary_cross_entropy_with_logits,\n            (input, target, weight, pos_weight),\n            input,\n            target,\n            weight=weight,\n            size_average=size_average,\n            reduce=reduce,\n            reduction=reduction,\n            pos_weight=pos_weight,\n        )\n    if size_average is not None or reduce is not None:\n        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n    else:\n        reduction_enum = _Reduction.get_enum(reduction)\n\n    if not (target.size() == input.size()):\n        raise ValueError(f\"Target size ({target.size()}) must be the same as input size ({input.size()})\")\n\n    return torch.binary_cross_entropy_with_logits(input, target, weight, pos_weight, reduction_enum)\n\n\ndef smooth_l1_loss(\n    input: Tensor,\n    target: Tensor,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n    beta: float = 1.0,\n) -> Tensor:\n    r\"\"\"Compute the Smooth L1 loss.\n\n    Function uses a squared term if the absolute\n    element-wise error falls below beta and an L1 term otherwise.\n\n    See :class:`~torch.nn.SmoothL1Loss` for details.\n    \"\"\"\n    if has_torch_function_variadic(input, target):\n        return handle_torch_function(\n            smooth_l1_loss,\n            (input, target),\n            input,\n            target,\n            size_average=size_average,\n            reduce=reduce,\n            reduction=reduction,\n            beta=beta,\n        )\n    if not (target.size() == input.size()):\n        warnings.warn(\n            f\"Using a target size ({target.size()}) that is different to the input size ({input.size()}). \"\n            \"This will likely lead to incorrect results due to broadcasting. \"\n            \"Please ensure they have the same size.\",\n            stacklevel=2,\n        )\n    if size_average is not None or reduce is not None:\n        reduction = _Reduction.legacy_get_string(size_average, reduce)\n\n    expanded_input, expanded_target = torch.broadcast_tensors(input, target)\n\n    if beta == 0.0:\n        return torch._C._nn.l1_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))\n    else:\n        return torch._C._nn.smooth_l1_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction), beta)\n\n\ndef huber_loss(\n    input: Tensor,\n    target: Tensor,\n    reduction: str = 'mean',\n    delta: float = 1.0,\n) -> Tensor:\n    r\"\"\"Compute the Huber loss.\n\n    Function uses a squared term if the absolute\n    element-wise error falls below delta and a delta-scaled L1 term otherwise.\n\n    When delta equals 1, this loss is equivalent to SmoothL1Loss.\n    In general, Huber loss differs from SmoothL1Loss by a factor of delta (AKA beta in Smooth L1).\n\n    See :class:`~torch.nn.HuberLoss` for details.\n    \"\"\"\n    if has_torch_function_variadic(input, target):\n        return handle_torch_function(\n            huber_loss,\n            (input, target),\n            input,\n            target,\n            reduction=reduction,\n            delta=delta,\n        )\n    if not (target.size() == input.size()):\n        warnings.warn(f\"Using a target size ({target.size()}) that is different to the input size ({input.size()}). \"\n                      \"This will likely lead to incorrect results due to broadcasting. \"\n                      \"Please ensure they have the same size.\",\n                      stacklevel=2)\n\n    expanded_input, expanded_target = torch.broadcast_tensors(input, target)\n    return torch._C._nn.huber_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction), delta)\n\n\ndef l1_loss(\n    input: Tensor,\n    target: Tensor,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n) -> Tensor:  # noqa: D400,D402\n    r\"\"\"l1_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor\n\n    Function that takes the mean element-wise absolute value difference.\n\n    See :class:`~torch.nn.L1Loss` for details.\n    \"\"\"\n    if has_torch_function_variadic(input, target):\n        return handle_torch_function(\n            l1_loss, (input, target), input, target, size_average=size_average, reduce=reduce, reduction=reduction\n        )\n    if not (target.size() == input.size()):\n        warnings.warn(\n            f\"Using a target size ({target.size()}) that is different to the input size ({input.size()}). \"\n            \"This will likely lead to incorrect results due to broadcasting. \"\n            \"Please ensure they have the same size.\",\n            stacklevel=2,\n        )\n    if size_average is not None or reduce is not None:\n        reduction = _Reduction.legacy_get_string(size_average, reduce)\n\n    expanded_input, expanded_target = torch.broadcast_tensors(input, target)\n    return torch._C._nn.l1_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))\n\n\ndef mse_loss(\n    input: Tensor,\n    target: Tensor,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n) -> Tensor:  # noqa: D400,D402\n    r\"\"\"mse_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor\n\n    Measures the element-wise mean squared error.\n    See :class:`~torch.nn.MSELoss` for details.\n    \"\"\"\n    if has_torch_function_variadic(input, target):\n        return handle_torch_function(\n            mse_loss, (input, target), input, target, size_average=size_average, reduce=reduce, reduction=reduction\n        )\n    if not (target.size() == input.size()):\n        warnings.warn(\n            f\"Using a target size ({target.size()}) that is different to the input size ({input.size()}). \"\n            \"This will likely lead to incorrect results due to broadcasting. \"\n            \"Please ensure they have the same size.\",\n            stacklevel=2,\n        )\n    if size_average is not None or reduce is not None:\n        reduction = _Reduction.legacy_get_string(size_average, reduce)\n\n    expanded_input, expanded_target = torch.broadcast_tensors(input, target)\n    return torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))\n\n\ndef margin_ranking_loss(\n    input1: Tensor,\n    input2: Tensor,\n    target: Tensor,\n    margin: float = 0,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n) -> Tensor:  # noqa: D400,D402\n    r\"\"\"margin_ranking_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') -> Tensor\n\n    See :class:`~torch.nn.MarginRankingLoss` for details.\n    \"\"\"\n    if has_torch_function_variadic(input1, input2, target):\n        return handle_torch_function(\n            margin_ranking_loss,\n            (input1, input2, target),\n            input1,\n            input2,\n            target,\n            margin=margin,\n            size_average=size_average,\n            reduce=reduce,\n            reduction=reduction,\n        )\n    if size_average is not None or reduce is not None:\n        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n    else:\n        reduction_enum = _Reduction.get_enum(reduction)\n    if (input1.dim() != input2.dim() or input1.dim() != target.dim()):\n        raise RuntimeError(\n            f\"margin_ranking_loss : All input tensors should have same dimension but got sizes: \"\n            f\"input1: {input1.size()}, input2: {input2.size()}, target: {target.size()} \"\n        )\n    return torch.margin_ranking_loss(input1, input2, target, margin, reduction_enum)\n\n\ndef hinge_embedding_loss(\n    input: Tensor,\n    target: Tensor,\n    margin: float = 1.0,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n) -> Tensor:  # noqa: D400,D402\n    r\"\"\"hinge_embedding_loss(input, target, margin=1.0, size_average=None, reduce=None, reduction='mean') -> Tensor\n\n    See :class:`~torch.nn.HingeEmbeddingLoss` for details.\n    \"\"\"\n    if has_torch_function_variadic(input, target):\n        return handle_torch_function(\n            hinge_embedding_loss,\n            (input, target),\n            input,\n            target,\n            margin=margin,\n            size_average=size_average,\n            reduce=reduce,\n            reduction=reduction,\n        )\n    if size_average is not None or reduce is not None:\n        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n    else:\n        reduction_enum = _Reduction.get_enum(reduction)\n    return torch.hinge_embedding_loss(input, target, margin, reduction_enum)\n\n\ndef multilabel_margin_loss(\n    input: Tensor,\n    target: Tensor,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n) -> Tensor:  # noqa: D400,D402\n    r\"\"\"multilabel_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor\n\n    See :class:`~torch.nn.MultiLabelMarginLoss` for details.\n    \"\"\"\n    if has_torch_function_variadic(input, target):\n        return handle_torch_function(\n            multilabel_margin_loss,\n            (input, target),\n            input,\n            target,\n            size_average=size_average,\n            reduce=reduce,\n            reduction=reduction,\n        )\n    if size_average is not None or reduce is not None:\n        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n    else:\n        reduction_enum = _Reduction.get_enum(reduction)\n    return torch._C._nn.multilabel_margin_loss(input, target, reduction_enum)\n\n\ndef soft_margin_loss(\n    input: Tensor,\n    target: Tensor,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n) -> Tensor:  # noqa: D400,D402\n    r\"\"\"\n    soft_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor\n\n    See :class:`~torch.nn.SoftMarginLoss` for details.\n    \"\"\"\n    if has_torch_function_variadic(input, target):\n        return handle_torch_function(\n            soft_margin_loss, (input, target), input, target, size_average=size_average, reduce=reduce, reduction=reduction\n        )\n    if size_average is not None or reduce is not None:\n        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n    else:\n        reduction_enum = _Reduction.get_enum(reduction)\n    return torch._C._nn.soft_margin_loss(input, target, reduction_enum)\n\n\ndef multilabel_soft_margin_loss(\n    input: Tensor,\n    target: Tensor,\n    weight: Optional[Tensor] = None,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n) -> Tensor:  # noqa: D400,D402\n    r\"\"\"multilabel_soft_margin_loss(input, target, weight=None, size_average=None, reduce=None, reduction='mean') -> Tensor\n\n    See :class:`~torch.nn.MultiLabelSoftMarginLoss` for details.\n    \"\"\"\n    if has_torch_function_variadic(input, target, weight):\n        return handle_torch_function(\n            multilabel_soft_margin_loss,\n            (input, target, weight),\n            input,\n            target,\n            weight=weight,\n            size_average=size_average,\n            reduce=reduce,\n            reduction=reduction,\n        )\n    if size_average is not None or reduce is not None:\n        reduction = _Reduction.legacy_get_string(size_average, reduce)\n\n    loss = -(target * logsigmoid(input) + (1 - target) * logsigmoid(-input))\n\n    if weight is not None:\n        loss = loss * weight\n\n    class_dim = input.dim() - 1\n    C = input.size(class_dim)\n    loss = loss.sum(dim=class_dim) / C  # only return N loss values\n\n    if reduction == \"none\":\n        ret = loss\n    elif reduction == \"mean\":\n        ret = loss.mean()\n    elif reduction == \"sum\":\n        ret = loss.sum()\n    else:\n        ret = input\n        raise ValueError(reduction + \" is not valid\")\n    return ret\n\n\ndef cosine_embedding_loss(\n    input1: Tensor,\n    input2: Tensor,\n    target: Tensor,\n    margin: float = 0,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n) -> Tensor:  # noqa: D400,D402\n    r\"\"\"cosine_embedding_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') -> Tensor\n\n    See :class:`~torch.nn.CosineEmbeddingLoss` for details.\n    \"\"\"\n    if has_torch_function_variadic(input1, input2, target):\n        return handle_torch_function(\n            cosine_embedding_loss,\n            (input1, input2, target),\n            input1,\n            input2,\n            target,\n            margin=margin,\n            size_average=size_average,\n            reduce=reduce,\n            reduction=reduction,\n        )\n    if size_average is not None or reduce is not None:\n        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n    else:\n        reduction_enum = _Reduction.get_enum(reduction)\n    return torch.cosine_embedding_loss(input1, input2, target, margin, reduction_enum)\n\n\ndef multi_margin_loss(\n    input: Tensor,\n    target: Tensor,\n    p: int = 1,\n    margin: float = 1.0,\n    weight: Optional[Tensor] = None,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n) -> Tensor:  # noqa: D400,D402\n    r\"\"\"multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None, reduce=None, reduction='mean') -> Tensor\n\n    See :class:`~torch.nn.MultiMarginLoss` for details.\n    \"\"\"\n    if has_torch_function_variadic(input, target, weight):\n        return handle_torch_function(\n            multi_margin_loss,\n            (input, target, weight),\n            input,\n            target,\n            p=p,\n            margin=margin,\n            weight=weight,\n            size_average=size_average,\n            reduce=reduce,\n            reduction=reduction,\n        )\n    if size_average is not None or reduce is not None:\n        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n    else:\n        reduction_enum = _Reduction.get_enum(reduction)\n    if p != 1 and p != 2:\n        raise ValueError(\"only p == 1 and p == 2 supported\")\n    if weight is not None:\n        if weight.dim() != 1:\n            raise ValueError(\"weight must be one-dimensional\")\n\n    return torch._C._nn.multi_margin_loss(input, target, p, margin, weight, reduction_enum)\n\n\npixel_shuffle = _add_docstr(\n    torch.pixel_shuffle,\n    r\"\"\"\npixel_shuffle(input, upscale_factor) -> Tensor\n\nRearranges elements in a tensor of shape :math:`(*, C \\times r^2, H, W)` to a\ntensor of shape :math:`(*, C, H \\times r, W \\times r)`, where r is the :attr:`upscale_factor`.\n\nSee :class:`~torch.nn.PixelShuffle` for details.\n\nArgs:\n    input (Tensor): the input tensor\n    upscale_factor (int): factor to increase spatial resolution by\n\nExamples::\n\n    >>> input = torch.randn(1, 9, 4, 4)\n    >>> output = torch.nn.functional.pixel_shuffle(input, 3)\n    >>> print(output.size())\n    torch.Size([1, 1, 12, 12])\n\"\"\",\n)\n\npixel_unshuffle = _add_docstr(\n    torch.pixel_unshuffle,\n    r\"\"\"\npixel_unshuffle(input, downscale_factor) -> Tensor\n\nReverses the :class:`~torch.nn.PixelShuffle` operation by rearranging elements in a\ntensor of shape :math:`(*, C, H \\times r, W \\times r)` to a tensor of shape\n:math:`(*, C \\times r^2, H, W)`, where r is the :attr:`downscale_factor`.\n\nSee :class:`~torch.nn.PixelUnshuffle` for details.\n\nArgs:\n    input (Tensor): the input tensor\n    downscale_factor (int): factor to increase spatial resolution by\n\nExamples::\n\n    >>> input = torch.randn(1, 1, 12, 12)\n    >>> output = torch.nn.functional.pixel_unshuffle(input, 3)\n    >>> print(output.size())\n    torch.Size([1, 9, 4, 4])\n\"\"\",\n)\n\nchannel_shuffle = _add_docstr(\n    torch.channel_shuffle,\n    r\"\"\"\nchannel_shuffle(input, groups) -> Tensor\n\nDivide the channels in a tensor of shape :math:`(*, C , H, W)`\ninto g groups and rearrange them as :math:`(*, C \\frac g, g, H, W)`,\nwhile keeping the original tensor shape.\n\nSee :class:`~torch.nn.ChannelShuffle` for details.\n\nArgs:\n    input (Tensor): the input tensor\n    groups (int): number of groups to divide channels in and rearrange.\n\nExamples::\n\n    >>> input = torch.randn(1, 4, 2, 2)\n    >>> print(input)\n    [[[[1, 2],\n       [3, 4]],\n      [[5, 6],\n       [7, 8]],\n      [[9, 10],\n       [11, 12]],\n      [[13, 14],\n       [15, 16]],\n     ]]\n    >>> output = torch.nn.functional.channel_shuffle(input, 2)\n    >>> print(output)\n    [[[[1, 2],\n       [3, 4]],\n      [[9, 10],\n       [11, 12]],\n      [[5, 6],\n       [7, 8]],\n      [[13, 14],\n       [15, 16]],\n     ]]\n\"\"\",\n)\n\nnative_channel_shuffle = _add_docstr(\n    torch.native_channel_shuffle,\n    r\"\"\"\nnative_channel_shuffle(input, groups) -> Tensor\n\nNative kernel level implementation of the `channel_shuffle`.\nThis function might become private in future releases, use with caution.\n\nDivide the channels in a tensor of shape :math:`(*, C , H, W)`\ninto g groups and rearrange them as :math:`(*, C \\frac g, g, H, W)`,\nwhile keeping the original tensor shape.\n\nSee :class:`~torch.nn.ChannelShuffle` for details.\n\nArgs:\n    input (Tensor): the input tensor\n    groups (int): number of groups to divide channels in and rearrange.\n\nExamples::\n\n    >>> input = torch.randn(1, 4, 2, 2)\n    >>> print(input)\n    [[[[1, 2],\n       [3, 4]],\n      [[5, 6],\n       [7, 8]],\n      [[9, 10],\n       [11, 12]],\n      [[13, 14],\n       [15, 16]],\n     ]]\n    >>> output = torch.nn.functional.native_channel_shuffle(input, 2)\n    >>> print(output)\n    [[[[1, 2],\n       [3, 4]],\n      [[9, 10],\n       [11, 12]],\n      [[5, 6],\n       [7, 8]],\n      [[13, 14],\n       [15, 16]],\n     ]]\n\"\"\",\n)\n\n@_overload  # noqa: F811\ndef upsample(input: Tensor, size: Optional[int] = None, scale_factor: Optional[float] = None, mode: str = \"nearest\", align_corners: Optional[bool] = None) -> Tensor:  # noqa: F811,B950\n    pass\n\n\n@_overload  # noqa: F811\ndef upsample(input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[float] = None, mode: str = \"nearest\", align_corners: Optional[bool] = None) -> Tensor:  # noqa: F811,B950\n    pass\n\n\ndef upsample(input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None):  # noqa: F811\n    r\"\"\"Upsample input.\n\n    Provided tensor is upsampled to either the given :attr:`size` or the given\n    :attr:`scale_factor`\n\n    .. warning::\n        This function is deprecated in favor of :func:`torch.nn.functional.interpolate`.\n        This is equivalent with ``nn.functional.interpolate(...)``.\n\n    Note:\n        {backward_reproducibility_note}\n\n    The algorithm used for upsampling is determined by :attr:`mode`.\n\n    Currently temporal, spatial and volumetric upsampling are supported, i.e.\n    expected inputs are 3-D, 4-D or 5-D in shape.\n\n    The input dimensions are interpreted in the form:\n    `mini-batch x channels x [optional depth] x [optional height] x width`.\n\n    The modes available for upsampling are: `nearest`, `linear` (3D-only),\n    `bilinear`, `bicubic` (4D-only), `trilinear` (5D-only)\n\n    Args:\n        input (Tensor): the input tensor\n        size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]):\n            output spatial size.\n        scale_factor (float or Tuple[float]): multiplier for spatial size. Has to match input size if it is a tuple.\n        mode (str): algorithm used for upsampling:\n            ``'nearest'`` | ``'linear'`` | ``'bilinear'`` | ``'bicubic'`` |\n            ``'trilinear'``. Default: ``'nearest'``\n        align_corners (bool, optional): Geometrically, we consider the pixels of the\n            input and output as squares rather than points.\n            If set to ``True``, the input and output tensors are aligned by the\n            center points of their corner pixels, preserving the values at the corner pixels.\n            If set to ``False``, the input and output tensors are aligned by the corner\n            points of their corner pixels, and the interpolation uses edge value padding\n            for out-of-boundary values, making this operation *independent* of input size\n            when :attr:`scale_factor` is kept the same. This only has an effect when :attr:`mode`\n            is ``'linear'``, ``'bilinear'``, ``'bicubic'`` or ``'trilinear'``.\n            Default: ``False``\n\n    .. note::\n        With ``mode='bicubic'``, it's possible to cause overshoot, in other words it can produce\n        negative values or values greater than 255 for images.\n        Explicitly call ``result.clamp(min=0, max=255)`` if you want to reduce the overshoot\n        when displaying the image.\n\n    .. warning::\n        With ``align_corners = True``, the linearly interpolating modes\n        (`linear`, `bilinear`, and `trilinear`) don't proportionally align the\n        output and input pixels, and thus the output values can depend on the\n        input size. This was the default behavior for these modes up to version\n        0.3.1. Since then, the default behavior is ``align_corners = False``.\n        See :class:`~torch.nn.Upsample` for concrete examples on how this\n        affects the outputs.\n\n    \"\"\"\n    warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n    return interpolate(input, size, scale_factor, mode, align_corners)\n\n\nif upsample.__doc__:\n    upsample.__doc__ = upsample.__doc__.format(**reproducibility_notes)\n\n\ndef _is_integer(x) -> bool:\n    r\"\"\"Type check the input number is an integer.\n\n    Will return True for int, SymInt, Numpy integers and Tensors with integer elements.\n    \"\"\"\n    if isinstance(x, (int, torch.SymInt)):\n        return True\n    if np is not None and isinstance(x, np.integer):\n        return True\n    return isinstance(x, Tensor) and not x.is_floating_point()\n\n\n@_overload  # noqa: F811\ndef interpolate(input: Tensor, size: Optional[int] = None, scale_factor: Optional[List[float]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False) -> Tensor:  # noqa: F811,B950\n    pass\n\n\n@_overload  # noqa: F811\ndef interpolate(input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[List[float]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False) -> Tensor:  # noqa: F811,B950\n    pass\n\n\n@_overload  # noqa: F811\ndef interpolate(input: Tensor, size: Optional[int] = None, scale_factor: Optional[float] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False) -> Tensor:  # noqa: F811,B950\n    pass\n\n\n@_overload  # noqa: F811\ndef interpolate(  # noqa: F811\n    input: Tensor,\n    size: Optional[List[int]] = None,\n    scale_factor: Optional[float] = None,\n    mode: str = \"nearest\",\n    align_corners: Optional[bool] = None,\n    recompute_scale_factor: Optional[bool] = None,\n    antialias: bool = False,\n) -> Tensor:  # noqa: F811\n    pass\n\ndef interpolate(input: Tensor, size: Optional[int] = None, scale_factor: Optional[List[float]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False) -> Tensor:  # noqa: F811,B950\n    r\"\"\"Down/up samples the input.\n\n    Tensor interpolated to either the given :attr:`size` or the given\n    :attr:`scale_factor`\n\n    The algorithm used for interpolation is determined by :attr:`mode`.\n\n    Currently temporal, spatial and volumetric sampling are supported, i.e.\n    expected inputs are 3-D, 4-D or 5-D in shape.\n\n    The input dimensions are interpreted in the form:\n    `mini-batch x channels x [optional depth] x [optional height] x width`.\n\n    The modes available for resizing are: `nearest`, `linear` (3D-only),\n    `bilinear`, `bicubic` (4D-only), `trilinear` (5D-only), `area`, `nearest-exact`\n\n    Args:\n        input (Tensor): the input tensor\n        size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]):\n            output spatial size.\n        scale_factor (float or Tuple[float]): multiplier for spatial size. If `scale_factor` is a tuple,\n            its length has to match the number of spatial dimensions; `input.dim() - 2`.\n        mode (str): algorithm used for upsampling:\n            ``'nearest'`` | ``'linear'`` | ``'bilinear'`` | ``'bicubic'`` |\n            ``'trilinear'`` | ``'area'`` | ``'nearest-exact'``. Default: ``'nearest'``\n        align_corners (bool, optional): Geometrically, we consider the pixels of the\n            input and output as squares rather than points.\n            If set to ``True``, the input and output tensors are aligned by the\n            center points of their corner pixels, preserving the values at the corner pixels.\n            If set to ``False``, the input and output tensors are aligned by the corner\n            points of their corner pixels, and the interpolation uses edge value padding\n            for out-of-boundary values, making this operation *independent* of input size\n            when :attr:`scale_factor` is kept the same. This only has an effect when :attr:`mode`\n            is ``'linear'``, ``'bilinear'``, ``'bicubic'`` or ``'trilinear'``.\n            Default: ``False``\n        recompute_scale_factor (bool, optional): recompute the scale_factor for use in the\n            interpolation calculation. If `recompute_scale_factor` is ``True``, then\n            `scale_factor` must be passed in and `scale_factor` is used to compute the\n            output `size`. The computed output `size` will be used to infer new scales for\n            the interpolation. Note that when `scale_factor` is floating-point, it may differ\n            from the recomputed `scale_factor` due to rounding and precision issues.\n            If `recompute_scale_factor` is ``False``, then `size` or `scale_factor` will\n            be used directly for interpolation. Default: ``None``.\n        antialias (bool, optional): flag to apply anti-aliasing. Default: ``False``. Using anti-alias\n            option together with ``align_corners=False``, interpolation result would match Pillow\n            result for downsampling operation. Supported modes: ``'bilinear'``, ``'bicubic'``.\n\n    .. note::\n        With ``mode='bicubic'``, it's possible to cause overshoot, in other words it can produce\n        negative values or values greater than 255 for images.\n        Explicitly call ``result.clamp(min=0, max=255)`` if you want to reduce the overshoot\n        when displaying the image.\n\n    .. note::\n        Mode ``mode='nearest-exact'`` matches Scikit-Image and PIL nearest neighbours interpolation\n        algorithms and fixes known issues with ``mode='nearest'``. This mode is introduced to keep\n        backward compatibility.\n        Mode ``mode='nearest'`` matches buggy OpenCV's ``INTER_NEAREST`` interpolation algorithm.\n\n    .. note::\n        The gradients for the dtype ``float16`` on CUDA may be inaccurate in the upsample operation\n        when using modes ``['linear', 'bilinear', 'bicubic', 'trilinear', 'area']``.\n        For more details, please refer to the discussion in\n        `issue#104157 <https://github.com/pytorch/pytorch/issues/104157>`_.\n\n    Note:\n        {backward_reproducibility_note}\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            interpolate,\n            (input,),\n            input,\n            size=size,\n            scale_factor=scale_factor,\n            mode=mode,\n            align_corners=align_corners,\n            recompute_scale_factor=recompute_scale_factor,\n            antialias=antialias\n        )\n\n    if mode in (\"nearest\", \"area\", \"nearest-exact\"):\n        if align_corners is not None:\n            raise ValueError(\n                \"align_corners option can only be set with the \"\n                \"interpolating modes: linear | bilinear | bicubic | trilinear\"\n            )\n    else:\n        if align_corners is None:\n            align_corners = False\n\n    dim = input.dim() - 2  # Number of spatial dimensions.\n\n    # Process size and scale_factor.  Validate that exactly one is set.\n    # Validate its length if it is a list, or expand it if it is a scalar.\n    # After this block, exactly one of output_size and scale_factors will\n    # be non-None, and it will be a list (or tuple).\n    if size is not None and scale_factor is not None:\n        raise ValueError(\"only one of size or scale_factor should be defined\")\n    elif size is not None:\n        assert scale_factor is None\n        scale_factors = None\n        if isinstance(size, (list, tuple)):\n            if len(size) != dim:\n                raise ValueError(\n                    \"Input and output must have the same number of spatial dimensions, but got \"\n                    f\"input with spatial dimensions of {list(input.shape[2:])} and output size of {size}. \"\n                    \"Please provide input tensor in (N, C, d1, d2, ...,dK) format and \"\n                    \"output size in (o1, o2, ...,oK) format.\"\n                )\n            if not torch.jit.is_scripting():\n                if not all(_is_integer(x) for x in size):\n                    raise TypeError(\n                        \"expected size to be one of int or Tuple[int] or Tuple[int, int] or \"\n                        f\"Tuple[int, int, int], but got size with types {[type(x) for x in size]}\"\n                    )\n            output_size = size\n        else:\n            output_size = [size for _ in range(dim)]\n    elif scale_factor is not None:\n        assert size is None\n        output_size = None\n        if isinstance(scale_factor, (list, tuple)):\n            if len(scale_factor) != dim:\n                raise ValueError(\n                    \"Input and scale_factor must have the same number of spatial dimensions, but \"\n                    f\"got input with spatial dimensions of {list(input.shape[2:])} and \"\n                    f\"scale_factor of shape {scale_factor}. \"\n                    \"Please provide input tensor in (N, C, d1, d2, ...,dK) format and \"\n                    \"scale_factor in (s1, s2, ...,sK) format.\"\n                )\n            scale_factors = scale_factor\n        else:\n            scale_factors = [scale_factor for _ in range(dim)]\n    else:\n        raise ValueError(\"either size or scale_factor should be defined\")\n\n    if recompute_scale_factor is not None and recompute_scale_factor and size is not None:\n        raise ValueError(\"recompute_scale_factor is not meaningful with an explicit size.\")\n\n    # \"area\" mode always requires an explicit size rather than scale factor.\n    # Re-use the recompute_scale_factor code path.\n    if mode == \"area\" and output_size is None:\n        recompute_scale_factor = True\n\n    if recompute_scale_factor is not None and recompute_scale_factor:\n        # We compute output_size here, then un-set scale_factors.\n        # The C++ code will recompute it based on the (integer) output size.\n        assert scale_factors is not None\n        if not torch.jit.is_scripting() and torch._C._get_tracing_state():\n            # make scale_factor a tensor in tracing so constant doesn't get baked in\n            output_size = [\n                (torch.floor((input.size(i + 2).float() * torch.tensor(scale_factors[i], dtype=torch.float32)).float()))\n                for i in range(dim)\n            ]\n        elif torch.jit.is_scripting():\n            output_size = [int(math.floor(float(input.size(i + 2)) * scale_factors[i]))\n                           for i in range(dim)]\n        else:\n            output_size = [\n                _sym_int(input.size(i + 2) * scale_factors[i])\n                for i in range(dim)\n            ]\n        scale_factors = None\n\n    if antialias and not (mode in (\"bilinear\", \"bicubic\") and input.ndim == 4):\n        raise ValueError(\"Anti-alias option is restricted to bilinear and bicubic modes and requires a 4-D tensor as input\")\n\n    if input.dim() == 3 and mode == \"nearest\":\n        return torch._C._nn.upsample_nearest1d(input, output_size, scale_factors)\n    if input.dim() == 4 and mode == \"nearest\":\n        return torch._C._nn.upsample_nearest2d(input, output_size, scale_factors)\n    if input.dim() == 5 and mode == \"nearest\":\n        return torch._C._nn.upsample_nearest3d(input, output_size, scale_factors)\n\n    if input.dim() == 3 and mode == \"nearest-exact\":\n        return torch._C._nn._upsample_nearest_exact1d(input, output_size, scale_factors)\n    if input.dim() == 4 and mode == \"nearest-exact\":\n        return torch._C._nn._upsample_nearest_exact2d(input, output_size, scale_factors)\n    if input.dim() == 5 and mode == \"nearest-exact\":\n        return torch._C._nn._upsample_nearest_exact3d(input, output_size, scale_factors)\n\n    if input.dim() == 3 and mode == \"area\":\n        assert output_size is not None\n        return adaptive_avg_pool1d(input, output_size)\n    if input.dim() == 4 and mode == \"area\":\n        assert output_size is not None\n        return adaptive_avg_pool2d(input, output_size)\n    if input.dim() == 5 and mode == \"area\":\n        assert output_size is not None\n        return adaptive_avg_pool3d(input, output_size)\n\n    if input.dim() == 3 and mode == \"linear\":\n        assert align_corners is not None\n        return torch._C._nn.upsample_linear1d(input, output_size, align_corners, scale_factors)\n    if input.dim() == 4 and mode == \"bilinear\":\n        assert align_corners is not None\n        if antialias:\n            return torch._C._nn._upsample_bilinear2d_aa(input, output_size, align_corners, scale_factors)\n        # Two levels are necessary to prevent TorchScript from touching\n        # are_deterministic_algorithms_enabled.\n        if not torch.jit.is_scripting():\n            if torch.are_deterministic_algorithms_enabled() and input.is_cuda:\n                # Use slow decomp whose backward will be in terms of index_put\n                # importlib is required because the import cannot be top level\n                # (cycle) and cannot be nested (TS doesn't support)\n                return importlib.import_module('torch._decomp.decompositions').upsample_bilinear2d_vec(\n                    input, output_size, align_corners, scale_factors)\n        return torch._C._nn.upsample_bilinear2d(input, output_size, align_corners, scale_factors)\n    if input.dim() == 5 and mode == \"trilinear\":\n        assert align_corners is not None\n        return torch._C._nn.upsample_trilinear3d(input, output_size, align_corners, scale_factors)\n    if input.dim() == 4 and mode == \"bicubic\":\n        assert align_corners is not None\n        if antialias:\n            return torch._C._nn._upsample_bicubic2d_aa(input, output_size, align_corners, scale_factors)\n        return torch._C._nn.upsample_bicubic2d(input, output_size, align_corners, scale_factors)\n\n    if input.dim() == 3 and mode == \"bilinear\":\n        raise NotImplementedError(\"Got 3D input, but bilinear mode needs 4D input\")\n    if input.dim() == 3 and mode == \"trilinear\":\n        raise NotImplementedError(\"Got 3D input, but trilinear mode needs 5D input\")\n    if input.dim() == 4 and mode == \"linear\":\n        raise NotImplementedError(\"Got 4D input, but linear mode needs 3D input\")\n    if input.dim() == 4 and mode == \"trilinear\":\n        raise NotImplementedError(\"Got 4D input, but trilinear mode needs 5D input\")\n    if input.dim() == 5 and mode == \"linear\":\n        raise NotImplementedError(\"Got 5D input, but linear mode needs 3D input\")\n    if input.dim() == 5 and mode == \"bilinear\":\n        raise NotImplementedError(\"Got 5D input, but bilinear mode needs 4D input\")\n\n    raise NotImplementedError(\n        \"Input Error: Only 3D, 4D and 5D input Tensors supported\"\n        f\" (got {input.dim()}D) for the modes: nearest | linear | bilinear | bicubic | trilinear | area | nearest-exact\"\n        f\" (got {mode})\"\n    )\n\n\nif interpolate.__doc__:\n    interpolate.__doc__ = interpolate.__doc__.format(**reproducibility_notes)\n\n\n@_overload  # noqa: F811\ndef upsample_nearest(input: Tensor, size: Optional[int] = None, scale_factor: Optional[float] = None) -> Tensor:  # noqa: F811\n    pass\n\n\n@_overload  # noqa: F811\ndef upsample_nearest(input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[float] = None) -> Tensor:  # noqa: F811\n    pass\n\n\ndef upsample_nearest(input, size=None, scale_factor=None):  # noqa: F811\n    r\"\"\"Upsamples the input, using nearest neighbours' pixel values.\n\n    .. warning::\n        This function is deprecated in favor of :func:`torch.nn.functional.interpolate`.\n        This is equivalent with ``nn.functional.interpolate(..., mode='nearest')``.\n\n    Currently spatial and volumetric upsampling are supported (i.e. expected\n    inputs are 4 or 5 dimensional).\n\n    Args:\n        input (Tensor): input\n        size (int or Tuple[int, int] or Tuple[int, int, int]): output spatia\n            size.\n        scale_factor (int): multiplier for spatial size. Has to be an integer.\n\n    Note:\n        {backward_reproducibility_note}\n    \"\"\"\n    # DeprecationWarning is ignored by default\n    warnings.warn(\"nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.\")\n    return interpolate(input, size, scale_factor, mode=\"nearest\")\n\n\nif upsample_nearest.__doc__:\n    upsample_nearest.__doc__ = upsample_nearest.__doc__.format(**reproducibility_notes)\n\n\n@_overload  # noqa: F811\ndef upsample_bilinear(\n    input: Tensor, size: Optional[int] = None, scale_factor: Optional[float] = None\n) -> Tensor:  # noqa: F811\n    pass\n\n\n@_overload  # noqa: F811\ndef upsample_bilinear(  # noqa: F811\n    input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[float] = None\n) -> Tensor:  # noqa: F811\n    pass\n\n\n@_overload  # noqa: F811\ndef upsample_bilinear(  # noqa: F811\n    input: Tensor, size: Optional[int] = None, scale_factor: Optional[List[float]] = None\n) -> Tensor:  # noqa: F811\n    pass\n\n\n@_overload  # noqa: F811\ndef upsample_bilinear(  # noqa: F811\n    input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[List[float]] = None\n) -> Tensor:  # noqa: F811\n    pass\n\n\ndef upsample_bilinear(input, size=None, scale_factor=None):  # noqa: F811\n    r\"\"\"Upsamples the input, using bilinear upsampling.\n\n    .. warning::\n        This function is deprecated in favor of :func:`torch.nn.functional.interpolate`.\n        This is equivalent with\n        ``nn.functional.interpolate(..., mode='bilinear', align_corners=True)``.\n\n    Expected inputs are spatial (4 dimensional). Use `upsample_trilinear` fo\n    volumetric (5 dimensional) inputs.\n\n    Args:\n        input (Tensor): input\n        size (int or Tuple[int, int]): output spatial size.\n        scale_factor (int or Tuple[int, int]): multiplier for spatial size\n\n    Note:\n        {backward_reproducibility_note}\n    \"\"\"\n    # DeprecationWarning is ignored by default\n    warnings.warn(\"nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\")\n    return interpolate(input, size, scale_factor, mode=\"bilinear\", align_corners=True)\n\n\nif upsample_bilinear.__doc__:\n    upsample_bilinear.__doc__ = upsample_bilinear.__doc__.format(**reproducibility_notes)\n\nGRID_SAMPLE_INTERPOLATION_MODES = {\n    \"bilinear\": 0,\n    \"nearest\": 1,\n    \"bicubic\": 2,\n}\n\nGRID_SAMPLE_PADDING_MODES = {\n    \"zeros\": 0,\n    \"border\": 1,\n    \"reflection\": 2,\n}\n\n\ndef grid_sample(\n    input: Tensor,\n    grid: Tensor,\n    mode: str = \"bilinear\",\n    padding_mode: str = \"zeros\",\n    align_corners: Optional[bool] = None,\n) -> Tensor:\n    r\"\"\"Compute grid sample.\n\n    Given an :attr:`input` and a flow-field :attr:`grid`, computes the\n    ``output`` using :attr:`input` values and pixel locations from :attr:`grid`.\n\n    Currently, only spatial (4-D) and volumetric (5-D) :attr:`input` are\n    supported.\n\n    In the spatial (4-D) case, for :attr:`input` with shape\n    :math:`(N, C, H_\\text{in}, W_\\text{in})` and :attr:`grid` with shape\n    :math:`(N, H_\\text{out}, W_\\text{out}, 2)`, the output will have shape\n    :math:`(N, C, H_\\text{out}, W_\\text{out})`.\n\n    For each output location ``output[n, :, h, w]``, the size-2 vector\n    ``grid[n, h, w]`` specifies :attr:`input` pixel locations ``x`` and ``y``,\n    which are used to interpolate the output value ``output[n, :, h, w]``.\n    In the case of 5D inputs, ``grid[n, d, h, w]`` specifies the\n    ``x``, ``y``, ``z`` pixel locations for interpolating\n    ``output[n, :, d, h, w]``. :attr:`mode` argument specifies ``nearest`` or\n    ``bilinear`` interpolation method to sample the input pixels.\n\n    :attr:`grid` specifies the sampling pixel locations normalized by the\n    :attr:`input` spatial dimensions. Therefore, it should have most values in\n    the range of ``[-1, 1]``. For example, values ``x = -1, y = -1`` is the\n    left-top pixel of :attr:`input`, and values  ``x = 1, y = 1`` is the\n    right-bottom pixel of :attr:`input`.\n\n    If :attr:`grid` has values outside the range of ``[-1, 1]``, the corresponding\n    outputs are handled as defined by :attr:`padding_mode`. Options are\n\n        * ``padding_mode=\"zeros\"``: use ``0`` for out-of-bound grid locations,\n        * ``padding_mode=\"border\"``: use border values for out-of-bound grid locations,\n        * ``padding_mode=\"reflection\"``: use values at locations reflected by\n          the border for out-of-bound grid locations. For location far away\n          from the border, it will keep being reflected until becoming in bound,\n          e.g., (normalized) pixel location ``x = -3.5`` reflects by border ``-1``\n          and becomes ``x' = 1.5``, then reflects by border ``1`` and becomes\n          ``x'' = -0.5``.\n\n    Note:\n        This function is often used in conjunction with :func:`affine_grid`\n        to build `Spatial Transformer Networks`_ .\n\n    Note:\n        When using the CUDA backend, this operation may induce nondeterministic\n        behaviour in its backward pass that is not easily switched off.\n        Please see the notes on :doc:`/notes/randomness` for background.\n\n    Note:\n        NaN values in :attr:`grid` would be interpreted as ``-1``.\n\n    Args:\n        input (Tensor): input of shape :math:`(N, C, H_\\text{in}, W_\\text{in})` (4-D case)\n                        or :math:`(N, C, D_\\text{in}, H_\\text{in}, W_\\text{in})` (5-D case)\n        grid (Tensor): flow-field of shape :math:`(N, H_\\text{out}, W_\\text{out}, 2)` (4-D case)\n                       or :math:`(N, D_\\text{out}, H_\\text{out}, W_\\text{out}, 3)` (5-D case)\n        mode (str): interpolation mode to calculate output values\n            ``'bilinear'`` | ``'nearest'`` | ``'bicubic'``. Default: ``'bilinear'``\n            Note: ``mode='bicubic'`` supports only 4-D input.\n            When ``mode='bilinear'`` and the input is 5-D, the interpolation mode\n            used internally will actually be trilinear. However, when the input is 4-D,\n            the interpolation mode will legitimately be bilinear.\n        padding_mode (str): padding mode for outside grid values\n            ``'zeros'`` | ``'border'`` | ``'reflection'``. Default: ``'zeros'``\n        align_corners (bool, optional): Geometrically, we consider the pixels of the\n            input  as squares rather than points.\n            If set to ``True``, the extrema (``-1`` and ``1``) are considered as referring\n            to the center points of the input's corner pixels. If set to ``False``, they\n            are instead considered as referring to the corner points of the input's corner\n            pixels, making the sampling more resolution agnostic.\n            This option parallels the ``align_corners`` option in\n            :func:`interpolate`, and so whichever option is used here\n            should also be used there to resize the input image before grid sampling.\n            Default: ``False``\n\n    Returns:\n        output (Tensor): output Tensor\n\n    .. _`Spatial Transformer Networks`:\n        https://arxiv.org/abs/1506.02025\n\n    .. warning::\n        When ``align_corners = True``, the grid positions depend on the pixel\n        size relative to the input image size, and so the locations sampled by\n        :func:`grid_sample` will differ for the same input given at different\n        resolutions (that is, after being upsampled or downsampled).\n        The default behavior up to version 1.2.0 was ``align_corners = True``.\n        Since then, the default behavior has been changed to ``align_corners = False``,\n        in order to bring it in line with the default for :func:`interpolate`.\n\n    .. note::\n        ``mode='bicubic'`` is implemented using the `cubic convolution algorithm`_ with :math:`\\alpha=-0.75`.\n        The constant :math:`\\alpha` might be different from packages to packages.\n        For example, `PIL`_ and `OpenCV`_ use -0.5 and -0.75 respectively.\n        This algorithm may \"overshoot\" the range of values it's interpolating.\n        For example, it may produce negative values or values greater than 255 when interpolating input in [0, 255].\n        Clamp the results with :func:`torch.clamp` to ensure they are within the valid range.\n    .. _`cubic convolution algorithm`: https://en.wikipedia.org/wiki/Bicubic_interpolation\n    .. _`PIL`: https://github.com/python-pillow/Pillow/blob/4634eafe3c695a014267eefdce830b4a825beed7/src/libImaging/Resample.c#L51\n    .. _`OpenCV`: https://github.com/opencv/opencv/blob/f345ed564a06178670750bad59526cfa4033be55/modules/imgproc/src/resize.cpp#L908\n    \"\"\"\n    if has_torch_function_variadic(input, grid):\n        return handle_torch_function(\n            grid_sample, (input, grid), input, grid, mode=mode, padding_mode=padding_mode, align_corners=align_corners\n        )\n    if mode != \"bilinear\" and mode != \"nearest\" and mode != \"bicubic\":\n        raise ValueError(\n            f\"nn.functional.grid_sample(): expected mode to be 'bilinear', 'nearest' or 'bicubic', but got: '{mode}'\"\n        )\n    if padding_mode != \"zeros\" and padding_mode != \"border\" and padding_mode != \"reflection\":\n        raise ValueError(\n            \"nn.functional.grid_sample(): expected padding_mode \"\n            \"to be 'zeros', 'border', or 'reflection', \"\n            f\"but got: '{padding_mode}'\"\n        )\n\n    if mode == \"bilinear\":\n        mode_enum = 0\n    elif mode == \"nearest\":\n        mode_enum = 1\n    else:  # mode == 'bicubic'\n        mode_enum = 2\n\n    if padding_mode == \"zeros\":\n        padding_mode_enum = 0\n    elif padding_mode == \"border\":\n        padding_mode_enum = 1\n    else:  # padding_mode == 'reflection'\n        padding_mode_enum = 2\n\n    if align_corners is None:\n        warnings.warn(\n            \"Default grid_sample and affine_grid behavior has changed \"\n            \"to align_corners=False since 1.3.0. Please specify \"\n            \"align_corners=True if the old behavior is desired. \"\n            \"See the documentation of grid_sample for details.\"\n        )\n        align_corners = False\n\n    return torch.grid_sampler(input, grid, mode_enum, padding_mode_enum, align_corners)\n\n\ndef affine_grid(theta: Tensor, size: List[int], align_corners: Optional[bool] = None) -> Tensor:\n    r\"\"\"Generate 2D or 3D flow field (sampling grid), given a batch of affine matrices :attr:`theta`.\n\n    .. note::\n        This function is often used in conjunction with :func:`grid_sample`\n        to build `Spatial Transformer Networks`_ .\n\n    Args:\n        theta (Tensor): input batch of affine matrices with shape\n            (:math:`N \\times 2 \\times 3`) for 2D or\n            (:math:`N \\times 3 \\times 4`) for 3D\n        size (torch.Size): the target output image size.\n            (:math:`N \\times C \\times H \\times W` for 2D or\n            :math:`N \\times C \\times D \\times H \\times W` for 3D)\n            Example: torch.Size((32, 3, 24, 24))\n        align_corners (bool, optional): if ``True``, consider ``-1`` and ``1``\n            to refer to the centers of the corner pixels rather than the image corners.\n            Refer to :func:`grid_sample` for a more complete description.\n            A grid generated by :func:`affine_grid` should be passed to :func:`grid_sample`\n            with the same setting for this option.\n            Default: ``False``\n\n    Returns:\n        output (Tensor): output Tensor of size (:math:`N \\times H \\times W \\times 2`)\n\n    .. _`Spatial Transformer Networks`:\n        https://arxiv.org/abs/1506.02025\n\n    .. warning::\n        When ``align_corners = True``, the grid positions depend on the pixel\n        size relative to the input image size, and so the locations sampled by\n        :func:`grid_sample` will differ for the same input given at different\n        resolutions (that is, after being upsampled or downsampled).\n        The default behavior up to version 1.2.0 was ``align_corners = True``.\n        Since then, the default behavior has been changed to ``align_corners = False``,\n        in order to bring it in line with the default for :func:`interpolate`.\n    .. warning::\n        When ``align_corners = True``, 2D affine transforms on 1D data and\n        3D affine transforms on 2D data (that is, when one of the spatial\n        dimensions has unit size) are ill-defined, and not an intended use case.\n        This is not a problem when ``align_corners = False``.\n        Up to version 1.2.0, all grid points along a unit dimension were\n        considered arbitrarily to be at ``-1``.\n        From version 1.3.0, under ``align_corners = True`` all grid points\n        along a unit dimension are considered to be at ``0``\n        (the center of the input image).\n    \"\"\"\n    if has_torch_function_unary(theta):\n        return handle_torch_function(affine_grid, (theta,), theta, size, align_corners=align_corners)\n    if align_corners is None:\n        warnings.warn(\n            \"Default grid_sample and affine_grid behavior has changed \"\n            \"to align_corners=False since 1.3.0. Please specify \"\n            \"align_corners=True if the old behavior is desired. \"\n            \"See the documentation of grid_sample for details.\"\n        )\n        align_corners = False\n\n    # enforce floating point dtype on theta\n    if not theta.is_floating_point():\n        raise ValueError(f\"Expected theta to have floating point type, but got {theta.dtype}\")\n    # check that shapes and sizes match\n    if len(size) == 4:\n        if theta.dim() != 3 or theta.shape[-2] != 2 or theta.shape[-1] != 3:\n            raise ValueError(\n                f\"Expected a batch of 2D affine matrices of shape Nx2x3 for size {size}. Got {theta.shape}.\"\n            )\n        spatial_size = size[-2:]  # spatial dimension sizes\n    elif len(size) == 5:\n        if theta.dim() != 3 or theta.shape[-2] != 3 or theta.shape[-1] != 4:\n            raise ValueError(\n                f\"Expected a batch of 3D affine matrices of shape Nx3x4 for size {size}. Got {theta.shape}.\"\n            )\n        spatial_size = size[-3:]  # spatial dimension sizes\n    else:\n        raise NotImplementedError(\n            \"affine_grid only supports 4D and 5D sizes, \"\n            \"for 2D and 3D affine transforms, respectively. \"\n            f\"Got size {size}.\"\n        )\n    # check for empty span\n    if align_corners and min(spatial_size) == 1:\n        warnings.warn(\n            \"Since version 1.3.0, affine_grid behavior has changed \"\n            \"for unit-size grids when align_corners=True. \"\n            \"This is not an intended use case of affine_grid. \"\n            \"See the documentation of affine_grid for details.\"\n        )\n    elif min(size) <= 0:\n        raise ValueError(f\"Expected non-zero, positive output size. Got {size}\")\n\n    return torch.affine_grid_generator(theta, size, align_corners)\n\n\ndef pad(input: Tensor, pad: List[int], mode: str = \"constant\", value: Optional[float] = None) -> Tensor:\n    r\"\"\"\npad(input, pad, mode=\"constant\", value=None) -> Tensor\n\nPads tensor.\n\nPadding size:\n    The padding size by which to pad some dimensions of :attr:`input`\n    are described starting from the last dimension and moving forward.\n    :math:`\\left\\lfloor\\frac{\\text{len(pad)}}{2}\\right\\rfloor` dimensions\n    of ``input`` will be padded.\n    For example, to pad only the last dimension of the input tensor, then\n    :attr:`pad` has the form\n    :math:`(\\text{padding\\_left}, \\text{padding\\_right})`;\n    to pad the last 2 dimensions of the input tensor, then use\n    :math:`(\\text{padding\\_left}, \\text{padding\\_right},`\n    :math:`\\text{padding\\_top}, \\text{padding\\_bottom})`;\n    to pad the last 3 dimensions, use\n    :math:`(\\text{padding\\_left}, \\text{padding\\_right},`\n    :math:`\\text{padding\\_top}, \\text{padding\\_bottom}`\n    :math:`\\text{padding\\_front}, \\text{padding\\_back})`.\n\nPadding mode:\n    See :class:`torch.nn.CircularPad2d`, :class:`torch.nn.ConstantPad2d`,\n    :class:`torch.nn.ReflectionPad2d`, and :class:`torch.nn.ReplicationPad2d`\n    for concrete examples on how each of the padding modes works. Constant\n    padding is implemented for arbitrary dimensions. Circular, replicate and\n    reflection padding are implemented for padding the last 3 dimensions of a\n    4D or 5D input tensor, the last 2 dimensions of a 3D or 4D input tensor,\n    or the last dimension of a 2D or 3D input tensor.\n\nNote:\n    When using the CUDA backend, this operation may induce nondeterministic\n    behaviour in its backward pass that is not easily switched off.\n    Please see the notes on :doc:`/notes/randomness` for background.\n\nArgs:\n    input (Tensor): N-dimensional tensor\n    pad (tuple): m-elements tuple, where\n        :math:`\\frac{m}{2} \\leq` input dimensions and :math:`m` is even.\n    mode: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.\n        Default: ``'constant'``\n    value: fill value for ``'constant'`` padding. Default: ``0``\n\nExamples::\n\n    >>> t4d = torch.empty(3, 3, 4, 2)\n    >>> p1d = (1, 1) # pad last dim by 1 on each side\n    >>> out = F.pad(t4d, p1d, \"constant\", 0)  # effectively zero padding\n    >>> print(out.size())\n    torch.Size([3, 3, 4, 4])\n    >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)\n    >>> out = F.pad(t4d, p2d, \"constant\", 0)\n    >>> print(out.size())\n    torch.Size([3, 3, 8, 4])\n    >>> t4d = torch.empty(3, 3, 4, 2)\n    >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)\n    >>> out = F.pad(t4d, p3d, \"constant\", 0)\n    >>> print(out.size())\n    torch.Size([3, 9, 7, 3])\n\n\"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            torch.nn.functional.pad, (input,), input, pad, mode=mode, value=value)\n    if not torch.jit.is_scripting():\n        if torch.are_deterministic_algorithms_enabled() and input.is_cuda:\n            if len(pad) == 4 and (input.dim() == 3 or input.dim() == 4) and mode == 'replicate':\n                # Use slow decomp whose backward will be in terms of index_put.\n                # importlib is required because the import cannot be top level\n                # (cycle) and cannot be nested (TS doesn't support)\n                return importlib.import_module('torch._decomp.decompositions').replication_pad2d(\n                    input, pad\n                )\n    return torch._C._nn.pad(input, pad, mode, value)\n\n# TODO: Fix via https://github.com/pytorch/pytorch/issues/75798\npad.__module__ = \"torch.nn.functional\"\n\n# distance\n\n\npairwise_distance = _add_docstr(\n    torch.pairwise_distance,\n    r\"\"\"\npairwise_distance(x1, x2, p=2.0, eps=1e-6, keepdim=False) -> Tensor\n\nSee :class:`torch.nn.PairwiseDistance` for details\n\"\"\")\n\n\npdist = _add_docstr(\n    torch.pdist,\n    r\"\"\"\npdist(input, p=2) -> Tensor\n\nComputes the p-norm distance between every pair of row vectors in the input.\nThis is identical to the upper triangular portion, excluding the diagonal, of\n`torch.norm(input[:, None] - input, dim=2, p=p)`. This function will be faster\nif the rows are contiguous.\n\nIf input has shape :math:`N \\times M` then the output will have shape\n:math:`\\frac{1}{2} N (N - 1)`.\n\nThis function is equivalent to ``scipy.spatial.distance.pdist(input,\n'minkowski', p=p)`` if :math:`p \\in (0, \\infty)`. When :math:`p = 0` it is\nequivalent to ``scipy.spatial.distance.pdist(input, 'hamming') * M``.\nWhen :math:`p = \\infty`, the closest scipy function is\n``scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max())``.\n\nArgs:\n    input: input tensor of shape :math:`N \\times M`.\n    p: p value for the p-norm distance to calculate between each vector pair\n        :math:`\\in [0, \\infty]`.\n\"\"\",\n)\n\n\ncosine_similarity = _add_docstr(\n    torch.cosine_similarity,\n    r\"\"\"\ncosine_similarity(x1, x2, dim=1, eps=1e-8) -> Tensor\n\nReturns cosine similarity between ``x1`` and ``x2``, computed along dim. ``x1`` and ``x2`` must be broadcastable\nto a common shape. ``dim`` refers to the dimension in this common shape. Dimension ``dim`` of the output is\nsqueezed (see :func:`torch.squeeze`), resulting in the\noutput tensor having 1 fewer dimension.\n\n.. math ::\n    \\text{similarity} = \\dfrac{x_1 \\cdot x_2}{\\max(\\Vert x_1 \\Vert _2, \\epsilon) \\cdot \\max(\\Vert x_2 \\Vert _2, \\epsilon)}\n\nSupports :ref:`type promotion <type-promotion-doc>`.\n\nArgs:\n    x1 (Tensor): First input.\n    x2 (Tensor): Second input.\n    dim (int, optional): Dimension along which cosine similarity is computed. Default: 1\n    eps (float, optional): Small value to avoid division by zero.\n        Default: 1e-8\n\nExample::\n\n    >>> input1 = torch.randn(100, 128)\n    >>> input2 = torch.randn(100, 128)\n    >>> output = F.cosine_similarity(input1, input2)\n    >>> print(output)\n\"\"\",\n)\n\n\none_hot = _add_docstr(\n    torch._C._nn.one_hot,\n    r\"\"\"\none_hot(tensor, num_classes=-1) -> LongTensor\n\nTakes LongTensor with index values of shape ``(*)`` and returns a tensor\nof shape ``(*, num_classes)`` that have zeros everywhere except where the\nindex of last dimension matches the corresponding value of the input tensor,\nin which case it will be 1.\n\nSee also `One-hot on Wikipedia`_ .\n\n.. _One-hot on Wikipedia:\n    https://en.wikipedia.org/wiki/One-hot\n\nArguments:\n    tensor (LongTensor): class values of any shape.\n    num_classes (int):  Total number of classes. If set to -1, the number\n        of classes will be inferred as one greater than the largest class\n        value in the input tensor.\n\nReturns:\n    LongTensor that has one more dimension with 1 values at the\n    index of last dimension indicated by the input, and 0 everywhere\n    else.\n\nExamples:\n    >>> F.one_hot(torch.arange(0, 5) % 3)\n    tensor([[1, 0, 0],\n            [0, 1, 0],\n            [0, 0, 1],\n            [1, 0, 0],\n            [0, 1, 0]])\n    >>> F.one_hot(torch.arange(0, 5) % 3, num_classes=5)\n    tensor([[1, 0, 0, 0, 0],\n            [0, 1, 0, 0, 0],\n            [0, 0, 1, 0, 0],\n            [1, 0, 0, 0, 0],\n            [0, 1, 0, 0, 0]])\n    >>> F.one_hot(torch.arange(0, 6).view(3,2) % 3)\n    tensor([[[1, 0, 0],\n             [0, 1, 0]],\n            [[0, 0, 1],\n             [1, 0, 0]],\n            [[0, 1, 0],\n             [0, 0, 1]]])\n\"\"\",\n)\n\n\ndef triplet_margin_loss(\n    anchor: Tensor,\n    positive: Tensor,\n    negative: Tensor,\n    margin: float = 1.0,\n    p: float = 2,\n    eps: float = 1e-6,\n    swap: bool = False,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = \"mean\",\n) -> Tensor:\n    r\"\"\"Compute the triplet loss between given input tensors and a margin greater than 0.\n\n    See :class:`~torch.nn.TripletMarginLoss` for details.\n    \"\"\"\n    if has_torch_function_variadic(anchor, positive, negative):\n        return handle_torch_function(\n            triplet_margin_loss,\n            (anchor, positive, negative),\n            anchor,\n            positive,\n            negative,\n            margin=margin,\n            p=p,\n            eps=eps,\n            swap=swap,\n            size_average=size_average,\n            reduce=reduce,\n            reduction=reduction,\n        )\n    if size_average is not None or reduce is not None:\n        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n    else:\n        reduction_enum = _Reduction.get_enum(reduction)\n    return torch.triplet_margin_loss(anchor, positive, negative, margin, p, eps, swap, reduction_enum)\n\n\ndef triplet_margin_with_distance_loss(\n    anchor: Tensor,\n    positive: Tensor,\n    negative: Tensor,\n    *,\n    distance_function: Optional[Callable[[Tensor, Tensor], Tensor]] = None,\n    margin: float = 1.0,\n    swap: bool = False,\n    reduction: str = \"mean\"\n) -> Tensor:\n    r\"\"\"Compute the triplet margin loss for input tensors using a custom distance function.\n\n    See :class:`~torch.nn.TripletMarginWithDistanceLoss` for details.\n    \"\"\"\n    if torch.jit.is_scripting():\n        raise NotImplementedError(\n            \"F.triplet_margin_with_distance_loss does not support JIT scripting: \"\n            \"functions requiring Callables cannot be scripted.\"\n        )\n\n    if has_torch_function_variadic(anchor, positive, negative):\n        return handle_torch_function(\n            triplet_margin_with_distance_loss,\n            (anchor, positive, negative),\n            anchor,\n            positive,\n            negative,\n            distance_function=distance_function,\n            margin=margin,\n            swap=swap,\n            reduction=reduction,\n        )\n\n    # Check validity of reduction mode\n    if reduction not in (\"mean\", \"sum\", \"none\"):\n        raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n    # Check dimensions\n    a_dim = anchor.ndim\n    p_dim = positive.ndim\n    n_dim = negative.ndim\n    if not (a_dim == p_dim and p_dim == n_dim):\n        raise RuntimeError(\n            f\"The anchor, positive, and negative tensors are expected to have \"\n            f\"the same number of dimensions, but got: anchor {a_dim}D, \"\n            f\"positive {p_dim}D, and negative {n_dim}D inputs\")\n\n    # Calculate loss\n    if distance_function is None:\n        distance_function = torch.pairwise_distance\n\n    dist_pos = distance_function(anchor, positive)\n    dist_neg = distance_function(anchor, negative)\n    # The distance swap is described in the paper \"Learning shallow\n    # convolutional feature descriptors with triplet losses\" by V. Balntas, E.\n    # Riba et al.  If True, and if the positive example is closer to the\n    # negative example than the anchor is, swaps the positive example and the\n    # anchor in the loss computation.\n    if swap:\n        dist_swap = distance_function(positive, negative)\n        dist_neg = torch.minimum(dist_neg, dist_swap)\n    loss = torch.clamp_min(margin + dist_pos - dist_neg, 0)\n\n    # Apply reduction\n    if reduction == \"sum\":\n        return torch.sum(loss)\n    elif reduction == \"mean\":\n        return torch.mean(loss)\n    else:  # reduction == \"none\"\n        return loss\n\n\ndef normalize(input: Tensor, p: float = 2.0, dim: int = 1, eps: float = 1e-12, out: Optional[Tensor] = None) -> Tensor:\n    r\"\"\"Perform :math:`L_p` normalization of inputs over specified dimension.\n\n    For a tensor :attr:`input` of sizes :math:`(n_0, ..., n_{dim}, ..., n_k)`, each\n    :math:`n_{dim}` -element vector :math:`v` along dimension :attr:`dim` is transformed as\n\n    .. math::\n        v = \\frac{v}{\\max(\\lVert v \\rVert_p, \\epsilon)}.\n\n    With the default arguments it uses the Euclidean norm over vectors along dimension :math:`1` for normalization.\n\n    Args:\n        input: input tensor of any shape\n        p (float): the exponent value in the norm formulation. Default: 2\n        dim (int or tuple of ints): the dimension to reduce. Default: 1\n        eps (float): small value to avoid division by zero. Default: 1e-12\n        out (Tensor, optional): the output tensor. If :attr:`out` is used, this\n                                operation won't be differentiable.\n    \"\"\"\n    if has_torch_function_variadic(input, out):\n        return handle_torch_function(normalize, (input, out), input, p=p, dim=dim, eps=eps, out=out)\n    if out is None:\n        denom = input.norm(p, dim, keepdim=True).clamp_min(eps).expand_as(input)\n        return input / denom\n    else:\n        denom = input.norm(p, dim, keepdim=True).clamp_min_(eps).expand_as(input)\n        return torch.div(input, denom, out=out)\n\n\ndef assert_int_or_pair(arg: List[int], arg_name: str, message: str) -> None:\n    assert isinstance(arg, int) or len(arg) == 2, message.format(arg_name)\n\n\ndef unfold(\n    input: Tensor, kernel_size: BroadcastingList2[int],\n    dilation: BroadcastingList2[int] = 1,\n    padding: BroadcastingList2[int] = 0,\n    stride: BroadcastingList2[int] = 1\n) -> Tensor:\n    r\"\"\"Extract sliding local blocks from a batched input tensor.\n\n    .. warning::\n        Currently, only 4-D input tensors (batched image-like tensors) are\n        supported.\n\n    .. warning::\n\n        More than one element of the unfolded tensor may refer to a single\n        memory location. As a result, in-place operations (especially ones that\n        are vectorized) may result in incorrect behavior. If you need to write\n        to the tensor, please clone it first.\n\n\n    See :class:`torch.nn.Unfold` for details\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            unfold, (input,), input, kernel_size, dilation=dilation, padding=padding, stride=stride\n        )\n    return torch._C._nn.im2col(input, _pair(kernel_size), _pair(dilation), _pair(padding), _pair(stride))\n\n\ndef fold(\n    input: Tensor, output_size: BroadcastingList2[int],\n    kernel_size: BroadcastingList2[int],\n    dilation: BroadcastingList2[int] = 1,\n    padding: BroadcastingList2[int] = 0,\n    stride: BroadcastingList2[int] = 1\n) -> Tensor:\n    r\"\"\"Combine an array of sliding local blocks into a large containing tensor.\n\n    .. warning::\n        Currently, only unbatched (3D) or batched (4D) image-like output tensors are supported.\n\n    See :class:`torch.nn.Fold` for details\n    \"\"\"\n    if has_torch_function_unary(input):\n        return handle_torch_function(\n            fold, (input,), input, output_size, kernel_size, dilation=dilation, padding=padding, stride=stride\n        )\n    return torch._C._nn.col2im(\n        input, _pair(output_size), _pair(kernel_size), _pair(dilation), _pair(padding), _pair(stride)\n    )\n\n#\n# multihead attention\n#\n\ndef _in_projection_packed(\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    w: Tensor,\n    b: Optional[Tensor] = None,\n) -> List[Tensor]:\n    r\"\"\"Perform the in-projection step of the attention operation, using packed weights.\n\n    Output is a triple containing projection tensors for query, key and value.\n\n    Args:\n        q, k, v: query, key and value tensors to be projected. For self-attention,\n            these are typically the same tensor; for encoder-decoder attention,\n            k and v are typically the same tensor. (We take advantage of these\n            identities for performance if they are present.) Regardless, q, k and v\n            must share a common embedding dimension; otherwise their shapes may vary.\n        w: projection weights for q, k and v, packed into a single tensor. Weights\n            are packed along dimension 0, in q, k, v order.\n        b: optional projection biases for q, k and v, packed into a single tensor\n            in q, k, v order.\n\n    Shape:\n        Inputs:\n        - q: :math:`(..., E)` where E is the embedding dimension\n        - k: :math:`(..., E)` where E is the embedding dimension\n        - v: :math:`(..., E)` where E is the embedding dimension\n        - w: :math:`(E * 3, E)` where E is the embedding dimension\n        - b: :math:`E * 3` where E is the embedding dimension\n\n        Output:\n        - in output list :math:`[q', k', v']`, each output tensor will have the\n            same shape as the corresponding input tensor.\n    \"\"\"\n    E = q.size(-1)\n    if k is v:\n        if q is k:\n            # self-attention\n            proj = linear(q, w, b)\n            # reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\n            proj = proj.unflatten(-1, (3, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n            return proj[0], proj[1], proj[2]\n        else:\n            # encoder-decoder attention\n            w_q, w_kv = w.split([E, E * 2])\n            if b is None:\n                b_q = b_kv = None\n            else:\n                b_q, b_kv = b.split([E, E * 2])\n            q_proj = linear(q, w_q, b_q)\n            kv_proj = linear(k, w_kv, b_kv)\n            # reshape to 2, E and not E, 2 is deliberate for better memory coalescing and keeping same order as chunk()\n            kv_proj = kv_proj.unflatten(-1, (2, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n            return (q_proj, kv_proj[0], kv_proj[1])\n    else:\n        w_q, w_k, w_v = w.chunk(3)\n        if b is None:\n            b_q = b_k = b_v = None\n        else:\n            b_q, b_k, b_v = b.chunk(3)\n        return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)\n\n\ndef _in_projection(\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    w_q: Tensor,\n    w_k: Tensor,\n    w_v: Tensor,\n    b_q: Optional[Tensor] = None,\n    b_k: Optional[Tensor] = None,\n    b_v: Optional[Tensor] = None,\n) -> Tuple[Tensor, Tensor, Tensor]:\n    r\"\"\"Perform the in-projection step of the attention operation.\n\n    This is simply a triple of linear projections,\n    with shape constraints on the weights which\n    ensure embedding dimension uniformity in the projected outputs.\n    Output is a triple containing projection tensors for query, key and value.\n\n    Args:\n        q, k, v: query, key and value tensors to be projected.\n        w_q, w_k, w_v: weights for q, k and v, respectively.\n        b_q, b_k, b_v: optional biases for q, k and v, respectively.\n\n    Shape:\n        Inputs:\n        - q: :math:`(Qdims..., Eq)` where Eq is the query embedding dimension and Qdims are any\n            number of leading dimensions.\n        - k: :math:`(Kdims..., Ek)` where Ek is the key embedding dimension and Kdims are any\n            number of leading dimensions.\n        - v: :math:`(Vdims..., Ev)` where Ev is the value embedding dimension and Vdims are any\n            number of leading dimensions.\n        - w_q: :math:`(Eq, Eq)`\n        - w_k: :math:`(Eq, Ek)`\n        - w_v: :math:`(Eq, Ev)`\n        - b_q: :math:`(Eq)`\n        - b_k: :math:`(Eq)`\n        - b_v: :math:`(Eq)`\n\n        Output: in output triple :math:`(q', k', v')`,\n         - q': :math:`[Qdims..., Eq]`\n         - k': :math:`[Kdims..., Eq]`\n         - v': :math:`[Vdims..., Eq]`\n\n    \"\"\"\n    Eq, Ek, Ev = q.size(-1), k.size(-1), v.size(-1)\n    assert w_q.shape == (Eq, Eq), f\"expecting query weights shape of {(Eq, Eq)}, but got {w_q.shape}\"\n    assert w_k.shape == (Eq, Ek), f\"expecting key weights shape of {(Eq, Ek)}, but got {w_k.shape}\"\n    assert w_v.shape == (Eq, Ev), f\"expecting value weights shape of {(Eq, Ev)}, but got {w_v.shape}\"\n    assert b_q is None or b_q.shape == (Eq,), f\"expecting query bias shape of {(Eq,)}, but got {b_q.shape}\"\n    assert b_k is None or b_k.shape == (Eq,), f\"expecting key bias shape of {(Eq,)}, but got {b_k.shape}\"\n    assert b_v is None or b_v.shape == (Eq,), f\"expecting value bias shape of {(Eq,)}, but got {b_v.shape}\"\n    return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)\n\nscaled_dot_product_attention = _add_docstr(\n    torch._C._nn.scaled_dot_product_attention, r\"\"\"\nscaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None) -> Tensor:\n\nComputes scaled dot product attention on query, key and value tensors, using\nan optional attention mask if passed, and applying dropout if a probability\ngreater than 0.0 is specified.\n\n.. code-block:: python\n\n    # Efficient implementation equivalent to the following:\n    def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None) -> torch.Tensor:\n        # Efficient implementation equivalent to the following:\n        L, S = query.size(-2), key.size(-2)\n        scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n        attn_bias = torch.zeros(L, S, dtype=query.dtype)\n        if is_causal:\n            assert attn_mask is None\n            temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n            attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n            attn_bias.to(query.dtype)\n\n        if attn_mask is not None:\n            if attn_mask.dtype == torch.bool:\n                attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n            else:\n                attn_bias += attn_mask\n        attn_weight = query @ key.transpose(-2, -1) * scale_factor\n        attn_weight += attn_bias\n        attn_weight = torch.softmax(attn_weight, dim=-1)\n        attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n        return attn_weight @ value\n\n.. warning:: This function is beta and subject to change.\n\nNote:\n\n    There are currently three supported implementations of scaled dot product attention:\n\n        - `FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning`_\n        - `Memory-Efficient Attention`_\n        - A PyTorch implementation defined in C++ matching the above formulation\n\n    The function may call optimized kernels for improved performance when using the CUDA backend.\n    For all other backends, the PyTorch implementation will be used.\n\n    All implementations are enabled by default. Scaled dot product attention attempts to automatically select the\n    most optimal implementation based on the inputs. In order to provide more fine-grained control over what implementation\n    is used, the following functions are provided for enabling and disabling implementations.\n    The context manager is the preferred mechanism:\n\n        - :func:`torch.backends.cuda.sdp_kernel`: A context manager used to enable/disable any of the implementations.\n        - :func:`torch.backends.cuda.enable_flash_sdp`: Enables or Disables FlashAttention.\n        - :func:`torch.backends.cuda.enable_mem_efficient_sdp`: Enables or Disables Memory-Efficient Attention.\n        - :func:`torch.backends.cuda.enable_math_sdp`: Enables or Disables the PyTorch C++ implementation.\n\n    Each of the fused kernels has specific input limitations. If the user requires the use of a specific fused implementation,\n    disable the PyTorch C++ implementation using :func:`torch.backends.cuda.sdp_kernel`.\n    In the event that a fused implementation is not available, an error will be raised with the\n    reasons why the fused implementation cannot run.\n\n    Due to the nature of fusing floating point operations, the output of this function may be different\n    depending on what backend kernel is chosen.\n    The c++ implementation supports torch.float64 and can be used when higher precision is required.\n    For more information please see :doc:`/notes/numerical_accuracy`\n\nNote:\n    {cudnn_reproducibility_note}\n\"\"\".format(**reproducibility_notes)\n    + r\"\"\"\nArgs:\n    query (Tensor): Query tensor; shape :math:`(N, ..., L, E)`.\n    key (Tensor): Key tensor; shape :math:`(N, ..., S, E)`.\n    value (Tensor): Value tensor; shape :math:`(N, ..., S, Ev)`.\n    attn_mask (optional Tensor): Attention mask; shape :math:`(N, ..., L, S)`. Two types of masks are supported.\n        A boolean mask where a value of True indicates that the element *should* take part in attention.\n        A float mask of the same type as query, key, value that is added to the attention score.\n    dropout_p (float): Dropout probability; if greater than 0.0, dropout is applied\n    is_causal (bool): If true, assumes causal attention masking and errors if both attn_mask and is_causal\n        are set.\n    scale (optional float): Scaling factor applied prior to softmax. If None, the default value is set\n        to :math:`\\frac{1}{\\sqrt{E}}`.\n\n\nReturns:\n    output (Tensor): Attention output; shape :math:`(N, ..., L, Ev)`.\n\nShape legend:\n    - :math:`N: \\text{Batch size} ... : \\text{Any number of other batch dimensions (optional)}`\n    - :math:`S: \\text{Source sequence length}`\n    - :math:`L: \\text{Target sequence length}`\n    - :math:`E: \\text{Embedding dimension of the query and key}`\n    - :math:`Ev: \\text{Embedding dimension of the value}`\n\nExamples::\n\n    >>> # Optionally use the context manager to ensure one of the fused kernels is run\n    >>> query = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n    >>> key = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n    >>> value = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n    >>> with torch.backends.cuda.sdp_kernel(enable_math=False):\n    >>>     F.scaled_dot_product_attention(query,key,value)\n\n.. _FlashAttention-2\\: Faster Attention with Better Parallelism and Work Partitioning:\n    https://arxiv.org/abs/2307.08691\n.. _Memory-Efficient Attention:\n    https://github.com/facebookresearch/xformers\n\n\"\"\")\n\ndef _mha_shape_check(query: Tensor, key: Tensor, value: Tensor,\n                     key_padding_mask: Optional[Tensor], attn_mask: Optional[Tensor], num_heads: int):\n    # Verifies the expected shape for `query, `key`, `value`, `key_padding_mask` and `attn_mask`\n    # and returns if the input is batched or not.\n    # Raises an error if `query` is not 2-D (unbatched) or 3-D (batched) tensor.\n\n    # Shape check.\n    if query.dim() == 3:\n        # Batched Inputs\n        is_batched = True\n        assert key.dim() == 3 and value.dim() == 3, \\\n            (\"For batched (3-D) `query`, expected `key` and `value` to be 3-D\"\n             f\" but found {key.dim()}-D and {value.dim()}-D tensors respectively\")\n        if key_padding_mask is not None:\n            assert key_padding_mask.dim() == 2, \\\n                (\"For batched (3-D) `query`, expected `key_padding_mask` to be `None` or 2-D\"\n                 f\" but found {key_padding_mask.dim()}-D tensor instead\")\n        if attn_mask is not None:\n            assert attn_mask.dim() in (2, 3), \\\n                (\"For batched (3-D) `query`, expected `attn_mask` to be `None`, 2-D or 3-D\"\n                 f\" but found {attn_mask.dim()}-D tensor instead\")\n    elif query.dim() == 2:\n        # Unbatched Inputs\n        is_batched = False\n        assert key.dim() == 2 and value.dim() == 2, \\\n            (\"For unbatched (2-D) `query`, expected `key` and `value` to be 2-D\"\n             f\" but found {key.dim()}-D and {value.dim()}-D tensors respectively\")\n\n        if key_padding_mask is not None:\n            assert key_padding_mask.dim() == 1, \\\n                (\"For unbatched (2-D) `query`, expected `key_padding_mask` to be `None` or 1-D\"\n                 f\" but found {key_padding_mask.dim()}-D tensor instead\")\n\n        if attn_mask is not None:\n            assert attn_mask.dim() in (2, 3), \\\n                (\"For unbatched (2-D) `query`, expected `attn_mask` to be `None`, 2-D or 3-D\"\n                 f\" but found {attn_mask.dim()}-D tensor instead\")\n            if attn_mask.dim() == 3:\n                expected_shape = (num_heads, query.shape[0], key.shape[0])\n                assert attn_mask.shape == expected_shape, \\\n                    (f\"Expected `attn_mask` shape to be {expected_shape} but got {attn_mask.shape}\")\n    else:\n        raise AssertionError(\n            f\"query should be unbatched 2D or batched 3D tensor but received {query.dim()}-D query tensor\")\n\n    return is_batched\n\ndef _canonical_mask(\n        mask: Optional[Tensor],\n        mask_name: str,\n        other_type: Optional[DType],\n        other_name: str,\n        target_type: DType,\n        check_other: bool = True,\n) -> Optional[Tensor]:\n\n    if mask is not None:\n        _mask_dtype = mask.dtype\n        _mask_is_float = torch.is_floating_point(mask)\n        if _mask_dtype != torch.bool and not _mask_is_float:\n            raise AssertionError(\n                f\"only bool and floating types of {mask_name} are supported\")\n        if check_other and other_type is not None:\n            if _mask_dtype != other_type:\n                warnings.warn(\n                    f\"Support for mismatched {mask_name} and {other_name} \"\n                    \"is deprecated. Use same type for both instead.\"\n                )\n        if not _mask_is_float:\n            mask = (\n                torch.zeros_like(mask, dtype=target_type)\n                .masked_fill_(mask, float(\"-inf\"))\n            )\n    return mask\n\ndef _none_or_dtype(input: Optional[Tensor]) -> Optional[DType]:\n    if input is None:\n        return None\n    elif isinstance(input, torch.Tensor):\n        return input.dtype\n    raise RuntimeError(\"input to _none_or_dtype() must be None or torch.Tensor\")\n\ndef multi_head_attention_forward(\n    query: Tensor,\n    key: Tensor,\n    value: Tensor,\n    embed_dim_to_check: int,\n    num_heads: int,\n    in_proj_weight: Optional[Tensor],\n    in_proj_bias: Optional[Tensor],\n    bias_k: Optional[Tensor],\n    bias_v: Optional[Tensor],\n    add_zero_attn: bool,\n    dropout_p: float,\n    out_proj_weight: Tensor,\n    out_proj_bias: Optional[Tensor],\n    training: bool = True,\n    key_padding_mask: Optional[Tensor] = None,\n    need_weights: bool = True,\n    attn_mask: Optional[Tensor] = None,\n    use_separate_proj_weight: bool = False,\n    q_proj_weight: Optional[Tensor] = None,\n    k_proj_weight: Optional[Tensor] = None,\n    v_proj_weight: Optional[Tensor] = None,\n    static_k: Optional[Tensor] = None,\n    static_v: Optional[Tensor] = None,\n    average_attn_weights: bool = True,\n    is_causal: bool = False,\n) -> Tuple[Tensor, Optional[Tensor]]:\n    r\"\"\"Forward method for MultiHeadAttention.\n\n    See :class:`torch.nn.MultiheadAttention` for details.\n\n    Args:\n        query, key, value: map a query and a set of key-value pairs to an output.\n            See \"Attention Is All You Need\" for more details.\n        embed_dim_to_check: total dimension of the model.\n        num_heads: parallel attention heads.\n        in_proj_weight, in_proj_bias: input projection weight and bias.\n        bias_k, bias_v: bias of the key and value sequences to be added at dim=0.\n        add_zero_attn: add a new batch of zeros to the key and\n                       value sequences at dim=1.\n        dropout_p: probability of an element to be zeroed.\n        out_proj_weight, out_proj_bias: the output projection weight and bias.\n        training: apply dropout if is ``True``.\n        key_padding_mask: if provided, specified padding elements in the key will\n            be ignored by the attention. This is an binary mask. When the value is True,\n            the corresponding value on the attention layer will be filled with -inf.\n        need_weights: output attn_output_weights.\n            Default: `True`\n            Note: `needs_weight` defaults to `True`, but should be set to `False`\n            For best performance when attention weights are not needed.\n            *Setting needs_weights to `True`\n            leads to a significant performance degradation.*\n        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n        is_causal: If specified, applies a causal mask as attention mask, and ignores\n            attn_mask for computing scaled dot product attention.\n            Default: ``False``.\n            .. warning::\n                is_causal is provides a hint that the attn_mask is the\n                causal mask.Providing incorrect hints can result in\n                incorrect execution, including forward and backward\n                compatibility.\n        use_separate_proj_weight: the function accept the proj. weights for query, key,\n            and value in different forms. If false, in_proj_weight will be used, which is\n            a combination of q_proj_weight, k_proj_weight, v_proj_weight.\n        q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.\n        static_k, static_v: static key and value used for attention operators.\n        average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across heads.\n            Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an effect\n            when ``need_weights=True.``. Default: True\n\n\n    Shape:\n        Inputs:\n        - query: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n          the embedding dimension.\n        - key: :math:`(S, E)` or :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n          the embedding dimension.\n        - value: :math:`(S, E)` or :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n          the embedding dimension.\n        - key_padding_mask: :math:`(S)` or :math:`(N, S)` where N is the batch size, S is the source sequence length.\n          If a FloatTensor is provided, it will be directly added to the value.\n          If a BoolTensor is provided, the positions with the\n          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n          S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked\n          positions. If a BoolTensor is provided, positions with ``True``\n          are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n          is provided, it will be added to the attention weight.\n        - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n        - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n\n        Outputs:\n        - attn_output: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n          E is the embedding dimension.\n        - attn_output_weights: Only returned when ``need_weights=True``. If ``average_attn_weights=True``, returns\n          attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or\n          :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and\n          :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per\n          head of shape :math:`(num_heads, L, S)` when input is unbatched or :math:`(N, num_heads, L, S)`.\n    \"\"\"\n    tens_ops = (query, key, value, in_proj_weight, in_proj_bias, bias_k, bias_v, out_proj_weight, out_proj_bias)\n    if has_torch_function(tens_ops):\n        return handle_torch_function(\n            multi_head_attention_forward,\n            tens_ops,\n            query,\n            key,\n            value,\n            embed_dim_to_check,\n            num_heads,\n            in_proj_weight,\n            in_proj_bias,\n            bias_k,\n            bias_v,\n            add_zero_attn,\n            dropout_p,\n            out_proj_weight,\n            out_proj_bias,\n            training=training,\n            key_padding_mask=key_padding_mask,\n            need_weights=need_weights,\n            attn_mask=attn_mask,\n            is_causal=is_causal,\n            use_separate_proj_weight=use_separate_proj_weight,\n            q_proj_weight=q_proj_weight,\n            k_proj_weight=k_proj_weight,\n            v_proj_weight=v_proj_weight,\n            static_k=static_k,\n            static_v=static_v,\n            average_attn_weights=average_attn_weights,\n        )\n\n    is_batched = _mha_shape_check(query, key, value, key_padding_mask, attn_mask, num_heads)\n\n    # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input\n    # is batched, run the computation and before returning squeeze the\n    # batch dimension so that the output doesn't carry this temporary batch dimension.\n    if not is_batched:\n        # unsqueeze if the input is unbatched\n        query = query.unsqueeze(1)\n        key = key.unsqueeze(1)\n        value = value.unsqueeze(1)\n        if key_padding_mask is not None:\n            key_padding_mask = key_padding_mask.unsqueeze(0)\n\n    # set up shape vars\n    tgt_len, bsz, embed_dim = query.shape\n    src_len, _, _ = key.shape\n\n    key_padding_mask = _canonical_mask(\n        mask=key_padding_mask,\n        mask_name=\"key_padding_mask\",\n        other_type=_none_or_dtype(attn_mask),\n        other_name=\"attn_mask\",\n        target_type=query.dtype\n    )\n\n    if is_causal and attn_mask is None:\n        raise RuntimeError(\n            \"Need attn_mask if specifying the is_causal hint. \"\n            \"You may use the Transformer module method \"\n            \"`generate_square_subsequent_mask` to create this mask.\"\n        )\n\n    if is_causal and key_padding_mask is None and not need_weights:\n        # when we have a kpm or need weights, we need attn_mask\n        # Otherwise, we use the is_causal hint go as is_causal\n        # indicator to SDPA.\n        attn_mask = None\n    else:\n        attn_mask = _canonical_mask(\n            mask=attn_mask,\n            mask_name=\"attn_mask\",\n            other_type=None,\n            other_name=\"\",\n            target_type=query.dtype,\n            check_other=False,\n        )\n\n        if key_padding_mask is not None:\n            # We have the attn_mask, and use that to merge kpm into it.\n            # Turn off use of is_causal hint, as the merged mask is no\n            # longer causal.\n            is_causal = False\n\n    assert embed_dim == embed_dim_to_check, \\\n        f\"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}\"\n    if isinstance(embed_dim, torch.Tensor):\n        # embed_dim can be a tensor when JIT tracing\n        head_dim = embed_dim.div(num_heads, rounding_mode='trunc')\n    else:\n        head_dim = embed_dim // num_heads\n    assert head_dim * num_heads == embed_dim, f\"embed_dim {embed_dim} not divisible by num_heads {num_heads}\"\n    if use_separate_proj_weight:\n        # allow MHA to have different embedding dimensions when separate projection weights are used\n        assert key.shape[:2] == value.shape[:2], \\\n            f\"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}\"\n    else:\n        assert key.shape == value.shape, f\"key shape {key.shape} does not match value shape {value.shape}\"\n\n    #\n    # compute in-projection\n    #\n    if not use_separate_proj_weight:\n        assert in_proj_weight is not None, \"use_separate_proj_weight is False but in_proj_weight is None\"\n        q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n    else:\n        assert q_proj_weight is not None, \"use_separate_proj_weight is True but q_proj_weight is None\"\n        assert k_proj_weight is not None, \"use_separate_proj_weight is True but k_proj_weight is None\"\n        assert v_proj_weight is not None, \"use_separate_proj_weight is True but v_proj_weight is None\"\n        if in_proj_bias is None:\n            b_q = b_k = b_v = None\n        else:\n            b_q, b_k, b_v = in_proj_bias.chunk(3)\n        q, k, v = _in_projection(query, key, value, q_proj_weight, k_proj_weight, v_proj_weight, b_q, b_k, b_v)\n\n    # prep attention mask\n\n    if attn_mask is not None:\n        # ensure attn_mask's dim is 3\n        if attn_mask.dim() == 2:\n            correct_2d_size = (tgt_len, src_len)\n            if attn_mask.shape != correct_2d_size:\n                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n            attn_mask = attn_mask.unsqueeze(0)\n        elif attn_mask.dim() == 3:\n            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n            if attn_mask.shape != correct_3d_size:\n                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n        else:\n            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n\n    # add bias along batch dimension (currently second)\n    if bias_k is not None and bias_v is not None:\n        assert static_k is None, \"bias cannot be added to static key.\"\n        assert static_v is None, \"bias cannot be added to static value.\"\n        k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n        v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n        if attn_mask is not None:\n            attn_mask = pad(attn_mask, (0, 1))\n        if key_padding_mask is not None:\n            key_padding_mask = pad(key_padding_mask, (0, 1))\n    else:\n        assert bias_k is None\n        assert bias_v is None\n\n    #\n    # reshape q, k, v for multihead attention and make em batch first\n    #\n    q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n    if static_k is None:\n        k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n    else:\n        # TODO finish disentangling control flow so we don't do in-projections when statics are passed\n        assert static_k.size(0) == bsz * num_heads, \\\n            f\"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}\"\n        assert static_k.size(2) == head_dim, \\\n            f\"expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}\"\n        k = static_k\n    if static_v is None:\n        v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n    else:\n        # TODO finish disentangling control flow so we don't do in-projections when statics are passed\n        assert static_v.size(0) == bsz * num_heads, \\\n            f\"expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}\"\n        assert static_v.size(2) == head_dim, \\\n            f\"expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}\"\n        v = static_v\n\n    # add zero attention along batch dimension (now first)\n    if add_zero_attn:\n        zero_attn_shape = (bsz * num_heads, 1, head_dim)\n        k = torch.cat([k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1)\n        v = torch.cat([v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1)\n        if attn_mask is not None:\n            attn_mask = pad(attn_mask, (0, 1))\n        if key_padding_mask is not None:\n            key_padding_mask = pad(key_padding_mask, (0, 1))\n\n    # update source sequence length after adjustments\n    src_len = k.size(1)\n\n    # merge key padding and attention masks\n    if key_padding_mask is not None:\n        assert key_padding_mask.shape == (bsz, src_len), \\\n            f\"expecting key_padding_mask shape of {(bsz, src_len)}, but got {key_padding_mask.shape}\"\n        key_padding_mask = key_padding_mask.view(bsz, 1, 1, src_len).   \\\n            expand(-1, num_heads, -1, -1).reshape(bsz * num_heads, 1, src_len)\n        if attn_mask is None:\n            attn_mask = key_padding_mask\n        else:\n            attn_mask = attn_mask + key_padding_mask\n\n    # adjust dropout probability\n    if not training:\n        dropout_p = 0.0\n\n    #\n    # (deep breath) calculate attention and out projection\n    #\n\n    if need_weights:\n        B, Nt, E = q.shape\n        q_scaled = q / math.sqrt(E)\n\n        assert not (is_causal and attn_mask is None), \"FIXME: is_causal not implemented for need_weights\"\n\n        if attn_mask is not None:\n            attn_output_weights = torch.baddbmm(attn_mask, q_scaled, k.transpose(-2, -1))\n        else:\n            attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))\n        attn_output_weights = softmax(attn_output_weights, dim=-1)\n        if dropout_p > 0.0:\n            attn_output_weights = dropout(attn_output_weights, p=dropout_p)\n\n        attn_output = torch.bmm(attn_output_weights, v)\n\n        attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n        attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n        attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))\n\n        # optionally average attention weights over heads\n        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n        if average_attn_weights:\n            attn_output_weights = attn_output_weights.mean(dim=1)\n\n        if not is_batched:\n            # squeeze the output if input was unbatched\n            attn_output = attn_output.squeeze(1)\n            attn_output_weights = attn_output_weights.squeeze(0)\n        return attn_output, attn_output_weights\n    else:\n        # attn_mask can be either (L,S) or (N*num_heads, L, S)\n        # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n        # in order to match the input for SDPA of (N, num_heads, L, S)\n        if attn_mask is not None:\n            if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n                attn_mask = attn_mask.unsqueeze(0)\n            else:\n                attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n\n        q = q.view(bsz, num_heads, tgt_len, head_dim)\n        k = k.view(bsz, num_heads, src_len, head_dim)\n        v = v.view(bsz, num_heads, src_len, head_dim)\n\n        attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n        attn_output = attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)\n\n        attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n        attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))\n        if not is_batched:\n            # squeeze the output if input was unbatched\n            attn_output = attn_output.squeeze(1)\n        return attn_output, None\n", 5484], "C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py": ["import warnings\nfrom typing import Optional, Tuple\n\nimport torch\nfrom torch import Tensor\nfrom .linear import NonDynamicallyQuantizableLinear\nfrom torch.nn.init import constant_, xavier_normal_, xavier_uniform_\nfrom torch.nn.parameter import Parameter\nfrom .module import Module\nfrom .. import functional as F\n\n__all__ = ['Threshold', 'ReLU', 'RReLU', 'Hardtanh', 'ReLU6', 'Sigmoid', 'Hardsigmoid', 'Tanh',\n           'SiLU', 'Mish', 'Hardswish', 'ELU', 'CELU', 'SELU', 'GLU', 'GELU', 'Hardshrink', 'LeakyReLU',\n           'LogSigmoid', 'Softplus', 'Softshrink', 'MultiheadAttention', 'PReLU', 'Softsign', 'Tanhshrink',\n           'Softmin', 'Softmax', 'Softmax2d', 'LogSoftmax']\n\n\nclass Threshold(Module):\n    r\"\"\"Thresholds each element of the input Tensor.\n\n    Threshold is defined as:\n\n    .. math::\n        y =\n        \\begin{cases}\n        x, &\\text{ if } x > \\text{threshold} \\\\\n        \\text{value}, &\\text{ otherwise }\n        \\end{cases}\n\n    Args:\n        threshold: The value to threshold at\n        value: The value to replace with\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    Examples::\n\n        >>> m = nn.Threshold(0.1, 20)\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n    __constants__ = ['threshold', 'value', 'inplace']\n\n    threshold: float\n    value: float\n    inplace: bool\n\n    def __init__(self, threshold: float, value: float, inplace: bool = False) -> None:\n        super().__init__()\n        self.threshold = threshold\n        self.value = value\n        self.inplace = inplace\n        # TODO: check in THNN (if inplace == True, then assert value <= threshold)\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.threshold(input, self.threshold, self.value, self.inplace)\n\n    def extra_repr(self):\n        inplace_str = ', inplace=True' if self.inplace else ''\n        return f'threshold={self.threshold}, value={self.value}{inplace_str}'\n\n\nclass ReLU(Module):\n    r\"\"\"Applies the rectified linear unit function element-wise:\n\n    :math:`\\text{ReLU}(x) = (x)^+ = \\max(0, x)`\n\n    Args:\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/ReLU.png\n\n    Examples::\n\n        >>> m = nn.ReLU()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n\n\n      An implementation of CReLU - https://arxiv.org/abs/1603.05201\n\n        >>> m = nn.ReLU()\n        >>> input = torch.randn(2).unsqueeze(0)\n        >>> output = torch.cat((m(input), m(-input)))\n    \"\"\"\n    __constants__ = ['inplace']\n    inplace: bool\n\n    def __init__(self, inplace: bool = False):\n        super().__init__()\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.relu(input, inplace=self.inplace)\n\n    def extra_repr(self) -> str:\n        inplace_str = 'inplace=True' if self.inplace else ''\n        return inplace_str\n\n\nclass RReLU(Module):\n    r\"\"\"Applies the randomized leaky rectified linear unit function, element-wise,\n    as described in the paper:\n\n    `Empirical Evaluation of Rectified Activations in Convolutional Network`_.\n\n    The function is defined as:\n\n    .. math::\n        \\text{RReLU}(x) =\n        \\begin{cases}\n            x & \\text{if } x \\geq 0 \\\\\n            ax & \\text{ otherwise }\n        \\end{cases}\n\n    where :math:`a` is randomly sampled from uniform distribution\n    :math:`\\mathcal{U}(\\text{lower}, \\text{upper})` during training while during\n    evaluation :math:`a` is fixed with :math:`a = \\frac{\\text{lower} + \\text{upper}}{2}`.\n\n     See: https://arxiv.org/pdf/1505.00853.pdf\n\n    Args:\n        lower: lower bound of the uniform distribution. Default: :math:`\\frac{1}{8}`\n        upper: upper bound of the uniform distribution. Default: :math:`\\frac{1}{3}`\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/RReLU.png\n\n    Examples::\n\n        >>> m = nn.RReLU(0.1, 0.3)\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n\n    .. _`Empirical Evaluation of Rectified Activations in Convolutional Network`:\n        https://arxiv.org/abs/1505.00853\n    \"\"\"\n    __constants__ = ['lower', 'upper', 'inplace']\n\n    lower: float\n    upper: float\n    inplace: bool\n\n    def __init__(\n        self,\n        lower: float = 1. / 8,\n        upper: float = 1. / 3,\n        inplace: bool = False\n    ):\n        super().__init__()\n        self.lower = lower\n        self.upper = upper\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.rrelu(input, self.lower, self.upper, self.training, self.inplace)\n\n    def extra_repr(self):\n        inplace_str = ', inplace=True' if self.inplace else ''\n        return f'lower={self.lower}, upper={self.upper}{inplace_str}'\n\n\nclass Hardtanh(Module):\n    r\"\"\"Applies the HardTanh function element-wise.\n\n    HardTanh is defined as:\n\n    .. math::\n        \\text{HardTanh}(x) = \\begin{cases}\n            \\text{max\\_val} & \\text{ if } x > \\text{ max\\_val } \\\\\n            \\text{min\\_val} & \\text{ if } x < \\text{ min\\_val } \\\\\n            x & \\text{ otherwise } \\\\\n        \\end{cases}\n\n    Args:\n        min_val: minimum value of the linear region range. Default: -1\n        max_val: maximum value of the linear region range. Default: 1\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Keyword arguments :attr:`min_value` and :attr:`max_value`\n    have been deprecated in favor of :attr:`min_val` and :attr:`max_val`.\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Hardtanh.png\n\n    Examples::\n\n        >>> m = nn.Hardtanh(-2, 2)\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n    __constants__ = ['min_val', 'max_val', 'inplace']\n\n    min_val: float\n    max_val: float\n    inplace: bool\n\n    def __init__(\n        self,\n        min_val: float = -1.,\n        max_val: float = 1.,\n        inplace: bool = False,\n        min_value: Optional[float] = None,\n        max_value: Optional[float] = None\n    ) -> None:\n        super().__init__()\n        if min_value is not None:\n            warnings.warn(\"keyword argument min_value is deprecated and rename to min_val\")\n            min_val = min_value\n        if max_value is not None:\n            warnings.warn(\"keyword argument max_value is deprecated and rename to max_val\")\n            max_val = max_value\n\n        self.min_val = min_val\n        self.max_val = max_val\n        self.inplace = inplace\n        assert self.max_val > self.min_val\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.hardtanh(input, self.min_val, self.max_val, self.inplace)\n\n    def extra_repr(self) -> str:\n        inplace_str = ', inplace=True' if self.inplace else ''\n        return f'min_val={self.min_val}, max_val={self.max_val}{inplace_str}'\n\n\nclass ReLU6(Hardtanh):\n    r\"\"\"Applies the element-wise function:\n\n    .. math::\n        \\text{ReLU6}(x) = \\min(\\max(0,x), 6)\n\n    Args:\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/ReLU6.png\n\n    Examples::\n\n        >>> m = nn.ReLU6()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    def __init__(self, inplace: bool = False):\n        super().__init__(0., 6., inplace)\n\n    def extra_repr(self) -> str:\n        inplace_str = 'inplace=True' if self.inplace else ''\n        return inplace_str\n\n\nclass Sigmoid(Module):\n    r\"\"\"Applies the element-wise function:\n\n    .. math::\n        \\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1 + \\exp(-x)}\n\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Sigmoid.png\n\n    Examples::\n\n        >>> m = nn.Sigmoid()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    def forward(self, input: Tensor) -> Tensor:\n        return torch.sigmoid(input)\n\n\nclass Hardsigmoid(Module):\n    r\"\"\"Applies the Hardsigmoid function element-wise.\n\n    Hardsigmoid is defined as:\n\n    .. math::\n        \\text{Hardsigmoid}(x) = \\begin{cases}\n            0 & \\text{if~} x \\le -3, \\\\\n            1 & \\text{if~} x \\ge +3, \\\\\n            x / 6 + 1 / 2 & \\text{otherwise}\n        \\end{cases}\n\n    Args:\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Hardsigmoid.png\n\n    Examples::\n\n        >>> m = nn.Hardsigmoid()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n    __constants__ = ['inplace']\n\n    inplace: bool\n\n    def __init__(self, inplace : bool = False) -> None:\n        super().__init__()\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.hardsigmoid(input, self.inplace)\n\n\nclass Tanh(Module):\n    r\"\"\"Applies the Hyperbolic Tangent (Tanh) function element-wise.\n\n    Tanh is defined as:\n\n    .. math::\n        \\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)} {\\exp(x) + \\exp(-x)}\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Tanh.png\n\n    Examples::\n\n        >>> m = nn.Tanh()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    def forward(self, input: Tensor) -> Tensor:\n        return torch.tanh(input)\n\nclass SiLU(Module):\n    r\"\"\"Applies the Sigmoid Linear Unit (SiLU) function, element-wise.\n    The SiLU function is also known as the swish function.\n\n    .. math::\n        \\text{silu}(x) = x * \\sigma(x), \\text{where } \\sigma(x) \\text{ is the logistic sigmoid.}\n\n    .. note::\n        See `Gaussian Error Linear Units (GELUs) <https://arxiv.org/abs/1606.08415>`_\n        where the SiLU (Sigmoid Linear Unit) was originally coined, and see\n        `Sigmoid-Weighted Linear Units for Neural Network Function Approximation\n        in Reinforcement Learning <https://arxiv.org/abs/1702.03118>`_ and `Swish:\n        a Self-Gated Activation Function <https://arxiv.org/abs/1710.05941v1>`_\n        where the SiLU was experimented with later.\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/SiLU.png\n\n    Examples::\n\n        >>> m = nn.SiLU()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n    __constants__ = ['inplace']\n    inplace: bool\n\n    def __init__(self, inplace: bool = False):\n        super().__init__()\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.silu(input, inplace=self.inplace)\n\n    def extra_repr(self) -> str:\n        inplace_str = 'inplace=True' if self.inplace else ''\n        return inplace_str\n\nclass Mish(Module):\n    r\"\"\"Applies the Mish function, element-wise.\n    Mish: A Self Regularized Non-Monotonic Neural Activation Function.\n\n    .. math::\n        \\text{Mish}(x) = x * \\text{Tanh}(\\text{Softplus}(x))\n\n    .. note::\n        See `Mish: A Self Regularized Non-Monotonic Neural Activation Function <https://arxiv.org/abs/1908.08681>`_\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Mish.png\n\n    Examples::\n\n        >>> m = nn.Mish()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n    __constants__ = ['inplace']\n    inplace: bool\n\n    def __init__(self, inplace: bool = False):\n        super().__init__()\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.mish(input, inplace=self.inplace)\n\n    def extra_repr(self) -> str:\n        inplace_str = 'inplace=True' if self.inplace else ''\n        return inplace_str\n\nclass Hardswish(Module):\n    r\"\"\"Applies the Hardswish function, element-wise, as described in the paper:\n    `Searching for MobileNetV3 <https://arxiv.org/abs/1905.02244>`_.\n\n    Hardswish is defined as:\n\n    .. math::\n        \\text{Hardswish}(x) = \\begin{cases}\n            0 & \\text{if~} x \\le -3, \\\\\n            x & \\text{if~} x \\ge +3, \\\\\n            x \\cdot (x + 3) /6 & \\text{otherwise}\n        \\end{cases}\n\n    Args:\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Hardswish.png\n\n    Examples::\n\n        >>> m = nn.Hardswish()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n    __constants__ = ['inplace']\n\n    inplace: bool\n\n    def __init__(self, inplace : bool = False) -> None:\n        super().__init__()\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.hardswish(input, self.inplace)\n\n\nclass ELU(Module):\n    r\"\"\"Applies the Exponential Linear Unit (ELU) function, element-wise, as described\n    in the paper: `Fast and Accurate Deep Network Learning by Exponential Linear\n    Units (ELUs) <https://arxiv.org/abs/1511.07289>`__.\n\n    ELU is defined as:\n\n    .. math::\n        \\text{ELU}(x) = \\begin{cases}\n        x, & \\text{ if } x > 0\\\\\n        \\alpha * (\\exp(x) - 1), & \\text{ if } x \\leq 0\n        \\end{cases}\n\n    Args:\n        alpha: the :math:`\\alpha` value for the ELU formulation. Default: 1.0\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/ELU.png\n\n    Examples::\n\n        >>> m = nn.ELU()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n    __constants__ = ['alpha', 'inplace']\n    alpha: float\n    inplace: bool\n\n    def __init__(self, alpha: float = 1., inplace: bool = False) -> None:\n        super().__init__()\n        self.alpha = alpha\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.elu(input, self.alpha, self.inplace)\n\n    def extra_repr(self) -> str:\n        inplace_str = ', inplace=True' if self.inplace else ''\n        return f'alpha={self.alpha}{inplace_str}'\n\n\nclass CELU(Module):\n    r\"\"\"Applies the element-wise function:\n\n    .. math::\n        \\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))\n\n    More details can be found in the paper `Continuously Differentiable Exponential Linear Units`_ .\n\n    Args:\n        alpha: the :math:`\\alpha` value for the CELU formulation. Default: 1.0\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/CELU.png\n\n    Examples::\n\n        >>> m = nn.CELU()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n\n    .. _`Continuously Differentiable Exponential Linear Units`:\n        https://arxiv.org/abs/1704.07483\n    \"\"\"\n    __constants__ = ['alpha', 'inplace']\n    alpha: float\n    inplace: bool\n\n    def __init__(self, alpha: float = 1., inplace: bool = False) -> None:\n        super().__init__()\n        self.alpha = alpha\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.celu(input, self.alpha, self.inplace)\n\n    def extra_repr(self) -> str:\n        inplace_str = ', inplace=True' if self.inplace else ''\n        return f'alpha={self.alpha}{inplace_str}'\n\n\nclass SELU(Module):\n    r\"\"\"Applied element-wise, as:\n\n    .. math::\n        \\text{SELU}(x) = \\text{scale} * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))\n\n    with :math:`\\alpha = 1.6732632423543772848170429916717` and\n    :math:`\\text{scale} = 1.0507009873554804934193349852946`.\n\n    .. warning::\n        When using ``kaiming_normal`` or ``kaiming_normal_`` for initialisation,\n        ``nonlinearity='linear'`` should be used instead of ``nonlinearity='selu'``\n        in order to get `Self-Normalizing Neural Networks`_.\n        See :func:`torch.nn.init.calculate_gain` for more information.\n\n    More details can be found in the paper `Self-Normalizing Neural Networks`_ .\n\n    Args:\n        inplace (bool, optional): can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/SELU.png\n\n    Examples::\n\n        >>> m = nn.SELU()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n\n    .. _Self-Normalizing Neural Networks: https://arxiv.org/abs/1706.02515\n    \"\"\"\n    __constants__ = ['inplace']\n    inplace: bool\n\n    def __init__(self, inplace: bool = False) -> None:\n        super().__init__()\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.selu(input, self.inplace)\n\n    def extra_repr(self) -> str:\n        inplace_str = 'inplace=True' if self.inplace else ''\n        return inplace_str\n\n\nclass GLU(Module):\n    r\"\"\"Applies the gated linear unit function\n    :math:`{GLU}(a, b)= a \\otimes \\sigma(b)` where :math:`a` is the first half\n    of the input matrices and :math:`b` is the second half.\n\n    Args:\n        dim (int): the dimension on which to split the input. Default: -1\n\n    Shape:\n        - Input: :math:`(\\ast_1, N, \\ast_2)` where `*` means, any number of additional\n          dimensions\n        - Output: :math:`(\\ast_1, M, \\ast_2)` where :math:`M=N/2`\n\n    Examples::\n\n        >>> m = nn.GLU()\n        >>> input = torch.randn(4, 2)\n        >>> output = m(input)\n    \"\"\"\n    __constants__ = ['dim']\n    dim: int\n\n    def __init__(self, dim: int = -1) -> None:\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.glu(input, self.dim)\n\n    def extra_repr(self) -> str:\n        return f'dim={self.dim}'\n\n\nclass GELU(Module):\n    r\"\"\"Applies the Gaussian Error Linear Units function:\n\n    .. math:: \\text{GELU}(x) = x * \\Phi(x)\n\n    where :math:`\\Phi(x)` is the Cumulative Distribution Function for Gaussian Distribution.\n\n    When the approximate argument is 'tanh', Gelu is estimated with:\n\n    .. math:: \\text{GELU}(x) = 0.5 * x * (1 + \\text{Tanh}(\\sqrt{2 / \\pi} * (x + 0.044715 * x^3)))\n\n    Args:\n        approximate (str, optional): the gelu approximation algorithm to use:\n            ``'none'`` | ``'tanh'``. Default: ``'none'``\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/GELU.png\n\n    Examples::\n\n        >>> m = nn.GELU()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n    __constants__ = ['approximate']\n    approximate: str\n\n    def __init__(self, approximate: str = 'none') -> None:\n        super().__init__()\n        self.approximate = approximate\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.gelu(input, approximate=self.approximate)\n\n    def extra_repr(self) -> str:\n        return f'approximate={repr(self.approximate)}'\n\n\nclass Hardshrink(Module):\n    r\"\"\"Applies the Hard Shrinkage (Hardshrink) function element-wise.\n\n    Hardshrink is defined as:\n\n    .. math::\n        \\text{HardShrink}(x) =\n        \\begin{cases}\n        x, & \\text{ if } x > \\lambda \\\\\n        x, & \\text{ if } x < -\\lambda \\\\\n        0, & \\text{ otherwise }\n        \\end{cases}\n\n    Args:\n        lambd: the :math:`\\lambda` value for the Hardshrink formulation. Default: 0.5\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Hardshrink.png\n\n    Examples::\n\n        >>> m = nn.Hardshrink()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n    __constants__ = ['lambd']\n    lambd: float\n\n    def __init__(self, lambd: float = 0.5) -> None:\n        super().__init__()\n        self.lambd = lambd\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.hardshrink(input, self.lambd)\n\n    def extra_repr(self) -> str:\n        return f'{self.lambd}'\n\n\nclass LeakyReLU(Module):\n    r\"\"\"Applies the element-wise function:\n\n    .. math::\n        \\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)\n\n\n    or\n\n    .. math::\n        \\text{LeakyReLU}(x) =\n        \\begin{cases}\n        x, & \\text{ if } x \\geq 0 \\\\\n        \\text{negative\\_slope} \\times x, & \\text{ otherwise }\n        \\end{cases}\n\n    Args:\n        negative_slope: Controls the angle of the negative slope (which is used for\n          negative input values). Default: 1e-2\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(*)` where `*` means, any number of additional\n          dimensions\n        - Output: :math:`(*)`, same shape as the input\n\n    .. image:: ../scripts/activation_images/LeakyReLU.png\n\n    Examples::\n\n        >>> m = nn.LeakyReLU(0.1)\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n    __constants__ = ['inplace', 'negative_slope']\n    inplace: bool\n    negative_slope: float\n\n    def __init__(self, negative_slope: float = 1e-2, inplace: bool = False) -> None:\n        super().__init__()\n        self.negative_slope = negative_slope\n        self.inplace = inplace\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.leaky_relu(input, self.negative_slope, self.inplace)\n\n    def extra_repr(self) -> str:\n        inplace_str = ', inplace=True' if self.inplace else ''\n        return f'negative_slope={self.negative_slope}{inplace_str}'\n\n\nclass LogSigmoid(Module):\n    r\"\"\"Applies the element-wise function:\n\n    .. math::\n        \\text{LogSigmoid}(x) = \\log\\left(\\frac{ 1 }{ 1 + \\exp(-x)}\\right)\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/LogSigmoid.png\n\n    Examples::\n\n        >>> m = nn.LogSigmoid()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.logsigmoid(input)\n\n\nclass Softplus(Module):\n    r\"\"\"Applies the Softplus function :math:`\\text{Softplus}(x) = \\frac{1}{\\beta} *\n    \\log(1 + \\exp(\\beta * x))` element-wise.\n\n    SoftPlus is a smooth approximation to the ReLU function and can be used\n    to constrain the output of a machine to always be positive.\n\n    For numerical stability the implementation reverts to the linear function\n    when :math:`input \\times \\beta > threshold`.\n\n    Args:\n        beta: the :math:`\\beta` value for the Softplus formulation. Default: 1\n        threshold: values above this revert to a linear function. Default: 20\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Softplus.png\n\n    Examples::\n\n        >>> m = nn.Softplus()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n    __constants__ = ['beta', 'threshold']\n    beta: int\n    threshold: int\n\n    def __init__(self, beta: int = 1, threshold: int = 20) -> None:\n        super().__init__()\n        self.beta = beta\n        self.threshold = threshold\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.softplus(input, self.beta, self.threshold)\n\n    def extra_repr(self) -> str:\n        return f'beta={self.beta}, threshold={self.threshold}'\n\n\nclass Softshrink(Module):\n    r\"\"\"Applies the soft shrinkage function elementwise:\n\n    .. math::\n        \\text{SoftShrinkage}(x) =\n        \\begin{cases}\n        x - \\lambda, & \\text{ if } x > \\lambda \\\\\n        x + \\lambda, & \\text{ if } x < -\\lambda \\\\\n        0, & \\text{ otherwise }\n        \\end{cases}\n\n    Args:\n        lambd: the :math:`\\lambda` (must be no less than zero) value for the Softshrink formulation. Default: 0.5\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Softshrink.png\n\n    Examples::\n\n        >>> m = nn.Softshrink()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n    __constants__ = ['lambd']\n    lambd: float\n\n    def __init__(self, lambd: float = 0.5) -> None:\n        super().__init__()\n        self.lambd = lambd\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.softshrink(input, self.lambd)\n\n    def extra_repr(self) -> str:\n        return str(self.lambd)\n\n\ndef _check_arg_device(x: Optional[torch.Tensor]) -> bool:\n    if x is not None:\n        return x.device.type in [\"cpu\", \"cuda\", torch.utils.backend_registration._privateuse1_backend_name]\n    return True\n\n\ndef _arg_requires_grad(x: Optional[torch.Tensor]) -> bool:\n    if x is not None:\n        return x.requires_grad\n    return False\n\n\ndef _is_make_fx_tracing():\n    if not torch.jit.is_scripting():\n        torch_dispatch_mode_stack = torch.utils._python_dispatch._get_current_dispatch_mode_stack()\n        return any(type(x) == torch.fx.experimental.proxy_tensor.ProxyTorchDispatchMode for x in torch_dispatch_mode_stack)\n    else:\n        return False\n\n\nclass MultiheadAttention(Module):\n    r\"\"\"Allows the model to jointly attend to information\n    from different representation subspaces as described in the paper:\n    `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_.\n\n    Multi-Head Attention is defined as:\n\n    .. math::\n        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n\n    where :math:`head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n\n    ``nn.MultiHeadAttention`` will use the optimized implementations of\n    ``scaled_dot_product_attention()`` when possible.\n\n    In addition to support for the new ``scaled_dot_product_attention()``\n    function, for speeding up Inference, MHA will use\n    fastpath inference with support for Nested Tensors, iff:\n\n    - self attention is being computed (i.e., ``query``, ``key``, and ``value`` are the same tensor).\n    - inputs are batched (3D) with ``batch_first==True``\n    - Either autograd is disabled (using ``torch.inference_mode`` or ``torch.no_grad``) or no tensor argument ``requires_grad``\n    - training is disabled (using ``.eval()``)\n    - ``add_bias_kv`` is ``False``\n    - ``add_zero_attn`` is ``False``\n    - ``batch_first`` is ``True`` and the input is batched\n    - ``kdim`` and ``vdim`` are equal to ``embed_dim``\n    - if a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ is passed, neither ``key_padding_mask``\n      nor ``attn_mask`` is passed\n    - autocast is disabled\n\n    If the optimized inference fastpath implementation is in use, a\n    `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ can be passed for\n    ``query``/``key``/``value`` to represent padding more efficiently than using a\n    padding mask. In this case, a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_\n    will be returned, and an additional speedup proportional to the fraction of the input\n    that is padding can be expected.\n\n    Args:\n        embed_dim: Total dimension of the model.\n        num_heads: Number of parallel attention heads. Note that ``embed_dim`` will be split\n            across ``num_heads`` (i.e. each head will have dimension ``embed_dim // num_heads``).\n        dropout: Dropout probability on ``attn_output_weights``. Default: ``0.0`` (no dropout).\n        bias: If specified, adds bias to input / output projection layers. Default: ``True``.\n        add_bias_kv: If specified, adds bias to the key and value sequences at dim=0. Default: ``False``.\n        add_zero_attn: If specified, adds a new batch of zeros to the key and value sequences at dim=1.\n            Default: ``False``.\n        kdim: Total number of features for keys. Default: ``None`` (uses ``kdim=embed_dim``).\n        vdim: Total number of features for values. Default: ``None`` (uses ``vdim=embed_dim``).\n        batch_first: If ``True``, then the input and output tensors are provided\n            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n\n    Examples::\n\n        >>> # xdoctest: +SKIP\n        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n\n    .. _`FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness`:\n         https://arxiv.org/abs/2205.14135\n\n    \"\"\"\n\n    __constants__ = ['batch_first']\n    bias_k: Optional[torch.Tensor]\n    bias_v: Optional[torch.Tensor]\n\n    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False,\n                 kdim=None, vdim=None, batch_first=False, device=None, dtype=None) -> None:\n        if embed_dim <= 0 or num_heads <= 0:\n            raise ValueError(\n                f\"embed_dim and num_heads must be greater than 0,\"\n                f\" got embed_dim={embed_dim} and num_heads={num_heads} instead\"\n            )\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.kdim = kdim if kdim is not None else embed_dim\n        self.vdim = vdim if vdim is not None else embed_dim\n        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.batch_first = batch_first\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n\n        if not self._qkv_same_embed_dim:\n            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))\n            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))\n            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))\n            self.register_parameter('in_proj_weight', None)\n        else:\n            self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim), **factory_kwargs))\n            self.register_parameter('q_proj_weight', None)\n            self.register_parameter('k_proj_weight', None)\n            self.register_parameter('v_proj_weight', None)\n\n        if bias:\n            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim, **factory_kwargs))\n        else:\n            self.register_parameter('in_proj_bias', None)\n        self.out_proj = NonDynamicallyQuantizableLinear(embed_dim, embed_dim, bias=bias, **factory_kwargs)\n\n        if add_bias_kv:\n            self.bias_k = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n            self.bias_v = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n        else:\n            self.bias_k = self.bias_v = None\n\n        self.add_zero_attn = add_zero_attn\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        if self._qkv_same_embed_dim:\n            xavier_uniform_(self.in_proj_weight)\n        else:\n            xavier_uniform_(self.q_proj_weight)\n            xavier_uniform_(self.k_proj_weight)\n            xavier_uniform_(self.v_proj_weight)\n\n        if self.in_proj_bias is not None:\n            constant_(self.in_proj_bias, 0.)\n            constant_(self.out_proj.bias, 0.)\n        if self.bias_k is not None:\n            xavier_normal_(self.bias_k)\n        if self.bias_v is not None:\n            xavier_normal_(self.bias_v)\n\n    def __setstate__(self, state):\n        # Support loading old MultiheadAttention checkpoints generated by v1.1.0\n        if '_qkv_same_embed_dim' not in state:\n            state['_qkv_same_embed_dim'] = True\n\n        super().__setstate__(state)\n\n    def forward(\n            self,\n            query: Tensor,\n            key: Tensor,\n            value: Tensor,\n            key_padding_mask: Optional[Tensor] = None,\n            need_weights: bool = True,\n            attn_mask: Optional[Tensor] = None,\n            average_attn_weights: bool = True,\n            is_causal : bool = False) -> Tuple[Tensor, Optional[Tensor]]:\n        r\"\"\"\n    Args:\n        query: Query embeddings of shape :math:`(L, E_q)` for unbatched input, :math:`(L, N, E_q)` when ``batch_first=False``\n            or :math:`(N, L, E_q)` when ``batch_first=True``, where :math:`L` is the target sequence length,\n            :math:`N` is the batch size, and :math:`E_q` is the query embedding dimension ``embed_dim``.\n            Queries are compared against key-value pairs to produce the output.\n            See \"Attention Is All You Need\" for more details.\n        key: Key embeddings of shape :math:`(S, E_k)` for unbatched input, :math:`(S, N, E_k)` when ``batch_first=False``\n            or :math:`(N, S, E_k)` when ``batch_first=True``, where :math:`S` is the source sequence length,\n            :math:`N` is the batch size, and :math:`E_k` is the key embedding dimension ``kdim``.\n            See \"Attention Is All You Need\" for more details.\n        value: Value embeddings of shape :math:`(S, E_v)` for unbatched input, :math:`(S, N, E_v)` when\n            ``batch_first=False`` or :math:`(N, S, E_v)` when ``batch_first=True``, where :math:`S` is the source\n            sequence length, :math:`N` is the batch size, and :math:`E_v` is the value embedding dimension ``vdim``.\n            See \"Attention Is All You Need\" for more details.\n        key_padding_mask: If specified, a mask of shape :math:`(N, S)` indicating which elements within ``key``\n            to ignore for the purpose of attention (i.e. treat as \"padding\"). For unbatched `query`, shape should be :math:`(S)`.\n            Binary and float masks are supported.\n            For a binary mask, a ``True`` value indicates that the corresponding ``key`` value will be ignored for\n            the purpose of attention. For a float mask, it will be directly added to the corresponding ``key`` value.\n        need_weights: If specified, returns ``attn_output_weights`` in addition to ``attn_outputs``.\n            Set ``need_weights=False`` to use the optimized ``scaled_dot_product_attention``\n            and achieve the best performance for MHA.\n            Default: ``True``.\n        attn_mask: If specified, a 2D or 3D mask preventing attention to certain positions. Must be of shape\n            :math:`(L, S)` or :math:`(N\\cdot\\text{num\\_heads}, L, S)`, where :math:`N` is the batch size,\n            :math:`L` is the target sequence length, and :math:`S` is the source sequence length. A 2D mask will be\n            broadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch.\n            Binary and float masks are supported. For a binary mask, a ``True`` value indicates that the\n            corresponding position is not allowed to attend. For a float mask, the mask values will be added to\n            the attention weight.\n            If both attn_mask and key_padding_mask are supplied, their types should match.\n        average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across\n            heads. Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an\n            effect when ``need_weights=True``. Default: ``True`` (i.e. average weights across heads)\n        is_causal: If specified, applies a causal mask as attention mask.\n            Default: ``False``.\n            Warning:\n            ``is_causal`` provides a hint that ``attn_mask`` is the\n            causal mask. Providing incorrect hints can result in\n            incorrect execution, including forward and backward\n            compatibility.\n\n    Outputs:\n        - **attn_output** - Attention outputs of shape :math:`(L, E)` when input is unbatched,\n          :math:`(L, N, E)` when ``batch_first=False`` or :math:`(N, L, E)` when ``batch_first=True``,\n          where :math:`L` is the target sequence length, :math:`N` is the batch size, and :math:`E` is the\n          embedding dimension ``embed_dim``.\n        - **attn_output_weights** - Only returned when ``need_weights=True``. If ``average_attn_weights=True``,\n          returns attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or\n          :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and\n          :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per\n          head of shape :math:`(\\text{num\\_heads}, L, S)` when input is unbatched or :math:`(N, \\text{num\\_heads}, L, S)`.\n\n        .. note::\n            `batch_first` argument is ignored for unbatched inputs.\n        \"\"\"\n\n        why_not_fast_path = ''\n        if ((attn_mask is not None and torch.is_floating_point(attn_mask))\n           or (key_padding_mask is not None) and torch.is_floating_point(key_padding_mask)):\n            why_not_fast_path = \"floating-point masks are not supported for fast path.\"\n\n        is_batched = query.dim() == 3\n\n        key_padding_mask = F._canonical_mask(\n            mask=key_padding_mask,\n            mask_name=\"key_padding_mask\",\n            other_type=F._none_or_dtype(attn_mask),\n            other_name=\"attn_mask\",\n            target_type=query.dtype\n        )\n\n        attn_mask = F._canonical_mask(\n            mask=attn_mask,\n            mask_name=\"attn_mask\",\n            other_type=None,\n            other_name=\"\",\n            target_type=query.dtype,\n            check_other=False,\n        )\n\n\n        if not is_batched:\n            why_not_fast_path = f\"input not batched; expected query.dim() of 3 but got {query.dim()}\"\n        elif query is not key or key is not value:\n            # When lifting this restriction, don't forget to either\n            # enforce that the dtypes all match or test cases where\n            # they don't!\n            why_not_fast_path = \"non-self attention was used (query, key, and value are not the same Tensor)\"\n        elif self.in_proj_bias is not None and query.dtype != self.in_proj_bias.dtype:\n            why_not_fast_path = f\"dtypes of query ({query.dtype}) and self.in_proj_bias ({self.in_proj_bias.dtype}) don't match\"\n        elif self.in_proj_weight is None:\n            why_not_fast_path = \"in_proj_weight was None\"\n        elif query.dtype != self.in_proj_weight.dtype:\n            # this case will fail anyway, but at least they'll get a useful error message.\n            why_not_fast_path = f\"dtypes of query ({query.dtype}) and self.in_proj_weight ({self.in_proj_weight.dtype}) don't match\"\n        elif self.training:\n            why_not_fast_path = \"training is enabled\"\n        elif (self.num_heads % 2) != 0:\n            why_not_fast_path = \"self.num_heads is not even\"\n        elif not self.batch_first:\n            why_not_fast_path = \"batch_first was not True\"\n        elif self.bias_k is not None:\n            why_not_fast_path = \"self.bias_k was not None\"\n        elif self.bias_v is not None:\n            why_not_fast_path = \"self.bias_v was not None\"\n        elif self.add_zero_attn:\n            why_not_fast_path = \"add_zero_attn was enabled\"\n        elif not self._qkv_same_embed_dim:\n            why_not_fast_path = \"_qkv_same_embed_dim was not True\"\n        elif query.is_nested and (key_padding_mask is not None or attn_mask is not None):\n            why_not_fast_path = \"supplying both src_key_padding_mask and src_mask at the same time \\\n                                 is not supported with NestedTensor input\"\n        elif torch.is_autocast_enabled():\n            why_not_fast_path = \"autocast is enabled\"\n\n        if not why_not_fast_path:\n            tensor_args = (\n                query,\n                key,\n                value,\n                self.in_proj_weight,\n                self.in_proj_bias,\n                self.out_proj.weight,\n                self.out_proj.bias,\n            )\n            # We have to use list comprehensions below because TorchScript does not support\n            # generator expressions.\n            if torch.overrides.has_torch_function(tensor_args):\n                why_not_fast_path = \"some Tensor argument has_torch_function\"\n            elif _is_make_fx_tracing():\n                why_not_fast_path = \"we are running make_fx tracing\"\n            elif not all(_check_arg_device(x) for x in tensor_args):\n                why_not_fast_path = (\"some Tensor argument's device is neither one of \"\n                                     f\"cpu, cuda or {torch.utils.backend_registration._privateuse1_backend_name}\")\n            elif torch.is_grad_enabled() and any(_arg_requires_grad(x) for x in tensor_args):\n                why_not_fast_path = (\"grad is enabled and at least one of query or the \"\n                                     \"input/output projection weights or biases requires_grad\")\n            if not why_not_fast_path:\n                merged_mask, mask_type = self.merge_masks(attn_mask, key_padding_mask, query)\n\n                if self.in_proj_bias is not None and self.in_proj_weight is not None:\n                    return torch._native_multi_head_attention(\n                        query,\n                        key,\n                        value,\n                        self.embed_dim,\n                        self.num_heads,\n                        self.in_proj_weight,\n                        self.in_proj_bias,\n                        self.out_proj.weight,\n                        self.out_proj.bias,\n                        merged_mask,\n                        need_weights,\n                        average_attn_weights,\n                        mask_type)\n\n        any_nested = query.is_nested or key.is_nested or value.is_nested\n        assert not any_nested, (\"MultiheadAttention does not support NestedTensor outside of its fast path. \" +\n                                f\"The fast path was not hit because {why_not_fast_path}\")\n\n        if self.batch_first and is_batched:\n            # make sure that the transpose op does not affect the \"is\" property\n            if key is value:\n                if query is key:\n                    query = key = value = query.transpose(1, 0)\n                else:\n                    query, key = (x.transpose(1, 0) for x in (query, key))\n                    value = key\n            else:\n                query, key, value = (x.transpose(1, 0) for x in (query, key, value))\n\n        if not self._qkv_same_embed_dim:\n            attn_output, attn_output_weights = F.multi_head_attention_forward(\n                query, key, value, self.embed_dim, self.num_heads,\n                self.in_proj_weight, self.in_proj_bias,\n                self.bias_k, self.bias_v, self.add_zero_attn,\n                self.dropout, self.out_proj.weight, self.out_proj.bias,\n                training=self.training,\n                key_padding_mask=key_padding_mask, need_weights=need_weights,\n                attn_mask=attn_mask,\n                use_separate_proj_weight=True,\n                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n                v_proj_weight=self.v_proj_weight,\n                average_attn_weights=average_attn_weights,\n                is_causal=is_causal)\n        else:\n            attn_output, attn_output_weights = F.multi_head_attention_forward(\n                query, key, value, self.embed_dim, self.num_heads,\n                self.in_proj_weight, self.in_proj_bias,\n                self.bias_k, self.bias_v, self.add_zero_attn,\n                self.dropout, self.out_proj.weight, self.out_proj.bias,\n                training=self.training,\n                key_padding_mask=key_padding_mask,\n                need_weights=need_weights,\n                attn_mask=attn_mask,\n                average_attn_weights=average_attn_weights,\n                is_causal=is_causal)\n        if self.batch_first and is_batched:\n            return attn_output.transpose(1, 0), attn_output_weights\n        else:\n            return attn_output, attn_output_weights\n\n    def merge_masks(self, attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor],\n                    query: Tensor) -> Tuple[Optional[Tensor], Optional[int]]:\n        r\"\"\"\n        Determine mask type and combine masks if necessary. If only one mask is provided, that mask\n        and the corresponding mask type will be returned. If both masks are provided, they will be both\n        expanded to shape ``(batch_size, num_heads, seq_len, seq_len)``, combined with logical ``or``\n        and mask type 2 will be returned\n        Args:\n            attn_mask: attention mask of shape ``(seq_len, seq_len)``, mask type 0\n            key_padding_mask: padding mask of shape ``(batch_size, seq_len)``, mask type 1\n            query: query embeddings of shape ``(batch_size, seq_len, embed_dim)``\n        Returns:\n            merged_mask: merged mask\n            mask_type: merged mask type (0, 1, or 2)\n        \"\"\"\n        mask_type: Optional[int] = None\n        merged_mask: Optional[Tensor] = None\n\n        if key_padding_mask is not None:\n            mask_type = 1\n            merged_mask = key_padding_mask\n\n        if attn_mask is not None:\n            # In this branch query can't be a nested tensor, so it has a shape\n            batch_size, seq_len, _ = query.shape\n            mask_type = 2\n\n            # Always expands attn_mask to 4D\n            if attn_mask.dim() == 3:\n                attn_mask_expanded = attn_mask.view(batch_size, -1, seq_len, seq_len)\n            else:  # attn_mask.dim() == 2:\n                attn_mask_expanded = attn_mask.view(1, 1, seq_len, seq_len).expand(batch_size, self.num_heads, -1, -1)\n            merged_mask = attn_mask_expanded\n\n            if key_padding_mask is not None:\n                key_padding_mask_expanded = key_padding_mask.view(batch_size, 1, 1, seq_len).expand(-1, self.num_heads, -1, -1)\n                merged_mask = attn_mask_expanded + key_padding_mask_expanded\n\n        # no attn_mask and no key_padding_mask, returns None, None\n        return merged_mask, mask_type\n\n\nclass PReLU(Module):\n    r\"\"\"Applies the element-wise function:\n\n    .. math::\n        \\text{PReLU}(x) = \\max(0,x) + a * \\min(0,x)\n\n    or\n\n    .. math::\n        \\text{PReLU}(x) =\n        \\begin{cases}\n        x, & \\text{ if } x \\geq 0 \\\\\n        ax, & \\text{ otherwise }\n        \\end{cases}\n\n    Here :math:`a` is a learnable parameter. When called without arguments, `nn.PReLU()` uses a single\n    parameter :math:`a` across all input channels. If called with `nn.PReLU(nChannels)`,\n    a separate :math:`a` is used for each input channel.\n\n\n    .. note::\n        weight decay should not be used when learning :math:`a` for good performance.\n\n    .. note::\n        Channel dim is the 2nd dim of input. When input has dims < 2, then there is\n        no channel dim and the number of channels = 1.\n\n    Args:\n        num_parameters (int): number of :math:`a` to learn.\n            Although it takes an int as input, there is only two values are legitimate:\n            1, or the number of channels at input. Default: 1\n        init (float): the initial value of :math:`a`. Default: 0.25\n\n    Shape:\n        - Input: :math:`( *)` where `*` means, any number of additional\n          dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    Attributes:\n        weight (Tensor): the learnable weights of shape (:attr:`num_parameters`).\n\n    .. image:: ../scripts/activation_images/PReLU.png\n\n    Examples::\n\n        >>> m = nn.PReLU()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n    __constants__ = ['num_parameters']\n    num_parameters: int\n\n    def __init__(self, num_parameters: int = 1, init: float = 0.25,\n                 device=None, dtype=None) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        self.num_parameters = num_parameters\n        super().__init__()\n        self.init = init\n        self.weight = Parameter(torch.empty(num_parameters, **factory_kwargs))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        torch.nn.init.constant_(self.weight, self.init)\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.prelu(input, self.weight)\n\n    def extra_repr(self) -> str:\n        return f'num_parameters={self.num_parameters}'\n\n\nclass Softsign(Module):\n    r\"\"\"Applies the element-wise function:\n\n    .. math::\n        \\text{SoftSign}(x) = \\frac{x}{ 1 + |x|}\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Softsign.png\n\n    Examples::\n\n        >>> m = nn.Softsign()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.softsign(input)\n\n\nclass Tanhshrink(Module):\n    r\"\"\"Applies the element-wise function:\n\n    .. math::\n        \\text{Tanhshrink}(x) = x - \\tanh(x)\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    .. image:: ../scripts/activation_images/Tanhshrink.png\n\n    Examples::\n\n        >>> m = nn.Tanhshrink()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    \"\"\"\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.tanhshrink(input)\n\n\nclass Softmin(Module):\n    r\"\"\"Applies the Softmin function to an n-dimensional input Tensor\n    rescaling them so that the elements of the n-dimensional output Tensor\n    lie in the range `[0, 1]` and sum to 1.\n\n    Softmin is defined as:\n\n    .. math::\n        \\text{Softmin}(x_{i}) = \\frac{\\exp(-x_i)}{\\sum_j \\exp(-x_j)}\n\n    Shape:\n        - Input: :math:`(*)` where `*` means, any number of additional\n          dimensions\n        - Output: :math:`(*)`, same shape as the input\n\n    Args:\n        dim (int): A dimension along which Softmin will be computed (so every slice\n            along dim will sum to 1).\n\n    Returns:\n        a Tensor of the same dimension and shape as the input, with\n        values in the range [0, 1]\n\n    Examples::\n\n        >>> m = nn.Softmin(dim=1)\n        >>> input = torch.randn(2, 3)\n        >>> output = m(input)\n    \"\"\"\n    __constants__ = ['dim']\n    dim: Optional[int]\n\n    def __init__(self, dim: Optional[int] = None) -> None:\n        super().__init__()\n        self.dim = dim\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n        if not hasattr(self, 'dim'):\n            self.dim = None\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.softmin(input, self.dim, _stacklevel=5)\n\n    def extra_repr(self):\n        return f'dim={self.dim}'\n\nclass Softmax(Module):\n    r\"\"\"Applies the Softmax function to an n-dimensional input Tensor\n    rescaling them so that the elements of the n-dimensional output Tensor\n    lie in the range [0,1] and sum to 1.\n\n    Softmax is defined as:\n\n    .. math::\n        \\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n\n    When the input Tensor is a sparse tensor then the unspecified\n    values are treated as ``-inf``.\n\n    Shape:\n        - Input: :math:`(*)` where `*` means, any number of additional\n          dimensions\n        - Output: :math:`(*)`, same shape as the input\n\n    Returns:\n        a Tensor of the same dimension and shape as the input with\n        values in the range [0, 1]\n\n    Args:\n        dim (int): A dimension along which Softmax will be computed (so every slice\n            along dim will sum to 1).\n\n    .. note::\n        This module doesn't work directly with NLLLoss,\n        which expects the Log to be computed between the Softmax and itself.\n        Use `LogSoftmax` instead (it's faster and has better numerical properties).\n\n    Examples::\n\n        >>> m = nn.Softmax(dim=1)\n        >>> input = torch.randn(2, 3)\n        >>> output = m(input)\n\n    \"\"\"\n    __constants__ = ['dim']\n    dim: Optional[int]\n\n    def __init__(self, dim: Optional[int] = None) -> None:\n        super().__init__()\n        self.dim = dim\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n        if not hasattr(self, 'dim'):\n            self.dim = None\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.softmax(input, self.dim, _stacklevel=5)\n\n    def extra_repr(self) -> str:\n        return f'dim={self.dim}'\n\n\nclass Softmax2d(Module):\n    r\"\"\"Applies SoftMax over features to each spatial location.\n\n    When given an image of ``Channels x Height x Width``, it will\n    apply `Softmax` to each location :math:`(Channels, h_i, w_j)`\n\n    Shape:\n        - Input: :math:`(N, C, H, W)` or :math:`(C, H, W)`.\n        - Output: :math:`(N, C, H, W)` or :math:`(C, H, W)` (same shape as input)\n\n    Returns:\n        a Tensor of the same dimension and shape as the input with\n        values in the range [0, 1]\n\n    Examples::\n\n        >>> m = nn.Softmax2d()\n        >>> # you softmax over the 2nd dimension\n        >>> input = torch.randn(2, 3, 12, 13)\n        >>> output = m(input)\n    \"\"\"\n\n    def forward(self, input: Tensor) -> Tensor:\n        if input.dim() not in (3, 4):\n            raise ValueError(\n                f\"Softmax2d: expected input to be 3D or 4D, got {input.dim()}D instead\"\n            )\n        return F.softmax(input, -3, _stacklevel=5)\n\n\nclass LogSoftmax(Module):\n    r\"\"\"Applies the :math:`\\log(\\text{Softmax}(x))` function to an n-dimensional\n    input Tensor. The LogSoftmax formulation can be simplified as:\n\n    .. math::\n        \\text{LogSoftmax}(x_{i}) = \\log\\left(\\frac{\\exp(x_i) }{ \\sum_j \\exp(x_j)} \\right)\n\n    Shape:\n        - Input: :math:`(*)` where `*` means, any number of additional\n          dimensions\n        - Output: :math:`(*)`, same shape as the input\n\n    Args:\n        dim (int): A dimension along which LogSoftmax will be computed.\n\n    Returns:\n        a Tensor of the same dimension and shape as the input with\n        values in the range [-inf, 0)\n\n    Examples::\n\n        >>> m = nn.LogSoftmax(dim=1)\n        >>> input = torch.randn(2, 3)\n        >>> output = m(input)\n    \"\"\"\n    __constants__ = ['dim']\n    dim: Optional[int]\n\n    def __init__(self, dim: Optional[int] = None) -> None:\n        super().__init__()\n        self.dim = dim\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n        if not hasattr(self, 'dim'):\n            self.dim = None\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.log_softmax(input, self.dim, _stacklevel=5)\n\n    def extra_repr(self):\n        return f'dim={self.dim}'\n", 1591], "C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py": ["from typing import List, Optional\n\nfrom torch import Tensor\nfrom .module import Module\nfrom .utils import _single, _pair, _triple\nfrom .. import functional as F\n\nfrom ..common_types import (_size_any_t, _size_1_t, _size_2_t, _size_3_t,\n                            _ratio_3_t, _ratio_2_t, _size_any_opt_t, _size_2_opt_t, _size_3_opt_t)\n\n__all__ = ['MaxPool1d', 'MaxPool2d', 'MaxPool3d', 'MaxUnpool1d', 'MaxUnpool2d', 'MaxUnpool3d',\n           'AvgPool1d', 'AvgPool2d', 'AvgPool3d', 'FractionalMaxPool2d', 'FractionalMaxPool3d', 'LPPool1d',\n           'LPPool2d', 'AdaptiveMaxPool1d', 'AdaptiveMaxPool2d', 'AdaptiveMaxPool3d', 'AdaptiveAvgPool1d',\n           'AdaptiveAvgPool2d', 'AdaptiveAvgPool3d']\n\nclass _MaxPoolNd(Module):\n    __constants__ = ['kernel_size', 'stride', 'padding', 'dilation',\n                     'return_indices', 'ceil_mode']\n    return_indices: bool\n    ceil_mode: bool\n\n    def __init__(self, kernel_size: _size_any_t, stride: Optional[_size_any_t] = None,\n                 padding: _size_any_t = 0, dilation: _size_any_t = 1,\n                 return_indices: bool = False, ceil_mode: bool = False) -> None:\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if (stride is not None) else kernel_size\n        self.padding = padding\n        self.dilation = dilation\n        self.return_indices = return_indices\n        self.ceil_mode = ceil_mode\n\n    def extra_repr(self) -> str:\n        return 'kernel_size={kernel_size}, stride={stride}, padding={padding}' \\\n            ', dilation={dilation}, ceil_mode={ceil_mode}'.format(**self.__dict__)\n\n\nclass MaxPool1d(_MaxPoolNd):\n    r\"\"\"Applies a 1D max pooling over an input signal composed of several input planes.\n\n    In the simplest case, the output value of the layer with input size :math:`(N, C, L)`\n    and output :math:`(N, C, L_{out})` can be precisely described as:\n\n    .. math::\n        out(N_i, C_j, k) = \\max_{m=0, \\ldots, \\text{kernel\\_size} - 1}\n                input(N_i, C_j, stride \\times k + m)\n\n    If :attr:`padding` is non-zero, then the input is implicitly padded with negative infinity on both sides\n    for :attr:`padding` number of points. :attr:`dilation` is the stride between the elements within the\n    sliding window. This `link`_ has a nice visualization of the pooling parameters.\n\n    Note:\n        When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding\n        or the input. Sliding windows that would start in the right padded region are ignored.\n\n    Args:\n        kernel_size: The size of the sliding window, must be > 0.\n        stride: The stride of the sliding window, must be > 0. Default value is :attr:`kernel_size`.\n        padding: Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.\n        dilation: The stride between elements within a sliding window, must be > 0.\n        return_indices: If ``True``, will return the argmax along with the max values.\n                        Useful for :class:`torch.nn.MaxUnpool1d` later\n        ceil_mode: If ``True``, will use `ceil` instead of `floor` to compute the output shape. This\n                   ensures that every element in the input tensor is covered by a sliding window.\n\n    Shape:\n        - Input: :math:`(N, C, L_{in})` or :math:`(C, L_{in})`.\n        - Output: :math:`(N, C, L_{out})` or :math:`(C, L_{out})`, where\n\n          .. math::\n              L_{out} = \\left\\lfloor \\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation}\n                    \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor\n\n    Examples::\n\n        >>> # pool of size=3, stride=2\n        >>> m = nn.MaxPool1d(3, stride=2)\n        >>> input = torch.randn(20, 16, 50)\n        >>> output = m(input)\n\n    .. _link:\n        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n    \"\"\"\n\n    kernel_size: _size_1_t\n    stride: _size_1_t\n    padding: _size_1_t\n    dilation: _size_1_t\n\n    def forward(self, input: Tensor):\n        return F.max_pool1d(input, self.kernel_size, self.stride,\n                            self.padding, self.dilation, ceil_mode=self.ceil_mode,\n                            return_indices=self.return_indices)\n\n\nclass MaxPool2d(_MaxPoolNd):\n    r\"\"\"Applies a 2D max pooling over an input signal composed of several input planes.\n\n    In the simplest case, the output value of the layer with input size :math:`(N, C, H, W)`,\n    output :math:`(N, C, H_{out}, W_{out})` and :attr:`kernel_size` :math:`(kH, kW)`\n    can be precisely described as:\n\n    .. math::\n        \\begin{aligned}\n            out(N_i, C_j, h, w) ={} & \\max_{m=0, \\ldots, kH-1} \\max_{n=0, \\ldots, kW-1} \\\\\n                                    & \\text{input}(N_i, C_j, \\text{stride[0]} \\times h + m,\n                                                   \\text{stride[1]} \\times w + n)\n        \\end{aligned}\n\n    If :attr:`padding` is non-zero, then the input is implicitly padded with negative infinity on both sides\n    for :attr:`padding` number of points. :attr:`dilation` controls the spacing between the kernel points.\n    It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.\n\n    Note:\n        When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding\n        or the input. Sliding windows that would start in the right padded region are ignored.\n\n    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:\n\n        - a single ``int`` -- in which case the same value is used for the height and width dimension\n        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,\n          and the second `int` for the width dimension\n\n    Args:\n        kernel_size: the size of the window to take a max over\n        stride: the stride of the window. Default value is :attr:`kernel_size`\n        padding: Implicit negative infinity padding to be added on both sides\n        dilation: a parameter that controls the stride of elements in the window\n        return_indices: if ``True``, will return the max indices along with the outputs.\n                        Useful for :class:`torch.nn.MaxUnpool2d` later\n        ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape\n\n    Shape:\n        - Input: :math:`(N, C, H_{in}, W_{in})` or :math:`(C, H_{in}, W_{in})`\n        - Output: :math:`(N, C, H_{out}, W_{out})` or :math:`(C, H_{out}, W_{out})`, where\n\n          .. math::\n              H_{out} = \\left\\lfloor\\frac{H_{in} + 2 * \\text{padding[0]} - \\text{dilation[0]}\n                    \\times (\\text{kernel\\_size[0]} - 1) - 1}{\\text{stride[0]}} + 1\\right\\rfloor\n\n          .. math::\n              W_{out} = \\left\\lfloor\\frac{W_{in} + 2 * \\text{padding[1]} - \\text{dilation[1]}\n                    \\times (\\text{kernel\\_size[1]} - 1) - 1}{\\text{stride[1]}} + 1\\right\\rfloor\n\n    Examples::\n\n        >>> # pool of square window of size=3, stride=2\n        >>> m = nn.MaxPool2d(3, stride=2)\n        >>> # pool of non-square window\n        >>> m = nn.MaxPool2d((3, 2), stride=(2, 1))\n        >>> input = torch.randn(20, 16, 50, 32)\n        >>> output = m(input)\n\n    .. _link:\n        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n    \"\"\"\n\n    kernel_size: _size_2_t\n    stride: _size_2_t\n    padding: _size_2_t\n    dilation: _size_2_t\n\n    def forward(self, input: Tensor):\n        return F.max_pool2d(input, self.kernel_size, self.stride,\n                            self.padding, self.dilation, ceil_mode=self.ceil_mode,\n                            return_indices=self.return_indices)\n\n\nclass MaxPool3d(_MaxPoolNd):\n    r\"\"\"Applies a 3D max pooling over an input signal composed of several input planes.\n\n    In the simplest case, the output value of the layer with input size :math:`(N, C, D, H, W)`,\n    output :math:`(N, C, D_{out}, H_{out}, W_{out})` and :attr:`kernel_size` :math:`(kD, kH, kW)`\n    can be precisely described as:\n\n    .. math::\n        \\begin{aligned}\n            \\text{out}(N_i, C_j, d, h, w) ={} & \\max_{k=0, \\ldots, kD-1} \\max_{m=0, \\ldots, kH-1} \\max_{n=0, \\ldots, kW-1} \\\\\n                                              & \\text{input}(N_i, C_j, \\text{stride[0]} \\times d + k,\n                                                             \\text{stride[1]} \\times h + m, \\text{stride[2]} \\times w + n)\n        \\end{aligned}\n\n    If :attr:`padding` is non-zero, then the input is implicitly padded with negative infinity on both sides\n    for :attr:`padding` number of points. :attr:`dilation` controls the spacing between the kernel points.\n    It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.\n\n    Note:\n        When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding\n        or the input. Sliding windows that would start in the right padded region are ignored.\n\n    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:\n\n        - a single ``int`` -- in which case the same value is used for the depth, height and width dimension\n        - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension,\n          the second `int` for the height dimension and the third `int` for the width dimension\n\n    Args:\n        kernel_size: the size of the window to take a max over\n        stride: the stride of the window. Default value is :attr:`kernel_size`\n        padding: Implicit negative infinity padding to be added on all three sides\n        dilation: a parameter that controls the stride of elements in the window\n        return_indices: if ``True``, will return the max indices along with the outputs.\n                        Useful for :class:`torch.nn.MaxUnpool3d` later\n        ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape\n\n    Shape:\n        - Input: :math:`(N, C, D_{in}, H_{in}, W_{in})` or :math:`(C, D_{in}, H_{in}, W_{in})`.\n        - Output: :math:`(N, C, D_{out}, H_{out}, W_{out})` or :math:`(C, D_{out}, H_{out}, W_{out})`, where\n\n          .. math::\n              D_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0] \\times\n                (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n\n          .. math::\n              H_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] - \\text{dilation}[1] \\times\n                (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n\n          .. math::\n              W_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] - \\text{dilation}[2] \\times\n                (\\text{kernel\\_size}[2] - 1) - 1}{\\text{stride}[2]} + 1\\right\\rfloor\n\n    Examples::\n\n        >>> # pool of square window of size=3, stride=2\n        >>> m = nn.MaxPool3d(3, stride=2)\n        >>> # pool of non-square window\n        >>> m = nn.MaxPool3d((3, 2, 2), stride=(2, 1, 2))\n        >>> input = torch.randn(20, 16, 50, 44, 31)\n        >>> output = m(input)\n\n    .. _link:\n        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n    \"\"\"  # noqa: E501\n\n    kernel_size: _size_3_t\n    stride: _size_3_t\n    padding: _size_3_t\n    dilation: _size_3_t\n\n    def forward(self, input: Tensor):\n        return F.max_pool3d(input, self.kernel_size, self.stride,\n                            self.padding, self.dilation, ceil_mode=self.ceil_mode,\n                            return_indices=self.return_indices)\n\n\nclass _MaxUnpoolNd(Module):\n\n    def extra_repr(self) -> str:\n        return f'kernel_size={self.kernel_size}, stride={self.stride}, padding={self.padding}'\n\n\nclass MaxUnpool1d(_MaxUnpoolNd):\n    r\"\"\"Computes a partial inverse of :class:`MaxPool1d`.\n\n    :class:`MaxPool1d` is not fully invertible, since the non-maximal values are lost.\n\n    :class:`MaxUnpool1d` takes in as input the output of :class:`MaxPool1d`\n    including the indices of the maximal values and computes a partial inverse\n    in which all non-maximal values are set to zero.\n\n    Note:\n        This operation may behave nondeterministically when the input indices has repeat values.\n        See https://github.com/pytorch/pytorch/issues/80827 and :doc:`/notes/randomness` for more information.\n\n    .. note:: :class:`MaxPool1d` can map several input sizes to the same output\n              sizes. Hence, the inversion process can get ambiguous.\n              To accommodate this, you can provide the needed output size\n              as an additional argument :attr:`output_size` in the forward call.\n              See the Inputs and Example below.\n\n    Args:\n        kernel_size (int or tuple): Size of the max pooling window.\n        stride (int or tuple): Stride of the max pooling window.\n            It is set to :attr:`kernel_size` by default.\n        padding (int or tuple): Padding that was added to the input\n\n    Inputs:\n        - `input`: the input Tensor to invert\n        - `indices`: the indices given out by :class:`~torch.nn.MaxPool1d`\n        - `output_size` (optional): the targeted output size\n\n    Shape:\n        - Input: :math:`(N, C, H_{in})` or :math:`(C, H_{in})`.\n        - Output: :math:`(N, C, H_{out})` or :math:`(C, H_{out})`, where\n\n          .. math::\n              H_{out} = (H_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{kernel\\_size}[0]\n\n          or as given by :attr:`output_size` in the call operator\n\n    Example::\n\n        >>> # xdoctest: +IGNORE_WANT(\"do other tests modify the global state?\")\n        >>> pool = nn.MaxPool1d(2, stride=2, return_indices=True)\n        >>> unpool = nn.MaxUnpool1d(2, stride=2)\n        >>> input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8]]])\n        >>> output, indices = pool(input)\n        >>> unpool(output, indices)\n        tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])\n\n        >>> # Example showcasing the use of output_size\n        >>> input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8, 9]]])\n        >>> output, indices = pool(input)\n        >>> unpool(output, indices, output_size=input.size())\n        tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.,  0.]]])\n\n        >>> unpool(output, indices)\n        tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])\n    \"\"\"\n\n    kernel_size: _size_1_t\n    stride: _size_1_t\n    padding: _size_1_t\n\n    def __init__(self, kernel_size: _size_1_t, stride: Optional[_size_1_t] = None, padding: _size_1_t = 0) -> None:\n        super().__init__()\n        self.kernel_size = _single(kernel_size)\n        self.stride = _single(stride if (stride is not None) else kernel_size)\n        self.padding = _single(padding)\n\n    def forward(self, input: Tensor, indices: Tensor, output_size: Optional[List[int]] = None) -> Tensor:\n        return F.max_unpool1d(input, indices, self.kernel_size, self.stride,\n                              self.padding, output_size)\n\n\nclass MaxUnpool2d(_MaxUnpoolNd):\n    r\"\"\"Computes a partial inverse of :class:`MaxPool2d`.\n\n    :class:`MaxPool2d` is not fully invertible, since the non-maximal values are lost.\n\n    :class:`MaxUnpool2d` takes in as input the output of :class:`MaxPool2d`\n    including the indices of the maximal values and computes a partial inverse\n    in which all non-maximal values are set to zero.\n\n    Note:\n        This operation may behave nondeterministically when the input indices has repeat values.\n        See https://github.com/pytorch/pytorch/issues/80827 and :doc:`/notes/randomness` for more information.\n\n    .. note:: :class:`MaxPool2d` can map several input sizes to the same output\n              sizes. Hence, the inversion process can get ambiguous.\n              To accommodate this, you can provide the needed output size\n              as an additional argument :attr:`output_size` in the forward call.\n              See the Inputs and Example below.\n\n    Args:\n        kernel_size (int or tuple): Size of the max pooling window.\n        stride (int or tuple): Stride of the max pooling window.\n            It is set to :attr:`kernel_size` by default.\n        padding (int or tuple): Padding that was added to the input\n\n    Inputs:\n        - `input`: the input Tensor to invert\n        - `indices`: the indices given out by :class:`~torch.nn.MaxPool2d`\n        - `output_size` (optional): the targeted output size\n\n    Shape:\n        - Input: :math:`(N, C, H_{in}, W_{in})` or :math:`(C, H_{in}, W_{in})`.\n        - Output: :math:`(N, C, H_{out}, W_{out})` or :math:`(C, H_{out}, W_{out})`, where\n\n          .. math::\n            H_{out} = (H_{in} - 1) \\times \\text{stride[0]} - 2 \\times \\text{padding[0]} + \\text{kernel\\_size[0]}\n\n          .. math::\n            W_{out} = (W_{in} - 1) \\times \\text{stride[1]} - 2 \\times \\text{padding[1]} + \\text{kernel\\_size[1]}\n\n          or as given by :attr:`output_size` in the call operator\n\n    Example::\n\n        >>> pool = nn.MaxPool2d(2, stride=2, return_indices=True)\n        >>> unpool = nn.MaxUnpool2d(2, stride=2)\n        >>> input = torch.tensor([[[[ 1.,  2.,  3.,  4.],\n                                    [ 5.,  6.,  7.,  8.],\n                                    [ 9., 10., 11., 12.],\n                                    [13., 14., 15., 16.]]]])\n        >>> output, indices = pool(input)\n        >>> unpool(output, indices)\n        tensor([[[[  0.,   0.,   0.,   0.],\n                  [  0.,   6.,   0.,   8.],\n                  [  0.,   0.,   0.,   0.],\n                  [  0.,  14.,   0.,  16.]]]])\n        >>> # Now using output_size to resolve an ambiguous size for the inverse\n        >>> input = torch.torch.tensor([[[[ 1.,  2.,  3., 4., 5.],\n                                          [ 6.,  7.,  8., 9., 10.],\n                                          [11., 12., 13., 14., 15.],\n                                          [16., 17., 18., 19., 20.]]]])\n        >>> output, indices = pool(input)\n        >>> # This call will not work without specifying output_size\n        >>> unpool(output, indices, output_size=input.size())\n        tensor([[[[ 0.,  0.,  0.,  0.,  0.],\n                  [ 0.,  7.,  0.,  9.,  0.],\n                  [ 0.,  0.,  0.,  0.,  0.],\n                  [ 0., 17.,  0., 19.,  0.]]]])\n\n\n    \"\"\"\n\n    kernel_size: _size_2_t\n    stride: _size_2_t\n    padding: _size_2_t\n\n    def __init__(self, kernel_size: _size_2_t, stride: Optional[_size_2_t] = None, padding: _size_2_t = 0) -> None:\n        super().__init__()\n        self.kernel_size = _pair(kernel_size)\n        self.stride = _pair(stride if (stride is not None) else kernel_size)\n        self.padding = _pair(padding)\n\n    def forward(self, input: Tensor, indices: Tensor, output_size: Optional[List[int]] = None) -> Tensor:\n        return F.max_unpool2d(input, indices, self.kernel_size, self.stride,\n                              self.padding, output_size)\n\n\nclass MaxUnpool3d(_MaxUnpoolNd):\n    r\"\"\"Computes a partial inverse of :class:`MaxPool3d`.\n\n    :class:`MaxPool3d` is not fully invertible, since the non-maximal values are lost.\n    :class:`MaxUnpool3d` takes in as input the output of :class:`MaxPool3d`\n    including the indices of the maximal values and computes a partial inverse\n    in which all non-maximal values are set to zero.\n\n    Note:\n        This operation may behave nondeterministically when the input indices has repeat values.\n        See https://github.com/pytorch/pytorch/issues/80827 and :doc:`/notes/randomness` for more information.\n\n    .. note:: :class:`MaxPool3d` can map several input sizes to the same output\n              sizes. Hence, the inversion process can get ambiguous.\n              To accommodate this, you can provide the needed output size\n              as an additional argument :attr:`output_size` in the forward call.\n              See the Inputs section below.\n\n    Args:\n        kernel_size (int or tuple): Size of the max pooling window.\n        stride (int or tuple): Stride of the max pooling window.\n            It is set to :attr:`kernel_size` by default.\n        padding (int or tuple): Padding that was added to the input\n\n    Inputs:\n        - `input`: the input Tensor to invert\n        - `indices`: the indices given out by :class:`~torch.nn.MaxPool3d`\n        - `output_size` (optional): the targeted output size\n\n    Shape:\n        - Input: :math:`(N, C, D_{in}, H_{in}, W_{in})` or :math:`(C, D_{in}, H_{in}, W_{in})`.\n        - Output: :math:`(N, C, D_{out}, H_{out}, W_{out})` or :math:`(C, D_{out}, H_{out}, W_{out})`, where\n\n          .. math::\n              D_{out} = (D_{in} - 1) \\times \\text{stride[0]} - 2 \\times \\text{padding[0]} + \\text{kernel\\_size[0]}\n\n          .. math::\n              H_{out} = (H_{in} - 1) \\times \\text{stride[1]} - 2 \\times \\text{padding[1]} + \\text{kernel\\_size[1]}\n\n          .. math::\n              W_{out} = (W_{in} - 1) \\times \\text{stride[2]} - 2 \\times \\text{padding[2]} + \\text{kernel\\_size[2]}\n\n          or as given by :attr:`output_size` in the call operator\n\n    Example::\n\n        >>> # pool of square window of size=3, stride=2\n        >>> pool = nn.MaxPool3d(3, stride=2, return_indices=True)\n        >>> unpool = nn.MaxUnpool3d(3, stride=2)\n        >>> output, indices = pool(torch.randn(20, 16, 51, 33, 15))\n        >>> unpooled_output = unpool(output, indices)\n        >>> unpooled_output.size()\n        torch.Size([20, 16, 51, 33, 15])\n    \"\"\"\n\n    kernel_size: _size_3_t\n    stride: _size_3_t\n    padding: _size_3_t\n\n    def __init__(self, kernel_size: _size_3_t, stride: Optional[_size_3_t] = None, padding: _size_3_t = 0) -> None:\n        super().__init__()\n        self.kernel_size = _triple(kernel_size)\n        self.stride = _triple(stride if (stride is not None) else kernel_size)\n        self.padding = _triple(padding)\n\n    def forward(self, input: Tensor, indices: Tensor, output_size: Optional[List[int]] = None) -> Tensor:\n        return F.max_unpool3d(input, indices, self.kernel_size, self.stride,\n                              self.padding, output_size)\n\n\nclass _AvgPoolNd(Module):\n    __constants__ = ['kernel_size', 'stride', 'padding', 'ceil_mode', 'count_include_pad']\n\n    def extra_repr(self) -> str:\n        return f'kernel_size={self.kernel_size}, stride={self.stride}, padding={self.padding}'\n\n\nclass AvgPool1d(_AvgPoolNd):\n    r\"\"\"Applies a 1D average pooling over an input signal composed of several input planes.\n\n    In the simplest case, the output value of the layer with input size :math:`(N, C, L)`,\n    output :math:`(N, C, L_{out})` and :attr:`kernel_size` :math:`k`\n    can be precisely described as:\n\n    .. math::\n\n        \\text{out}(N_i, C_j, l) = \\frac{1}{k} \\sum_{m=0}^{k-1}\n                               \\text{input}(N_i, C_j, \\text{stride} \\times l + m)\n\n    If :attr:`padding` is non-zero, then the input is implicitly zero-padded on both sides\n    for :attr:`padding` number of points.\n\n    Note:\n        When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding\n        or the input. Sliding windows that would start in the right padded region are ignored.\n\n    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding` can each be\n    an ``int`` or a one-element tuple.\n\n    Args:\n        kernel_size: the size of the window\n        stride: the stride of the window. Default value is :attr:`kernel_size`\n        padding: implicit zero padding to be added on both sides\n        ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape\n        count_include_pad: when True, will include the zero-padding in the averaging calculation\n\n    Shape:\n        - Input: :math:`(N, C, L_{in})` or :math:`(C, L_{in})`.\n        - Output: :math:`(N, C, L_{out})` or :math:`(C, L_{out})`, where\n\n          .. math::\n              L_{out} = \\left\\lfloor \\frac{L_{in} +\n              2 \\times \\text{padding} - \\text{kernel\\_size}}{\\text{stride}} + 1\\right\\rfloor\n\n    Examples::\n\n        >>> # pool with window of size=3, stride=2\n        >>> m = nn.AvgPool1d(3, stride=2)\n        >>> m(torch.tensor([[[1., 2, 3, 4, 5, 6, 7]]]))\n        tensor([[[2., 4., 6.]]])\n    \"\"\"\n\n    kernel_size: _size_1_t\n    stride: _size_1_t\n    padding: _size_1_t\n    ceil_mode: bool\n    count_include_pad: bool\n\n    def __init__(self, kernel_size: _size_1_t, stride: _size_1_t = None, padding: _size_1_t = 0, ceil_mode: bool = False,\n                 count_include_pad: bool = True) -> None:\n        super().__init__()\n        self.kernel_size = _single(kernel_size)\n        self.stride = _single(stride if stride is not None else kernel_size)\n        self.padding = _single(padding)\n        self.ceil_mode = ceil_mode\n        self.count_include_pad = count_include_pad\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.avg_pool1d(\n            input, self.kernel_size, self.stride, self.padding, self.ceil_mode,\n            self.count_include_pad)\n\n\nclass AvgPool2d(_AvgPoolNd):\n    r\"\"\"Applies a 2D average pooling over an input signal composed of several input planes.\n\n    In the simplest case, the output value of the layer with input size :math:`(N, C, H, W)`,\n    output :math:`(N, C, H_{out}, W_{out})` and :attr:`kernel_size` :math:`(kH, kW)`\n    can be precisely described as:\n\n    .. math::\n\n        out(N_i, C_j, h, w)  = \\frac{1}{kH * kW} \\sum_{m=0}^{kH-1} \\sum_{n=0}^{kW-1}\n                               input(N_i, C_j, stride[0] \\times h + m, stride[1] \\times w + n)\n\n    If :attr:`padding` is non-zero, then the input is implicitly zero-padded on both sides\n    for :attr:`padding` number of points.\n\n    Note:\n        When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding\n        or the input. Sliding windows that would start in the right padded region are ignored.\n\n    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding` can either be:\n\n        - a single ``int`` -- in which case the same value is used for the height and width dimension\n        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,\n          and the second `int` for the width dimension\n\n    Args:\n        kernel_size: the size of the window\n        stride: the stride of the window. Default value is :attr:`kernel_size`\n        padding: implicit zero padding to be added on both sides\n        ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape\n        count_include_pad: when True, will include the zero-padding in the averaging calculation\n        divisor_override: if specified, it will be used as divisor, otherwise size of the pooling region will be used.\n\n\n    Shape:\n        - Input: :math:`(N, C, H_{in}, W_{in})` or :math:`(C, H_{in}, W_{in})`.\n        - Output: :math:`(N, C, H_{out}, W_{out})` or :math:`(C, H_{out}, W_{out})`, where\n\n          .. math::\n              H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] -\n                \\text{kernel\\_size}[0]}{\\text{stride}[0]} + 1\\right\\rfloor\n\n          .. math::\n              W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] -\n                \\text{kernel\\_size}[1]}{\\text{stride}[1]} + 1\\right\\rfloor\n\n    Examples::\n\n        >>> # pool of square window of size=3, stride=2\n        >>> m = nn.AvgPool2d(3, stride=2)\n        >>> # pool of non-square window\n        >>> m = nn.AvgPool2d((3, 2), stride=(2, 1))\n        >>> input = torch.randn(20, 16, 50, 32)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = ['kernel_size', 'stride', 'padding', 'ceil_mode', 'count_include_pad', 'divisor_override']\n\n    kernel_size: _size_2_t\n    stride: _size_2_t\n    padding: _size_2_t\n    ceil_mode: bool\n    count_include_pad: bool\n\n    def __init__(self, kernel_size: _size_2_t, stride: Optional[_size_2_t] = None, padding: _size_2_t = 0,\n                 ceil_mode: bool = False, count_include_pad: bool = True, divisor_override: Optional[int] = None) -> None:\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if (stride is not None) else kernel_size\n        self.padding = padding\n        self.ceil_mode = ceil_mode\n        self.count_include_pad = count_include_pad\n        self.divisor_override = divisor_override\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.avg_pool2d(input, self.kernel_size, self.stride,\n                            self.padding, self.ceil_mode, self.count_include_pad, self.divisor_override)\n\n\nclass AvgPool3d(_AvgPoolNd):\n    r\"\"\"Applies a 3D average pooling over an input signal composed of several input planes.\n\n    In the simplest case, the output value of the layer with input size :math:`(N, C, D, H, W)`,\n    output :math:`(N, C, D_{out}, H_{out}, W_{out})` and :attr:`kernel_size` :math:`(kD, kH, kW)`\n    can be precisely described as:\n\n    .. math::\n        \\begin{aligned}\n            \\text{out}(N_i, C_j, d, h, w) ={} & \\sum_{k=0}^{kD-1} \\sum_{m=0}^{kH-1} \\sum_{n=0}^{kW-1} \\\\\n                                              & \\frac{\\text{input}(N_i, C_j, \\text{stride}[0] \\times d + k,\n                                                      \\text{stride}[1] \\times h + m, \\text{stride}[2] \\times w + n)}\n                                                     {kD \\times kH \\times kW}\n        \\end{aligned}\n\n    If :attr:`padding` is non-zero, then the input is implicitly zero-padded on all three sides\n    for :attr:`padding` number of points.\n\n    Note:\n        When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding\n        or the input. Sliding windows that would start in the right padded region are ignored.\n\n    The parameters :attr:`kernel_size`, :attr:`stride` can either be:\n\n        - a single ``int`` -- in which case the same value is used for the depth, height and width dimension\n        - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension,\n          the second `int` for the height dimension and the third `int` for the width dimension\n\n    Args:\n        kernel_size: the size of the window\n        stride: the stride of the window. Default value is :attr:`kernel_size`\n        padding: implicit zero padding to be added on all three sides\n        ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape\n        count_include_pad: when True, will include the zero-padding in the averaging calculation\n        divisor_override: if specified, it will be used as divisor, otherwise :attr:`kernel_size` will be used\n\n    Shape:\n        - Input: :math:`(N, C, D_{in}, H_{in}, W_{in})` or :math:`(C, D_{in}, H_{in}, W_{in})`.\n        - Output: :math:`(N, C, D_{out}, H_{out}, W_{out})` or\n          :math:`(C, D_{out}, H_{out}, W_{out})`, where\n\n          .. math::\n              D_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] -\n                    \\text{kernel\\_size}[0]}{\\text{stride}[0]} + 1\\right\\rfloor\n\n          .. math::\n              H_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] -\n                    \\text{kernel\\_size}[1]}{\\text{stride}[1]} + 1\\right\\rfloor\n\n          .. math::\n              W_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] -\n                    \\text{kernel\\_size}[2]}{\\text{stride}[2]} + 1\\right\\rfloor\n\n    Examples::\n\n        >>> # pool of square window of size=3, stride=2\n        >>> m = nn.AvgPool3d(3, stride=2)\n        >>> # pool of non-square window\n        >>> m = nn.AvgPool3d((3, 2, 2), stride=(2, 1, 2))\n        >>> input = torch.randn(20, 16, 50, 44, 31)\n        >>> output = m(input)\n    \"\"\"\n\n    __constants__ = ['kernel_size', 'stride', 'padding', 'ceil_mode', 'count_include_pad', 'divisor_override']\n\n    kernel_size: _size_3_t\n    stride: _size_3_t\n    padding: _size_3_t\n    ceil_mode: bool\n    count_include_pad: bool\n\n    def __init__(self, kernel_size: _size_3_t, stride: Optional[_size_3_t] = None, padding: _size_3_t = 0,\n                 ceil_mode: bool = False, count_include_pad: bool = True, divisor_override: Optional[int] = None) -> None:\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if (stride is not None) else kernel_size\n        self.padding = padding\n        self.ceil_mode = ceil_mode\n        self.count_include_pad = count_include_pad\n        self.divisor_override = divisor_override\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.avg_pool3d(input, self.kernel_size, self.stride,\n                            self.padding, self.ceil_mode, self.count_include_pad, self.divisor_override)\n\n    def __setstate__(self, d):\n        super().__setstate__(d)\n        self.__dict__.setdefault('padding', 0)\n        self.__dict__.setdefault('ceil_mode', False)\n        self.__dict__.setdefault('count_include_pad', True)\n\n\nclass FractionalMaxPool2d(Module):\n    r\"\"\"Applies a 2D fractional max pooling over an input signal composed of several input planes.\n\n    Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham\n\n    The max-pooling operation is applied in :math:`kH \\times kW` regions by a stochastic\n    step size determined by the target output size.\n    The number of output features is equal to the number of input planes.\n\n    .. note:: Exactly one of ``output_size`` or ``output_ratio`` must be defined.\n\n    Args:\n        kernel_size: the size of the window to take a max over.\n                     Can be a single number k (for a square kernel of k x k) or a tuple `(kh, kw)`\n        output_size: the target output size of the image of the form `oH x oW`.\n                     Can be a tuple `(oH, oW)` or a single number oH for a square image `oH x oH`\n        output_ratio: If one wants to have an output size as a ratio of the input size, this option can be given.\n                      This has to be a number or tuple in the range (0, 1)\n        return_indices: if ``True``, will return the indices along with the outputs.\n                        Useful to pass to :meth:`nn.MaxUnpool2d`. Default: ``False``\n\n    Shape:\n        - Input: :math:`(N, C, H_{in}, W_{in})` or :math:`(C, H_{in}, W_{in})`.\n        - Output: :math:`(N, C, H_{out}, W_{out})` or :math:`(C, H_{out}, W_{out})`, where\n          :math:`(H_{out}, W_{out})=\\text{output\\_size}` or\n          :math:`(H_{out}, W_{out})=\\text{output\\_ratio} \\times (H_{in}, W_{in})`.\n\n    Examples:\n        >>> # pool of square window of size=3, and target output size 13x12\n        >>> m = nn.FractionalMaxPool2d(3, output_size=(13, 12))\n        >>> # pool of square window and target output size being half of input image size\n        >>> m = nn.FractionalMaxPool2d(3, output_ratio=(0.5, 0.5))\n        >>> input = torch.randn(20, 16, 50, 32)\n        >>> output = m(input)\n\n    .. _Fractional MaxPooling:\n        https://arxiv.org/abs/1412.6071\n    \"\"\"\n\n    __constants__ = ['kernel_size', 'return_indices', 'output_size',\n                     'output_ratio']\n\n    kernel_size: _size_2_t\n    return_indices: bool\n    output_size: _size_2_t\n    output_ratio: _ratio_2_t\n\n    def __init__(self, kernel_size: _size_2_t, output_size: Optional[_size_2_t] = None,\n                 output_ratio: Optional[_ratio_2_t] = None,\n                 return_indices: bool = False, _random_samples=None) -> None:\n        super().__init__()\n        self.kernel_size = _pair(kernel_size)\n        self.return_indices = return_indices\n        self.register_buffer('_random_samples', _random_samples)\n        self.output_size = _pair(output_size) if output_size is not None else None\n        self.output_ratio = _pair(output_ratio) if output_ratio is not None else None\n        if output_size is None and output_ratio is None:\n            raise ValueError(\"FractionalMaxPool2d requires specifying either \"\n                             \"an output size, or a pooling ratio\")\n        if output_size is not None and output_ratio is not None:\n            raise ValueError(\"only one of output_size and output_ratio may be specified\")\n        if self.output_ratio is not None:\n            if not (0 < self.output_ratio[0] < 1 and 0 < self.output_ratio[1] < 1):\n                raise ValueError(f\"output_ratio must be between 0 and 1 (got {output_ratio})\")\n\n    def forward(self, input: Tensor):\n        return F.fractional_max_pool2d(\n            input, self.kernel_size, self.output_size, self.output_ratio,\n            self.return_indices,\n            _random_samples=self._random_samples)\n\n\nclass FractionalMaxPool3d(Module):\n    r\"\"\"Applies a 3D fractional max pooling over an input signal composed of several input planes.\n\n    Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham\n\n    The max-pooling operation is applied in :math:`kT \\times kH \\times kW` regions by a stochastic\n    step size determined by the target output size.\n    The number of output features is equal to the number of input planes.\n\n    .. note:: Exactly one of ``output_size`` or ``output_ratio`` must be defined.\n\n    Args:\n        kernel_size: the size of the window to take a max over.\n                     Can be a single number k (for a square kernel of k x k x k) or a tuple `(kt x kh x kw)`\n        output_size: the target output size of the image of the form `oT x oH x oW`.\n                     Can be a tuple `(oT, oH, oW)` or a single number oH for a square image `oH x oH x oH`\n        output_ratio: If one wants to have an output size as a ratio of the input size, this option can be given.\n                      This has to be a number or tuple in the range (0, 1)\n        return_indices: if ``True``, will return the indices along with the outputs.\n                        Useful to pass to :meth:`nn.MaxUnpool3d`. Default: ``False``\n\n    Shape:\n        - Input: :math:`(N, C, T_{in}, H_{in}, W_{in})` or :math:`(C, T_{in}, H_{in}, W_{in})`.\n        - Output: :math:`(N, C, T_{out}, H_{out}, W_{out})` or :math:`(C, T_{out}, H_{out}, W_{out})`, where\n          :math:`(T_{out}, H_{out}, W_{out})=\\text{output\\_size}` or\n          :math:`(T_{out}, H_{out}, W_{out})=\\text{output\\_ratio} \\times (T_{in}, H_{in}, W_{in})`\n\n    Examples:\n        >>> # pool of cubic window of size=3, and target output size 13x12x11\n        >>> m = nn.FractionalMaxPool3d(3, output_size=(13, 12, 11))\n        >>> # pool of cubic window and target output size being half of input size\n        >>> m = nn.FractionalMaxPool3d(3, output_ratio=(0.5, 0.5, 0.5))\n        >>> input = torch.randn(20, 16, 50, 32, 16)\n        >>> output = m(input)\n\n    .. _Fractional MaxPooling:\n        https://arxiv.org/abs/1412.6071\n    \"\"\"\n\n    __constants__ = ['kernel_size', 'return_indices', 'output_size',\n                     'output_ratio']\n    kernel_size: _size_3_t\n    return_indices: bool\n    output_size: _size_3_t\n    output_ratio: _ratio_3_t\n\n    def __init__(self, kernel_size: _size_3_t, output_size: Optional[_size_3_t] = None,\n                 output_ratio: Optional[_ratio_3_t] = None,\n                 return_indices: bool = False, _random_samples=None) -> None:\n        super().__init__()\n        self.kernel_size = _triple(kernel_size)\n        self.return_indices = return_indices\n        self.register_buffer('_random_samples', _random_samples)\n        self.output_size = _triple(output_size) if output_size is not None else None\n        self.output_ratio = _triple(output_ratio) if output_ratio is not None else None\n        if output_size is None and output_ratio is None:\n            raise ValueError(\"FractionalMaxPool3d requires specifying either \"\n                             \"an output size, or a pooling ratio\")\n        if output_size is not None and output_ratio is not None:\n            raise ValueError(\"only one of output_size and output_ratio may be specified\")\n        if self.output_ratio is not None:\n            if not (0 < self.output_ratio[0] < 1 and 0 < self.output_ratio[1] < 1 and 0 < self.output_ratio[2] < 1):\n                raise ValueError(f\"output_ratio must be between 0 and 1 (got {output_ratio})\")\n\n    def forward(self, input: Tensor):\n        return F.fractional_max_pool3d(\n            input, self.kernel_size, self.output_size, self.output_ratio,\n            self.return_indices,\n            _random_samples=self._random_samples)\n\n\nclass _LPPoolNd(Module):\n    __constants__ = ['norm_type', 'kernel_size', 'stride', 'ceil_mode']\n\n    norm_type: float\n    ceil_mode: bool\n\n    def __init__(self, norm_type: float, kernel_size: _size_any_t, stride: Optional[_size_any_t] = None,\n                 ceil_mode: bool = False) -> None:\n        super().__init__()\n        self.norm_type = norm_type\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.ceil_mode = ceil_mode\n\n    def extra_repr(self) -> str:\n        return 'norm_type={norm_type}, kernel_size={kernel_size}, stride={stride}, ' \\\n            'ceil_mode={ceil_mode}'.format(**self.__dict__)\n\n\nclass LPPool1d(_LPPoolNd):\n    r\"\"\"Applies a 1D power-average pooling over an input signal composed of several input planes.\n\n    On each window, the function computed is:\n\n    .. math::\n        f(X) = \\sqrt[p]{\\sum_{x \\in X} x^{p}}\n\n    - At p = :math:`\\infty`, one gets Max Pooling\n    - At p = 1, one gets Sum Pooling (which is proportional to Average Pooling)\n\n    .. note:: If the sum to the power of `p` is zero, the gradient of this function is\n              not defined. This implementation will set the gradient to zero in this case.\n\n    Args:\n        kernel_size: a single int, the size of the window\n        stride: a single int, the stride of the window. Default value is :attr:`kernel_size`\n        ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape\n\n    Shape:\n        - Input: :math:`(N, C, L_{in})` or :math:`(C, L_{in})`.\n        - Output: :math:`(N, C, L_{out})` or :math:`(C, L_{out})`, where\n\n          .. math::\n              L_{out} = \\left\\lfloor\\frac{L_{in} - \\text{kernel\\_size}}{\\text{stride}} + 1\\right\\rfloor\n\n    Examples::\n        >>> # power-2 pool of window of length 3, with stride 2.\n        >>> m = nn.LPPool1d(2, 3, stride=2)\n        >>> input = torch.randn(20, 16, 50)\n        >>> output = m(input)\n    \"\"\"\n\n    kernel_size: _size_1_t\n    stride: _size_1_t\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.lp_pool1d(input, float(self.norm_type), self.kernel_size,\n                           self.stride, self.ceil_mode)\n\n\nclass LPPool2d(_LPPoolNd):\n    r\"\"\"Applies a 2D power-average pooling over an input signal composed of several input planes.\n\n    On each window, the function computed is:\n\n    .. math::\n        f(X) = \\sqrt[p]{\\sum_{x \\in X} x^{p}}\n\n    - At p = :math:`\\infty`, one gets Max Pooling\n    - At p = 1, one gets Sum Pooling (which is proportional to average pooling)\n\n    The parameters :attr:`kernel_size`, :attr:`stride` can either be:\n\n        - a single ``int`` -- in which case the same value is used for the height and width dimension\n        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,\n          and the second `int` for the width dimension\n\n    .. note:: If the sum to the power of `p` is zero, the gradient of this function is\n              not defined. This implementation will set the gradient to zero in this case.\n\n    Args:\n        kernel_size: the size of the window\n        stride: the stride of the window. Default value is :attr:`kernel_size`\n        ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape\n\n    Shape:\n        - Input: :math:`(N, C, H_{in}, W_{in})`\n        - Output: :math:`(N, C, H_{out}, W_{out})`, where\n\n          .. math::\n              H_{out} = \\left\\lfloor\\frac{H_{in} - \\text{kernel\\_size}[0]}{\\text{stride}[0]} + 1\\right\\rfloor\n\n          .. math::\n              W_{out} = \\left\\lfloor\\frac{W_{in} - \\text{kernel\\_size}[1]}{\\text{stride}[1]} + 1\\right\\rfloor\n\n    Examples::\n\n        >>> # power-2 pool of square window of size=3, stride=2\n        >>> m = nn.LPPool2d(2, 3, stride=2)\n        >>> # pool of non-square window of power 1.2\n        >>> m = nn.LPPool2d(1.2, (3, 2), stride=(2, 1))\n        >>> input = torch.randn(20, 16, 50, 32)\n        >>> output = m(input)\n\n    \"\"\"\n\n    kernel_size: _size_2_t\n    stride: _size_2_t\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.lp_pool2d(input, float(self.norm_type), self.kernel_size,\n                           self.stride, self.ceil_mode)\n\n\nclass _AdaptiveMaxPoolNd(Module):\n    __constants__ = ['output_size', 'return_indices']\n    return_indices: bool\n\n    def __init__(self, output_size: _size_any_opt_t, return_indices: bool = False) -> None:\n        super().__init__()\n        self.output_size = output_size\n        self.return_indices = return_indices\n\n    def extra_repr(self) -> str:\n        return f'output_size={self.output_size}'\n\n# FIXME (by @ssnl): Improve adaptive pooling docs: specify what the input and\n#   output shapes are, and how the operation computes output.\n\n\nclass AdaptiveMaxPool1d(_AdaptiveMaxPoolNd):\n    r\"\"\"Applies a 1D adaptive max pooling over an input signal composed of several input planes.\n\n    The output size is :math:`L_{out}`, for any input size.\n    The number of output features is equal to the number of input planes.\n\n    Args:\n        output_size: the target output size :math:`L_{out}`.\n        return_indices: if ``True``, will return the indices along with the outputs.\n                        Useful to pass to nn.MaxUnpool1d. Default: ``False``\n\n    Shape:\n        - Input: :math:`(N, C, L_{in})` or :math:`(C, L_{in})`.\n        - Output: :math:`(N, C, L_{out})` or :math:`(C, L_{out})`, where\n          :math:`L_{out}=\\text{output\\_size}`.\n\n    Examples:\n        >>> # target output size of 5\n        >>> m = nn.AdaptiveMaxPool1d(5)\n        >>> input = torch.randn(1, 64, 8)\n        >>> output = m(input)\n\n    \"\"\"\n\n    output_size: _size_1_t\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.adaptive_max_pool1d(input, self.output_size, self.return_indices)\n\n\nclass AdaptiveMaxPool2d(_AdaptiveMaxPoolNd):\n    r\"\"\"Applies a 2D adaptive max pooling over an input signal composed of several input planes.\n\n    The output is of size :math:`H_{out} \\times W_{out}`, for any input size.\n    The number of output features is equal to the number of input planes.\n\n    Args:\n        output_size: the target output size of the image of the form :math:`H_{out} \\times W_{out}`.\n                     Can be a tuple :math:`(H_{out}, W_{out})` or a single :math:`H_{out}` for a\n                     square image :math:`H_{out} \\times H_{out}`. :math:`H_{out}` and :math:`W_{out}`\n                     can be either a ``int``, or ``None`` which means the size will be the same as that\n                     of the input.\n        return_indices: if ``True``, will return the indices along with the outputs.\n                        Useful to pass to nn.MaxUnpool2d. Default: ``False``\n\n    Shape:\n        - Input: :math:`(N, C, H_{in}, W_{in})` or :math:`(C, H_{in}, W_{in})`.\n        - Output: :math:`(N, C, H_{out}, W_{out})` or :math:`(C, H_{out}, W_{out})`, where\n          :math:`(H_{out}, W_{out})=\\text{output\\_size}`.\n\n    Examples:\n        >>> # target output size of 5x7\n        >>> m = nn.AdaptiveMaxPool2d((5, 7))\n        >>> input = torch.randn(1, 64, 8, 9)\n        >>> output = m(input)\n        >>> # target output size of 7x7 (square)\n        >>> m = nn.AdaptiveMaxPool2d(7)\n        >>> input = torch.randn(1, 64, 10, 9)\n        >>> output = m(input)\n        >>> # target output size of 10x7\n        >>> m = nn.AdaptiveMaxPool2d((None, 7))\n        >>> input = torch.randn(1, 64, 10, 9)\n        >>> output = m(input)\n\n    \"\"\"\n\n    output_size: _size_2_opt_t\n\n    def forward(self, input: Tensor):\n        return F.adaptive_max_pool2d(input, self.output_size, self.return_indices)\n\n\nclass AdaptiveMaxPool3d(_AdaptiveMaxPoolNd):\n    r\"\"\"Applies a 3D adaptive max pooling over an input signal composed of several input planes.\n\n    The output is of size :math:`D_{out} \\times H_{out} \\times W_{out}`, for any input size.\n    The number of output features is equal to the number of input planes.\n\n    Args:\n        output_size: the target output size of the image of the form :math:`D_{out} \\times H_{out} \\times W_{out}`.\n                     Can be a tuple :math:`(D_{out}, H_{out}, W_{out})` or a single\n                     :math:`D_{out}` for a cube :math:`D_{out} \\times D_{out} \\times D_{out}`.\n                     :math:`D_{out}`, :math:`H_{out}` and :math:`W_{out}` can be either a\n                     ``int``, or ``None`` which means the size will be the same as that of the input.\n\n        return_indices: if ``True``, will return the indices along with the outputs.\n                        Useful to pass to nn.MaxUnpool3d. Default: ``False``\n\n    Shape:\n        - Input: :math:`(N, C, D_{in}, H_{in}, W_{in})` or :math:`(C, D_{in}, H_{in}, W_{in})`.\n        - Output: :math:`(N, C, D_{out}, H_{out}, W_{out})` or :math:`(C, D_{out}, H_{out}, W_{out})`,\n          where :math:`(D_{out}, H_{out}, W_{out})=\\text{output\\_size}`.\n\n    Examples:\n        >>> # target output size of 5x7x9\n        >>> m = nn.AdaptiveMaxPool3d((5, 7, 9))\n        >>> input = torch.randn(1, 64, 8, 9, 10)\n        >>> output = m(input)\n        >>> # target output size of 7x7x7 (cube)\n        >>> m = nn.AdaptiveMaxPool3d(7)\n        >>> input = torch.randn(1, 64, 10, 9, 8)\n        >>> output = m(input)\n        >>> # target output size of 7x9x8\n        >>> m = nn.AdaptiveMaxPool3d((7, None, None))\n        >>> input = torch.randn(1, 64, 10, 9, 8)\n        >>> output = m(input)\n\n    \"\"\"\n\n    output_size: _size_3_opt_t\n\n    def forward(self, input: Tensor):\n        return F.adaptive_max_pool3d(input, self.output_size, self.return_indices)\n\n\nclass _AdaptiveAvgPoolNd(Module):\n    __constants__ = ['output_size']\n\n    def __init__(self, output_size: _size_any_opt_t) -> None:\n        super().__init__()\n        self.output_size = output_size\n\n    def extra_repr(self) -> str:\n        return f'output_size={self.output_size}'\n\n\nclass AdaptiveAvgPool1d(_AdaptiveAvgPoolNd):\n    r\"\"\"Applies a 1D adaptive average pooling over an input signal composed of several input planes.\n\n    The output size is :math:`L_{out}`, for any input size.\n    The number of output features is equal to the number of input planes.\n\n    Args:\n        output_size: the target output size :math:`L_{out}`.\n\n    Shape:\n        - Input: :math:`(N, C, L_{in})` or :math:`(C, L_{in})`.\n        - Output: :math:`(N, C, L_{out})` or :math:`(C, L_{out})`, where\n          :math:`L_{out}=\\text{output\\_size}`.\n\n    Examples:\n        >>> # target output size of 5\n        >>> m = nn.AdaptiveAvgPool1d(5)\n        >>> input = torch.randn(1, 64, 8)\n        >>> output = m(input)\n\n    \"\"\"\n\n    output_size: _size_1_t\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.adaptive_avg_pool1d(input, self.output_size)\n\n\nclass AdaptiveAvgPool2d(_AdaptiveAvgPoolNd):\n    r\"\"\"Applies a 2D adaptive average pooling over an input signal composed of several input planes.\n\n    The output is of size H x W, for any input size.\n    The number of output features is equal to the number of input planes.\n\n    Args:\n        output_size: the target output size of the image of the form H x W.\n                     Can be a tuple (H, W) or a single H for a square image H x H.\n                     H and W can be either a ``int``, or ``None`` which means the size will\n                     be the same as that of the input.\n\n    Shape:\n        - Input: :math:`(N, C, H_{in}, W_{in})` or :math:`(C, H_{in}, W_{in})`.\n        - Output: :math:`(N, C, S_{0}, S_{1})` or :math:`(C, S_{0}, S_{1})`, where\n          :math:`S=\\text{output\\_size}`.\n\n    Examples:\n        >>> # target output size of 5x7\n        >>> m = nn.AdaptiveAvgPool2d((5, 7))\n        >>> input = torch.randn(1, 64, 8, 9)\n        >>> output = m(input)\n        >>> # target output size of 7x7 (square)\n        >>> m = nn.AdaptiveAvgPool2d(7)\n        >>> input = torch.randn(1, 64, 10, 9)\n        >>> output = m(input)\n        >>> # target output size of 10x7\n        >>> m = nn.AdaptiveAvgPool2d((None, 7))\n        >>> input = torch.randn(1, 64, 10, 9)\n        >>> output = m(input)\n\n    \"\"\"\n\n    output_size: _size_2_opt_t\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.adaptive_avg_pool2d(input, self.output_size)\n\n\nclass AdaptiveAvgPool3d(_AdaptiveAvgPoolNd):\n    r\"\"\"Applies a 3D adaptive average pooling over an input signal composed of several input planes.\n\n    The output is of size D x H x W, for any input size.\n    The number of output features is equal to the number of input planes.\n\n    Args:\n        output_size: the target output size of the form D x H x W.\n                     Can be a tuple (D, H, W) or a single number D for a cube D x D x D.\n                     D, H and W can be either a ``int``, or ``None`` which means the size will\n                     be the same as that of the input.\n\n    Shape:\n        - Input: :math:`(N, C, D_{in}, H_{in}, W_{in})` or :math:`(C, D_{in}, H_{in}, W_{in})`.\n        - Output: :math:`(N, C, S_{0}, S_{1}, S_{2})` or :math:`(C, S_{0}, S_{1}, S_{2})`,\n          where :math:`S=\\text{output\\_size}`.\n\n    Examples:\n        >>> # target output size of 5x7x9\n        >>> m = nn.AdaptiveAvgPool3d((5, 7, 9))\n        >>> input = torch.randn(1, 64, 8, 9, 10)\n        >>> output = m(input)\n        >>> # target output size of 7x7x7 (cube)\n        >>> m = nn.AdaptiveAvgPool3d(7)\n        >>> input = torch.randn(1, 64, 10, 9, 8)\n        >>> output = m(input)\n        >>> # target output size of 7x9x8\n        >>> m = nn.AdaptiveAvgPool3d((7, None, None))\n        >>> input = torch.randn(1, 64, 10, 9, 8)\n        >>> output = m(input)\n\n    \"\"\"\n\n    output_size: _size_3_opt_t\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.adaptive_avg_pool3d(input, self.output_size)\n", 1229], "C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py": ["import warnings\nfrom collections import OrderedDict, abc as container_abcs\nfrom itertools import chain, islice\nimport operator\n\nimport torch\nfrom .module import Module\nfrom ..parameter import Parameter\nfrom torch._jit_internal import _copy_to_script_wrapper\n\nfrom typing import Any, Dict, Iterable, Iterator, Mapping, Optional, overload, Tuple, TypeVar, Union\nfrom typing_extensions import Self\n\n__all__ = ['Container', 'Sequential', 'ModuleList', 'ModuleDict', 'ParameterList', 'ParameterDict']\n\nT = TypeVar('T', bound=Module)\n\n\n# Copied from torch.nn.modules.module, required for a custom __repr__ for ModuleList\ndef _addindent(s_, numSpaces):\n    s = s_.split('\\n')\n    # don't do anything for single-line stuff\n    if len(s) == 1:\n        return s_\n    first = s.pop(0)\n    s = [(numSpaces * ' ') + line for line in s]\n    s = '\\n'.join(s)\n    s = first + '\\n' + s\n    return s\n\n\nclass Container(Module):\n\n    def __init__(self, **kwargs: Any) -> None:\n        super().__init__()\n        # DeprecationWarning is ignored by default <sigh>\n        warnings.warn(\"nn.Container is deprecated. All of it's functionality \"\n                      \"is now implemented in nn.Module. Subclass that instead.\")\n        for key, value in kwargs.items():\n            self.add_module(key, value)\n\n\nclass Sequential(Module):\n    r\"\"\"A sequential container.\n\n    Modules will be added to it in the order they are passed in the\n    constructor. Alternatively, an ``OrderedDict`` of modules can be\n    passed in. The ``forward()`` method of ``Sequential`` accepts any\n    input and forwards it to the first module it contains. It then\n    \"chains\" outputs to inputs sequentially for each subsequent module,\n    finally returning the output of the last module.\n\n    The value a ``Sequential`` provides over manually calling a sequence\n    of modules is that it allows treating the whole container as a\n    single module, such that performing a transformation on the\n    ``Sequential`` applies to each of the modules it stores (which are\n    each a registered submodule of the ``Sequential``).\n\n    What's the difference between a ``Sequential`` and a\n    :class:`torch.nn.ModuleList`? A ``ModuleList`` is exactly what it\n    sounds like--a list for storing ``Module`` s! On the other hand,\n    the layers in a ``Sequential`` are connected in a cascading way.\n\n    Example::\n\n        # Using Sequential to create a small model. When `model` is run,\n        # input will first be passed to `Conv2d(1,20,5)`. The output of\n        # `Conv2d(1,20,5)` will be used as the input to the first\n        # `ReLU`; the output of the first `ReLU` will become the input\n        # for `Conv2d(20,64,5)`. Finally, the output of\n        # `Conv2d(20,64,5)` will be used as input to the second `ReLU`\n        model = nn.Sequential(\n                  nn.Conv2d(1,20,5),\n                  nn.ReLU(),\n                  nn.Conv2d(20,64,5),\n                  nn.ReLU()\n                )\n\n        # Using Sequential with OrderedDict. This is functionally the\n        # same as the above code\n        model = nn.Sequential(OrderedDict([\n                  ('conv1', nn.Conv2d(1,20,5)),\n                  ('relu1', nn.ReLU()),\n                  ('conv2', nn.Conv2d(20,64,5)),\n                  ('relu2', nn.ReLU())\n                ]))\n    \"\"\"\n\n    _modules: Dict[str, Module]  # type: ignore[assignment]\n\n    @overload\n    def __init__(self, *args: Module) -> None:\n        ...\n\n    @overload\n    def __init__(self, arg: 'OrderedDict[str, Module]') -> None:\n        ...\n\n    def __init__(self, *args):\n        super().__init__()\n        if len(args) == 1 and isinstance(args[0], OrderedDict):\n            for key, module in args[0].items():\n                self.add_module(key, module)\n        else:\n            for idx, module in enumerate(args):\n                self.add_module(str(idx), module)\n\n    def _get_item_by_idx(self, iterator, idx) -> T:  # type: ignore[misc, type-var]\n        \"\"\"Get the idx-th item of the iterator.\"\"\"\n        size = len(self)\n        idx = operator.index(idx)\n        if not -size <= idx < size:\n            raise IndexError(f'index {idx} is out of range')\n        idx %= size\n        return next(islice(iterator, idx, None))\n\n    @_copy_to_script_wrapper\n    def __getitem__(self, idx: Union[slice, int]) -> Union['Sequential', T]:\n        if isinstance(idx, slice):\n            return self.__class__(OrderedDict(list(self._modules.items())[idx]))\n        else:\n            return self._get_item_by_idx(self._modules.values(), idx)\n\n    def __setitem__(self, idx: int, module: Module) -> None:\n        key: str = self._get_item_by_idx(self._modules.keys(), idx)\n        return setattr(self, key, module)\n\n    def __delitem__(self, idx: Union[slice, int]) -> None:\n        if isinstance(idx, slice):\n            for key in list(self._modules.keys())[idx]:\n                delattr(self, key)\n        else:\n            key = self._get_item_by_idx(self._modules.keys(), idx)\n            delattr(self, key)\n        # To preserve numbering\n        str_indices = [str(i) for i in range(len(self._modules))]\n        self._modules = OrderedDict(list(zip(str_indices, self._modules.values())))\n\n    @_copy_to_script_wrapper\n    def __len__(self) -> int:\n        return len(self._modules)\n\n    def __add__(self, other) -> 'Sequential':\n        if isinstance(other, Sequential):\n            ret = Sequential()\n            for layer in self:\n                ret.append(layer)\n            for layer in other:\n                ret.append(layer)\n            return ret\n        else:\n            raise ValueError('add operator supports only objects '\n                             f'of Sequential class, but {str(type(other))} is given.')\n\n    def pop(self, key: Union[int, slice]) -> Module:\n        v = self[key]\n        del self[key]\n        return v\n\n    def __iadd__(self, other) -> Self:\n        if isinstance(other, Sequential):\n            offset = len(self)\n            for i, module in enumerate(other):\n                self.add_module(str(i + offset), module)\n            return self\n        else:\n            raise ValueError('add operator supports only objects '\n                             f'of Sequential class, but {str(type(other))} is given.')\n\n    def __mul__(self, other: int) -> 'Sequential':\n        if not isinstance(other, int):\n            raise TypeError(f\"unsupported operand type(s) for *: {type(self)} and {type(other)}\")\n        elif (other <= 0):\n            raise ValueError(f\"Non-positive multiplication factor {other} for {type(self)}\")\n        else:\n            combined = Sequential()\n            offset = 0\n            for _ in range(other):\n                for module in self:\n                    combined.add_module(str(offset), module)\n                    offset += 1\n            return combined\n\n    def __rmul__(self, other: int) -> 'Sequential':\n        return self.__mul__(other)\n\n    def __imul__(self, other: int) -> Self:\n        if not isinstance(other, int):\n            raise TypeError(f\"unsupported operand type(s) for *: {type(self)} and {type(other)}\")\n        elif (other <= 0):\n            raise ValueError(f\"Non-positive multiplication factor {other} for {type(self)}\")\n        else:\n            len_original = len(self)\n            offset = len(self)\n            for _ in range(other - 1):\n                for i in range(len_original):\n                    self.add_module(str(i + offset), self._modules[str(i)])\n                offset += len_original\n            return self\n\n    @_copy_to_script_wrapper\n    def __dir__(self):\n        keys = super().__dir__()\n        keys = [key for key in keys if not key.isdigit()]\n        return keys\n\n    @_copy_to_script_wrapper\n    def __iter__(self) -> Iterator[Module]:\n        return iter(self._modules.values())\n\n    # NB: We can't really type check this function as the type of input\n    # may change dynamically (as is tested in\n    # TestScript.test_sequential_intermediary_types).  Cannot annotate\n    # with Any as TorchScript expects a more precise type\n    def forward(self, input):\n        for module in self:\n            input = module(input)\n        return input\n\n    def append(self, module: Module) -> 'Sequential':\n        r\"\"\"Append a given module to the end.\n\n        Args:\n            module (nn.Module): module to append\n        \"\"\"\n        self.add_module(str(len(self)), module)\n        return self\n\n    def insert(self, index: int, module: Module) -> 'Sequential':\n        if not isinstance(module, Module):\n            raise AssertionError(\n                f'module should be of type: {Module}')\n        n = len(self._modules)\n        if not (-n <= index <= n):\n            raise IndexError(\n                f'Index out of range: {index}')\n        if index < 0:\n            index += n\n        for i in range(n, index, -1):\n            self._modules[str(i)] = self._modules[str(i - 1)]\n        self._modules[str(index)] = module\n        return self\n\n    def extend(self, sequential) -> 'Sequential':\n        for layer in sequential:\n            self.append(layer)\n        return self\n\n\nclass ModuleList(Module):\n    r\"\"\"Holds submodules in a list.\n\n    :class:`~torch.nn.ModuleList` can be indexed like a regular Python list, but\n    modules it contains are properly registered, and will be visible by all\n    :class:`~torch.nn.Module` methods.\n\n    Args:\n        modules (iterable, optional): an iterable of modules to add\n\n    Example::\n\n        class MyModule(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n\n            def forward(self, x):\n                # ModuleList can act as an iterable, or be indexed using ints\n                for i, l in enumerate(self.linears):\n                    x = self.linears[i // 2](x) + l(x)\n                return x\n    \"\"\"\n\n    _modules: Dict[str, Module]  # type: ignore[assignment]\n\n    def __init__(self, modules: Optional[Iterable[Module]] = None) -> None:\n        super().__init__()\n        if modules is not None:\n            self += modules\n\n    def _get_abs_string_index(self, idx):\n        \"\"\"Get the absolute index for the list of modules.\"\"\"\n        idx = operator.index(idx)\n        if not (-len(self) <= idx < len(self)):\n            raise IndexError(f'index {idx} is out of range')\n        if idx < 0:\n            idx += len(self)\n        return str(idx)\n\n    @_copy_to_script_wrapper\n    def __getitem__(self, idx: Union[int, slice]) -> Union[Module, 'ModuleList']:\n        if isinstance(idx, slice):\n            return self.__class__(list(self._modules.values())[idx])\n        else:\n            return self._modules[self._get_abs_string_index(idx)]\n\n    def __setitem__(self, idx: int, module: Module) -> None:\n        idx = self._get_abs_string_index(idx)\n        return setattr(self, str(idx), module)\n\n    def __delitem__(self, idx: Union[int, slice]) -> None:\n        if isinstance(idx, slice):\n            for k in range(len(self._modules))[idx]:\n                delattr(self, str(k))\n        else:\n            delattr(self, self._get_abs_string_index(idx))\n        # To preserve numbering, self._modules is being reconstructed with modules after deletion\n        str_indices = [str(i) for i in range(len(self._modules))]\n        self._modules = OrderedDict(list(zip(str_indices, self._modules.values())))\n\n    @_copy_to_script_wrapper\n    def __len__(self) -> int:\n        return len(self._modules)\n\n    @_copy_to_script_wrapper\n    def __iter__(self) -> Iterator[Module]:\n        return iter(self._modules.values())\n\n    def __iadd__(self, modules: Iterable[Module]) -> Self:\n        return self.extend(modules)\n\n    def __add__(self, other: Iterable[Module]) -> 'ModuleList':\n        combined = ModuleList()\n        for i, module in enumerate(chain(self, other)):\n            combined.add_module(str(i), module)\n        return combined\n\n    def __repr__(self):\n        \"\"\"Return a custom repr for ModuleList that compresses repeated module representations.\"\"\"\n        list_of_reprs = [repr(item) for item in self]\n        if len(list_of_reprs) == 0:\n            return self._get_name() + '()'\n\n        start_end_indices = [[0, 0]]\n        repeated_blocks = [list_of_reprs[0]]\n        for i, r in enumerate(list_of_reprs[1:], 1):\n            if r == repeated_blocks[-1]:\n                start_end_indices[-1][1] += 1\n                continue\n\n            start_end_indices.append([i, i])\n            repeated_blocks.append(r)\n\n        lines = []\n        main_str = self._get_name() + '('\n        for (start_id, end_id), b in zip(start_end_indices, repeated_blocks):\n            local_repr = f\"({start_id}): {b}\"  # default repr\n\n            if start_id != end_id:\n                n = end_id - start_id + 1\n                local_repr = f\"({start_id}-{end_id}): {n} x {b}\"\n\n            local_repr = _addindent(local_repr, 2)\n            lines.append(local_repr)\n\n        main_str += '\\n  ' + '\\n  '.join(lines) + '\\n'\n        main_str += ')'\n        return main_str\n\n    @_copy_to_script_wrapper\n    def __dir__(self):\n        keys = super().__dir__()\n        keys = [key for key in keys if not key.isdigit()]\n        return keys\n\n    def insert(self, index: int, module: Module) -> None:\n        r\"\"\"Insert a given module before a given index in the list.\n\n        Args:\n            index (int): index to insert.\n            module (nn.Module): module to insert\n        \"\"\"\n        for i in range(len(self._modules), index, -1):\n            self._modules[str(i)] = self._modules[str(i - 1)]\n        self._modules[str(index)] = module\n\n    def append(self, module: Module) -> 'ModuleList':\n        r\"\"\"Append a given module to the end of the list.\n\n        Args:\n            module (nn.Module): module to append\n        \"\"\"\n        self.add_module(str(len(self)), module)\n        return self\n\n    def pop(self, key: Union[int, slice]) -> Module:\n        v = self[key]\n        del self[key]\n        return v\n\n    def extend(self, modules: Iterable[Module]) -> Self:\n        r\"\"\"Append modules from a Python iterable to the end of the list.\n\n        Args:\n            modules (iterable): iterable of modules to append\n        \"\"\"\n        if not isinstance(modules, container_abcs.Iterable):\n            raise TypeError(\"ModuleList.extend should be called with an \"\n                            \"iterable, but got \" + type(modules).__name__)\n        offset = len(self)\n        for i, module in enumerate(modules):\n            self.add_module(str(offset + i), module)\n        return self\n\n    # remove forward alltogether to fallback on Module's _forward_unimplemented\n\n\nclass ModuleDict(Module):\n    r\"\"\"Holds submodules in a dictionary.\n\n    :class:`~torch.nn.ModuleDict` can be indexed like a regular Python dictionary,\n    but modules it contains are properly registered, and will be visible by all\n    :class:`~torch.nn.Module` methods.\n\n    :class:`~torch.nn.ModuleDict` is an **ordered** dictionary that respects\n\n    * the order of insertion, and\n\n    * in :meth:`~torch.nn.ModuleDict.update`, the order of the merged\n      ``OrderedDict``, ``dict`` (started from Python 3.6) or another\n      :class:`~torch.nn.ModuleDict` (the argument to\n      :meth:`~torch.nn.ModuleDict.update`).\n\n    Note that :meth:`~torch.nn.ModuleDict.update` with other unordered mapping\n    types (e.g., Python's plain ``dict`` before Python version 3.6) does not\n    preserve the order of the merged mapping.\n\n    Args:\n        modules (iterable, optional): a mapping (dictionary) of (string: module)\n            or an iterable of key-value pairs of type (string, module)\n\n    Example::\n\n        class MyModule(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.choices = nn.ModuleDict({\n                        'conv': nn.Conv2d(10, 10, 3),\n                        'pool': nn.MaxPool2d(3)\n                })\n                self.activations = nn.ModuleDict([\n                        ['lrelu', nn.LeakyReLU()],\n                        ['prelu', nn.PReLU()]\n                ])\n\n            def forward(self, x, choice, act):\n                x = self.choices[choice](x)\n                x = self.activations[act](x)\n                return x\n    \"\"\"\n\n    _modules: Dict[str, Module]  # type: ignore[assignment]\n\n    def __init__(self, modules: Optional[Mapping[str, Module]] = None) -> None:\n        super().__init__()\n        if modules is not None:\n            self.update(modules)\n\n    @_copy_to_script_wrapper\n    def __getitem__(self, key: str) -> Module:\n        return self._modules[key]\n\n    def __setitem__(self, key: str, module: Module) -> None:\n        self.add_module(key, module)\n\n    def __delitem__(self, key: str) -> None:\n        del self._modules[key]\n\n    @_copy_to_script_wrapper\n    def __len__(self) -> int:\n        return len(self._modules)\n\n    @_copy_to_script_wrapper\n    def __iter__(self) -> Iterator[str]:\n        return iter(self._modules)\n\n    @_copy_to_script_wrapper\n    def __contains__(self, key: str) -> bool:\n        return key in self._modules\n\n    def clear(self) -> None:\n        \"\"\"Remove all items from the ModuleDict.\"\"\"\n        self._modules.clear()\n\n    def pop(self, key: str) -> Module:\n        r\"\"\"Remove key from the ModuleDict and return its module.\n\n        Args:\n            key (str): key to pop from the ModuleDict\n        \"\"\"\n        v = self[key]\n        del self[key]\n        return v\n\n    @_copy_to_script_wrapper\n    def keys(self) -> Iterable[str]:\n        r\"\"\"Return an iterable of the ModuleDict keys.\"\"\"\n        return self._modules.keys()\n\n    @_copy_to_script_wrapper\n    def items(self) -> Iterable[Tuple[str, Module]]:\n        r\"\"\"Return an iterable of the ModuleDict key/value pairs.\"\"\"\n        return self._modules.items()\n\n    @_copy_to_script_wrapper\n    def values(self) -> Iterable[Module]:\n        r\"\"\"Return an iterable of the ModuleDict values.\"\"\"\n        return self._modules.values()\n\n    def update(self, modules: Mapping[str, Module]) -> None:\n        r\"\"\"Update the :class:`~torch.nn.ModuleDict` with key-value pairs from a mapping, overwriting existing keys.\n\n        .. note::\n            If :attr:`modules` is an ``OrderedDict``, a :class:`~torch.nn.ModuleDict`, or\n            an iterable of key-value pairs, the order of new elements in it is preserved.\n\n        Args:\n            modules (iterable): a mapping (dictionary) from string to :class:`~torch.nn.Module`,\n                or an iterable of key-value pairs of type (string, :class:`~torch.nn.Module`)\n        \"\"\"\n        if not isinstance(modules, container_abcs.Iterable):\n            raise TypeError(\"ModuleDict.update should be called with an \"\n                            \"iterable of key/value pairs, but got \" +\n                            type(modules).__name__)\n\n        if isinstance(modules, (OrderedDict, ModuleDict, container_abcs.Mapping)):\n            for key, module in modules.items():\n                self[key] = module\n        else:\n            # modules here can be a list with two items\n            for j, m in enumerate(modules):\n                if not isinstance(m, container_abcs.Iterable):\n                    raise TypeError(\"ModuleDict update sequence element \"\n                                    \"#\" + str(j) + \" should be Iterable; is\" +\n                                    type(m).__name__)\n                if not len(m) == 2:\n                    raise ValueError(\"ModuleDict update sequence element \"\n                                     \"#\" + str(j) + \" has length \" + str(len(m)) +\n                                     \"; 2 is required\")\n                # modules can be Mapping (what it's typed at), or a list: [(name1, module1), (name2, module2)]\n                # that's too cumbersome to type correctly with overloads, so we add an ignore here\n                self[m[0]] = m[1]  # type: ignore[assignment]\n\n    # remove forward alltogether to fallback on Module's _forward_unimplemented\n\n\nclass ParameterList(Module):\n    r\"\"\"Holds parameters in a list.\n\n    :class:`~torch.nn.ParameterList` can be used like a regular Python\n    list, but Tensors that are :class:`~torch.nn.Parameter` are properly registered,\n    and will be visible by all :class:`~torch.nn.Module` methods.\n\n    Note that the constructor, assigning an element of the list, the\n    :meth:`~torch.nn.ParameterDict.append` method and the :meth:`~torch.nn.ParameterDict.extend`\n    method will convert any :class:`~torch.Tensor` into :class:`~torch.nn.Parameter`.\n\n    Args:\n        parameters (iterable, optional): an iterable of elements to add to the list.\n\n    Example::\n\n        class MyModule(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])\n\n            def forward(self, x):\n                # ParameterList can act as an iterable, or be indexed using ints\n                for i, p in enumerate(self.params):\n                    x = self.params[i // 2].mm(x) + p.mm(x)\n                return x\n    \"\"\"\n\n    def __init__(self, values: Optional[Iterable[Any]] = None) -> None:\n        super().__init__()\n        self._size = 0\n        if values is not None:\n            self += values\n\n    def _get_abs_string_index(self, idx):\n        \"\"\"Get the absolute index for the list of modules.\"\"\"\n        idx = operator.index(idx)\n        if not (-len(self) <= idx < len(self)):\n            raise IndexError(f'index {idx} is out of range')\n        if idx < 0:\n            idx += len(self)\n        return str(idx)\n\n    @overload\n    def __getitem__(self, idx: int) -> Any:\n        ...\n\n    @overload\n    def __getitem__(self: T, idx: slice) -> T:\n        ...\n\n    def __getitem__(self, idx):\n        if isinstance(idx, slice):\n            start, stop, step = idx.indices(len(self))\n            out = self.__class__()\n            for i in range(start, stop, step):\n                out.append(self[i])\n            return out\n        else:\n            idx = self._get_abs_string_index(idx)\n            return getattr(self, str(idx))\n\n    def __setitem__(self, idx: int, param: Any) -> None:\n        # Note that all other function that add an entry to the list part of\n        # the ParameterList end up here. So this is the only place where we need\n        # to wrap things into Parameter if needed.\n        # Objects added via setattr() are not in the list part and thus won't\n        # call into this function.\n        idx = self._get_abs_string_index(idx)\n        if isinstance(param, torch.Tensor) and not isinstance(param, Parameter):\n            param = Parameter(param)\n        return setattr(self, str(idx), param)\n\n    def __len__(self) -> int:\n        return self._size\n\n    def __iter__(self) -> Iterator[Any]:\n        return iter(self[i] for i in range(len(self)))\n\n    def __iadd__(self, parameters: Iterable[Any]) -> Self:\n        return self.extend(parameters)\n\n    def __dir__(self):\n        keys = super().__dir__()\n        keys = [key for key in keys if not key.isdigit()]\n        return keys\n\n    def append(self, value: Any) -> 'ParameterList':\n        \"\"\"Append a given value at the end of the list.\n\n        Args:\n            value (Any): value to append\n        \"\"\"\n        new_idx = len(self)\n        self._size += 1\n        self[new_idx] = value\n        return self\n\n    def extend(self, values: Iterable[Any]) -> Self:\n        \"\"\"Append values from a Python iterable to the end of the list.\n\n        Args:\n            values (iterable): iterable of values to append\n        \"\"\"\n        # Tensor is an iterable but we never want to unpack it here\n        if not isinstance(values, container_abcs.Iterable) or isinstance(values, torch.Tensor):\n            raise TypeError(\"ParameterList.extend should be called with an \"\n                            \"iterable, but got \" + type(values).__name__)\n        for value in values:\n            self.append(value)\n        return self\n\n    def extra_repr(self) -> str:\n        child_lines = []\n        for k, p in enumerate(self):\n            if isinstance(p, torch.Tensor):\n                size_str = 'x'.join(str(size) for size in p.size())\n                if p.device.type in [\"cuda\", torch._C._get_privateuse1_backend_name()]:\n                    device_str = f' ({p.device})'\n                else:\n                    device_str = ''\n                parastr = '{} containing: [{} of size {}{}]'.format(\n                    \"Parameter\" if isinstance(p, Parameter) else \"Tensor\",\n                    p.dtype, size_str, device_str)\n                child_lines.append('  (' + str(k) + '): ' + parastr)\n            else:\n                child_lines.append('  (' + str(k) + '): Object of type: ' + type(p).__name__)\n\n        tmpstr = '\\n'.join(child_lines)\n        return tmpstr\n\n    def __call__(self, *args, **kwargs):\n        raise RuntimeError('ParameterList should not be called.')\n\n\nclass ParameterDict(Module):\n    r\"\"\"Holds parameters in a dictionary.\n\n    ParameterDict can be indexed like a regular Python dictionary, but Parameters it\n    contains are properly registered, and will be visible by all Module methods.\n    Other objects are treated as would be done by a regular Python dictionary\n\n    :class:`~torch.nn.ParameterDict` is an **ordered** dictionary.\n    :meth:`~torch.nn.ParameterDict.update` with other unordered mapping\n    types (e.g., Python's plain ``dict``) does not preserve the order of the\n    merged mapping. On the other hand, ``OrderedDict`` or another :class:`~torch.nn.ParameterDict`\n    will preserve their ordering.\n\n    Note that the constructor, assigning an element of the dictionary and the\n    :meth:`~torch.nn.ParameterDict.update` method will convert any :class:`~torch.Tensor` into\n    :class:`~torch.nn.Parameter`.\n\n    Args:\n        values (iterable, optional): a mapping (dictionary) of\n            (string : Any) or an iterable of key-value pairs\n            of type (string, Any)\n\n    Example::\n\n        class MyModule(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.params = nn.ParameterDict({\n                        'left': nn.Parameter(torch.randn(5, 10)),\n                        'right': nn.Parameter(torch.randn(5, 10))\n                })\n\n            def forward(self, x, choice):\n                x = self.params[choice].mm(x)\n                return x\n    \"\"\"\n\n    def __init__(self, parameters: Any = None) -> None:\n        super().__init__()\n        self._keys: Dict[str, None] = {}\n        if parameters is not None:\n            self.update(parameters)\n\n    def _key_to_attr(self, key: str) -> str:\n        if not isinstance(key, str):\n            raise TypeError(\"Index given to ParameterDict cannot be used as a key as it is \"\n                            f\"not a string (type is '{type(key).__name__}'). Open an issue on \"\n                            \"github if you need non-string keys.\")\n        else:\n            # Use the key as-is so that `.named_parameters()` returns the right thing\n            return key\n\n    def __getitem__(self, key: str) -> Any:\n        attr = self._key_to_attr(key)\n        return getattr(self, attr)\n\n    def __setitem__(self, key: str, value: Any) -> None:\n        # Note that all other function that add an entry to the dictionary part of\n        # the ParameterDict end up here. So this is the only place where we need\n        # to wrap things into Parameter if needed.\n        # Objects added via setattr() are not in the dictionary part and thus won't\n        # call into this function.\n        self._keys[key] = None\n        attr = self._key_to_attr(key)\n        if isinstance(value, torch.Tensor) and not isinstance(value, Parameter):\n            value = Parameter(value)\n        setattr(self, attr, value)\n\n    def __delitem__(self, key: str) -> None:\n        del self._keys[key]\n        attr = self._key_to_attr(key)\n        delattr(self, attr)\n\n    def __len__(self) -> int:\n        return len(self._keys)\n\n    def __iter__(self) -> Iterator[str]:\n        return iter(self._keys)\n\n    def __reversed__(self) -> Iterator[str]:\n        return reversed(list(self._keys))\n\n    def copy(self) -> 'ParameterDict':\n        \"\"\"Return a copy of this :class:`~torch.nn.ParameterDict` instance.\"\"\"\n        # We have to use an OrderedDict because the ParameterDict constructor\n        # behaves differently on plain dict vs OrderedDict\n        return ParameterDict(OrderedDict((k, self[k]) for k in self._keys))\n\n    def __contains__(self, key: str) -> bool:\n        return key in self._keys\n\n    def setdefault(self, key: str, default: Optional[Any] = None) -> Any:\n        \"\"\"Set the default for a key in the Parameterdict.\n\n        If key is in the ParameterDict, return its value.\n        If not, insert `key` with a parameter `default` and return `default`.\n        `default` defaults to `None`.\n\n        Args:\n            key (str): key to set default for\n            default (Any): the parameter set to the key\n        \"\"\"\n        if key not in self:\n            self[key] = default\n        return self[key]\n\n    def clear(self) -> None:\n        \"\"\"Remove all items from the ParameterDict.\"\"\"\n        for k in self._keys.copy():\n            del self[k]\n\n    def pop(self, key: str) -> Any:\n        r\"\"\"Remove key from the ParameterDict and return its parameter.\n\n        Args:\n            key (str): key to pop from the ParameterDict\n        \"\"\"\n        v = self[key]\n        del self[key]\n        return v\n\n    def popitem(self) -> Tuple[str, Any]:\n        \"\"\"Remove and return the last inserted `(key, parameter)` pair from the ParameterDict.\"\"\"\n        k, _ = self._keys.popitem()\n        # We need the key in the _keys to be able to access/del\n        self._keys[k] = None\n        val = self[k]\n        del self[k]\n        return k, val\n\n    def get(self, key: str, default: Optional[Any] = None) -> Any:\n        r\"\"\"Return the parameter associated with key if present. Otherwise return default if provided, None if not.\n\n        Args:\n            key (str): key to get from the ParameterDict\n            default (Parameter, optional): value to return if key not present\n        \"\"\"\n        return self[key] if key in self else default\n\n    def fromkeys(self, keys: Iterable[str], default: Optional[Any] = None) -> 'ParameterDict':\n        r\"\"\"Return a new ParameterDict with the keys provided.\n\n        Args:\n            keys (iterable, string): keys to make the new ParameterDict from\n            default (Parameter, optional): value to set for all keys\n        \"\"\"\n        return ParameterDict((k, default) for k in keys)\n\n    def keys(self) -> Iterable[str]:\n        r\"\"\"Return an iterable of the ParameterDict keys.\"\"\"\n        return self._keys.keys()\n\n    def items(self) -> Iterable[Tuple[str, Any]]:\n        r\"\"\"Return an iterable of the ParameterDict key/value pairs.\"\"\"\n        return ((k, self[k]) for k in self._keys)\n\n    def values(self) -> Iterable[Any]:\n        r\"\"\"Return an iterable of the ParameterDict values.\"\"\"\n        return (self[k] for k in self._keys)\n\n    def update(self, parameters: Union[Mapping[str, Any], 'ParameterDict']) -> None:\n        r\"\"\"Update the :class:`~torch.nn.ParameterDict` with key-value pairs from ``parameters``, overwriting existing keys.\n\n        .. note::\n            If :attr:`parameters` is an ``OrderedDict``, a :class:`~torch.nn.ParameterDict`, or\n            an iterable of key-value pairs, the order of new elements in it is preserved.\n\n        Args:\n            parameters (iterable): a mapping (dictionary) from string to\n                :class:`~torch.nn.Parameter`, or an iterable of\n                key-value pairs of type (string, :class:`~torch.nn.Parameter`)\n        \"\"\"\n        if not isinstance(parameters, container_abcs.Iterable):\n            raise TypeError(\"ParametersDict.update should be called with an \"\n                            \"iterable of key/value pairs, but got \" +\n                            type(parameters).__name__)\n\n        if isinstance(parameters, (OrderedDict, ParameterDict)):\n            for key, parameter in parameters.items():\n                self[key] = parameter\n        elif isinstance(parameters, container_abcs.Mapping):\n            for key, parameter in sorted(parameters.items()):\n                self[key] = parameter\n        else:\n            for j, p in enumerate(parameters):\n                if not isinstance(p, container_abcs.Iterable):\n                    raise TypeError(\"ParameterDict update sequence element \"\n                                    \"#\" + str(j) + \" should be Iterable; is\" +\n                                    type(p).__name__)\n                if not len(p) == 2:\n                    raise ValueError(\"ParameterDict update sequence element \"\n                                     \"#\" + str(j) + \" has length \" + str(len(p)) +\n                                     \"; 2 is required\")\n                # parameters as length-2 list too cumbersome to type, see ModuleDict.update comment\n                self[p[0]] = p[1]  # type: ignore[assignment]\n\n    def extra_repr(self) -> str:\n        child_lines = []\n        for k, p in self.items():\n            if isinstance(p, torch.Tensor):\n                size_str = 'x'.join(str(size) for size in p.size())\n                if p.device.type in [\"cuda\", torch._C._get_privateuse1_backend_name()]:\n                    device_str = f' ({p.device})'\n                else:\n                    device_str = ''\n                parastr = '{} containing: [{} of size {}{}]'.format(\n                    \"Parameter\" if isinstance(p, Parameter) else \"Tensor\",\n                    torch.typename(p), size_str, device_str)\n                child_lines.append('  (' + str(k) + '): ' + parastr)\n            else:\n                child_lines.append('  (' + str(k) + '): Object of type: ' + type(p).__name__)\n        tmpstr = '\\n'.join(child_lines)\n        return tmpstr\n\n    def __call__(self, input):\n        raise RuntimeError('ParameterDict should not be called.')\n\n    def __or__(self, other: 'ParameterDict') -> 'ParameterDict':\n        copy = self.copy()\n        copy.update(other)\n        return copy\n\n    def __ror__(self, other: 'ParameterDict') -> 'ParameterDict':\n        copy = other.copy()\n        copy.update(self)\n        return copy\n\n    def __ior__(self, other : 'ParameterDict') -> Self:\n        self.update(other)\n        return self\n", 911], "C:\\Users\\adam/.cache\\torch\\hub\\pytorch_vision_v0.10.0\\torchvision\\models\\resnet.py": ["import torch\nfrom torch import Tensor\nimport torch.nn as nn\nfrom .utils import load_state_dict_from_url\nfrom typing import Type, Any, Callable, Union, List, Optional\n\n\n__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n           'wide_resnet50_2', 'wide_resnet101_2']\n\n\nmodel_urls = {\n    'resnet18': 'https://download.pytorch.org/models/resnet18-f37072fd.pth',\n    'resnet34': 'https://download.pytorch.org/models/resnet34-b627a593.pth',\n    'resnet50': 'https://download.pytorch.org/models/resnet50-0676ba61.pth',\n    'resnet101': 'https://download.pytorch.org/models/resnet101-63fe2227.pth',\n    'resnet152': 'https://download.pytorch.org/models/resnet152-394f9c45.pth',\n    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n}\n\n\ndef conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n\n\ndef conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion: int = 1\n\n    def __init__(\n        self,\n        inplanes: int,\n        planes: int,\n        stride: int = 1,\n        downsample: Optional[nn.Module] = None,\n        groups: int = 1,\n        base_width: int = 64,\n        dilation: int = 1,\n        norm_layer: Optional[Callable[..., nn.Module]] = None\n    ) -> None:\n        super(BasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n        if dilation > 1:\n            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x: Tensor) -> Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n    # This variant is also known as ResNet V1.5 and improves accuracy according to\n    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n\n    expansion: int = 4\n\n    def __init__(\n        self,\n        inplanes: int,\n        planes: int,\n        stride: int = 1,\n        downsample: Optional[nn.Module] = None,\n        groups: int = 1,\n        base_width: int = 64,\n        dilation: int = 1,\n        norm_layer: Optional[Callable[..., nn.Module]] = None\n    ) -> None:\n        super(Bottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x: Tensor) -> Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(\n        self,\n        block: Type[Union[BasicBlock, Bottleneck]],\n        layers: List[int],\n        num_classes: int = 1000,\n        zero_init_residual: bool = False,\n        groups: int = 1,\n        width_per_group: int = 64,\n        replace_stride_with_dilation: Optional[List[bool]] = None,\n        norm_layer: Optional[Callable[..., nn.Module]] = None\n    ) -> None:\n        super(ResNet, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self._norm_layer = norm_layer\n\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            # each element in the tuple indicates if we should replace\n            # the 2x2 stride with a dilated convolution instead\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(\"replace_stride_with_dilation should be None \"\n                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n                                       dilate=replace_stride_with_dilation[0])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                       dilate=replace_stride_with_dilation[1])\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                       dilate=replace_stride_with_dilation[2])\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n\n    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int,\n                    stride: int = 1, dilate: bool = False) -> nn.Sequential:\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n                            self.base_width, previous_dilation, norm_layer))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=self.groups,\n                                base_width=self.base_width, dilation=self.dilation,\n                                norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def _forward_impl(self, x: Tensor) -> Tensor:\n        # See note [TorchScript super()]\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n\n    def forward(self, x: Tensor) -> Tensor:\n        return self._forward_impl(x)\n\n\ndef _resnet(\n    arch: str,\n    block: Type[Union[BasicBlock, Bottleneck]],\n    layers: List[int],\n    pretrained: bool,\n    progress: bool,\n    **kwargs: Any\n) -> ResNet:\n    model = ResNet(block, layers, **kwargs)\n    if pretrained:\n        state_dict = load_state_dict_from_url(model_urls[arch],\n                                              progress=progress)\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef resnet18(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n    r\"\"\"ResNet-18 model from\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"\n    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n                   **kwargs)\n\n\ndef resnet34(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n    r\"\"\"ResNet-34 model from\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"\n    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress,\n                   **kwargs)\n\n\ndef resnet50(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n    r\"\"\"ResNet-50 model from\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"\n    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress,\n                   **kwargs)\n\n\ndef resnet101(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n    r\"\"\"ResNet-101 model from\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"\n    return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, progress,\n                   **kwargs)\n\n\ndef resnet152(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n    r\"\"\"ResNet-152 model from\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"\n    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress,\n                   **kwargs)\n\n\ndef resnext50_32x4d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n    r\"\"\"ResNeXt-50 32x4d model from\n    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"\n    kwargs['groups'] = 32\n    kwargs['width_per_group'] = 4\n    return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3],\n                   pretrained, progress, **kwargs)\n\n\ndef resnext101_32x8d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n    r\"\"\"ResNeXt-101 32x8d model from\n    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"\n    kwargs['groups'] = 32\n    kwargs['width_per_group'] = 8\n    return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3],\n                   pretrained, progress, **kwargs)\n\n\ndef wide_resnet50_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n    r\"\"\"Wide ResNet-50-2 model from\n    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_.\n\n    The model is the same as ResNet except for the bottleneck number of channels\n    which is twice larger in every block. The number of channels in outer 1x1\n    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"\n    kwargs['width_per_group'] = 64 * 2\n    return _resnet('wide_resnet50_2', Bottleneck, [3, 4, 6, 3],\n                   pretrained, progress, **kwargs)\n\n\ndef wide_resnet101_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n    r\"\"\"Wide ResNet-101-2 model from\n    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_.\n\n    The model is the same as ResNet except for the bottleneck number of channels\n    which is twice larger in every block. The number of channels in outer 1x1\n    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"\n    kwargs['width_per_group'] = 64 * 2\n    return _resnet('wide_resnet101_2', Bottleneck, [3, 4, 23, 3],\n                   pretrained, progress, **kwargs)\n", 389], "C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\utils.py": ["import collections\nfrom itertools import repeat\nfrom typing import List, Dict, Any\n\n__all__ = ['consume_prefix_in_state_dict_if_present']\n\n\ndef _ntuple(n, name=\"parse\"):\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    parse.__name__ = name\n    return parse\n\n\n_single = _ntuple(1, \"_single\")\n_pair = _ntuple(2, \"_pair\")\n_triple = _ntuple(3, \"_triple\")\n_quadruple = _ntuple(4, \"_quadruple\")\n\n\ndef _reverse_repeat_tuple(t, n):\n    r\"\"\"Reverse the order of `t` and repeat each element for `n` times.\n\n    This can be used to translate padding arg used by Conv and Pooling modules\n    to the ones used by `F.pad`.\n    \"\"\"\n    return tuple(x for x in reversed(t) for _ in range(n))\n\n\ndef _list_with_default(out_size: List[int], defaults: List[int]) -> List[int]:\n    import torch\n    if isinstance(out_size, (int, torch.SymInt)):\n        return out_size\n    if len(defaults) <= len(out_size):\n        raise ValueError(\n            f\"Input dimension should be at least {len(out_size) + 1}\"\n        )\n    return [\n        v if v is not None else d for v, d in zip(out_size, defaults[-len(out_size) :])\n    ]\n\n\ndef consume_prefix_in_state_dict_if_present(\n    state_dict: Dict[str, Any], prefix: str\n) -> None:\n    r\"\"\"Strip the prefix in state_dict in place, if any.\n\n    ..note::\n        Given a `state_dict` from a DP/DDP model, a local model can load it by applying\n        `consume_prefix_in_state_dict_if_present(state_dict, \"module.\")` before calling\n        :meth:`torch.nn.Module.load_state_dict`.\n\n    Args:\n        state_dict (OrderedDict): a state-dict to be loaded to the model.\n        prefix (str): prefix.\n    \"\"\"\n    keys = sorted(state_dict.keys())\n    for key in keys:\n        if key.startswith(prefix):\n            newkey = key[len(prefix) :]\n            state_dict[newkey] = state_dict.pop(key)\n\n    # also strip the prefix in metadata if any.\n    if \"_metadata\" in state_dict:\n        metadata = state_dict[\"_metadata\"]\n        for key in list(metadata.keys()):\n            # for the metadata dict, the key can be:\n            # '': for the DDP module, which we want to remove.\n            # 'module': for the actual model.\n            # 'module.xx.xx': for the rest.\n\n            if len(key) == 0:\n                continue\n            newkey = key[len(prefix) :]\n            metadata[newkey] = metadata.pop(key)\n", 78], "C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py": ["import math\nfrom typing import Any\n\nimport torch\nfrom torch import Tensor\nfrom torch.nn.parameter import Parameter, UninitializedParameter\nfrom .. import functional as F\nfrom .. import init\nfrom .module import Module\nfrom .lazy import LazyModuleMixin\n\n\n__all__ = [\n    'Bilinear',\n    'Identity',\n    'LazyLinear',\n    'Linear',\n]\n\n\nclass Identity(Module):\n    r\"\"\"A placeholder identity operator that is argument-insensitive.\n\n    Args:\n        args: any argument (unused)\n        kwargs: any keyword argument (unused)\n\n    Shape:\n        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n        - Output: :math:`(*)`, same shape as the input.\n\n    Examples::\n\n        >>> m = nn.Identity(54, unused_argument1=0.1, unused_argument2=False)\n        >>> input = torch.randn(128, 20)\n        >>> output = m(input)\n        >>> print(output.size())\n        torch.Size([128, 20])\n\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        super().__init__()\n\n    def forward(self, input: Tensor) -> Tensor:\n        return input\n\n\nclass Linear(Module):\n    r\"\"\"Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.\n\n    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n\n    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n\n    Args:\n        in_features: size of each input sample\n        out_features: size of each output sample\n        bias: If set to ``False``, the layer will not learn an additive bias.\n            Default: ``True``\n\n    Shape:\n        - Input: :math:`(*, H_{in})` where :math:`*` means any number of\n          dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\n        - Output: :math:`(*, H_{out})` where all but the last dimension\n          are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n\n    Attributes:\n        weight: the learnable weights of the module of shape\n            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n            :math:`k = \\frac{1}{\\text{in\\_features}}`\n        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n                If :attr:`bias` is ``True``, the values are initialized from\n                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n                :math:`k = \\frac{1}{\\text{in\\_features}}`\n\n    Examples::\n\n        >>> m = nn.Linear(20, 30)\n        >>> input = torch.randn(128, 20)\n        >>> output = m(input)\n        >>> print(output.size())\n        torch.Size([128, 30])\n    \"\"\"\n\n    __constants__ = ['in_features', 'out_features']\n    in_features: int\n    out_features: int\n    weight: Tensor\n\n    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n                 device=None, dtype=None) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n        if bias:\n            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n        # uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\n        # https://github.com/pytorch/pytorch/issues/57109\n        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.linear(input, self.weight, self.bias)\n\n    def extra_repr(self) -> str:\n        return f'in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}'\n\n\n# This class exists solely to avoid triggering an obscure error when scripting\n# an improperly quantized attention layer. See this issue for details:\n# https://github.com/pytorch/pytorch/issues/58969\n# TODO: fail fast on quantization API usage error, then remove this class\n# and replace uses of it with plain Linear\nclass NonDynamicallyQuantizableLinear(Linear):\n    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n                 device=None, dtype=None) -> None:\n        super().__init__(in_features, out_features, bias=bias,\n                         device=device, dtype=dtype)\n\n\nclass Bilinear(Module):\n    r\"\"\"Applies a bilinear transformation to the incoming data: :math:`y = x_1^T A x_2 + b`.\n\n    Args:\n        in1_features: size of each first input sample\n        in2_features: size of each second input sample\n        out_features: size of each output sample\n        bias: If set to False, the layer will not learn an additive bias.\n            Default: ``True``\n\n    Shape:\n        - Input1: :math:`(*, H_{in1})` where :math:`H_{in1}=\\text{in1\\_features}` and\n          :math:`*` means any number of additional dimensions including none. All but the last dimension\n          of the inputs should be the same.\n        - Input2: :math:`(*, H_{in2})` where :math:`H_{in2}=\\text{in2\\_features}`.\n        - Output: :math:`(*, H_{out})` where :math:`H_{out}=\\text{out\\_features}`\n          and all but the last dimension are the same shape as the input.\n\n    Attributes:\n        weight: the learnable weights of the module of shape\n            :math:`(\\text{out\\_features}, \\text{in1\\_features}, \\text{in2\\_features})`.\n            The values are initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n            :math:`k = \\frac{1}{\\text{in1\\_features}}`\n        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n                If :attr:`bias` is ``True``, the values are initialized from\n                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n                :math:`k = \\frac{1}{\\text{in1\\_features}}`\n\n    Examples::\n\n        >>> m = nn.Bilinear(20, 30, 40)\n        >>> input1 = torch.randn(128, 20)\n        >>> input2 = torch.randn(128, 30)\n        >>> output = m(input1, input2)\n        >>> print(output.size())\n        torch.Size([128, 40])\n    \"\"\"\n\n    __constants__ = ['in1_features', 'in2_features', 'out_features']\n    in1_features: int\n    in2_features: int\n    out_features: int\n    weight: Tensor\n\n    def __init__(self, in1_features: int, in2_features: int, out_features: int, bias: bool = True,\n                 device=None, dtype=None) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__()\n        self.in1_features = in1_features\n        self.in2_features = in2_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.empty((out_features, in1_features, in2_features), **factory_kwargs))\n\n        if bias:\n            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        bound = 1 / math.sqrt(self.weight.size(1))\n        init.uniform_(self.weight, -bound, bound)\n        if self.bias is not None:\n            init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, input1: Tensor, input2: Tensor) -> Tensor:\n        return F.bilinear(input1, input2, self.weight, self.bias)\n\n    def extra_repr(self) -> str:\n        return 'in1_features={}, in2_features={}, out_features={}, bias={}'.format(\n            self.in1_features, self.in2_features, self.out_features, self.bias is not None\n        )\n\n\nclass LazyLinear(LazyModuleMixin, Linear):\n    r\"\"\"A :class:`torch.nn.Linear` module where `in_features` is inferred.\n\n    In this module, the `weight` and `bias` are of :class:`torch.nn.UninitializedParameter`\n    class. They will be initialized after the first call to ``forward`` is done and the\n    module will become a regular :class:`torch.nn.Linear` module. The ``in_features`` argument\n    of the :class:`Linear` is inferred from the ``input.shape[-1]``.\n\n    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation\n    on lazy modules and their limitations.\n\n    Args:\n        out_features: size of each output sample\n        bias: If set to ``False``, the layer will not learn an additive bias.\n            Default: ``True``\n\n    Attributes:\n        weight: the learnable weights of the module of shape\n            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n            :math:`k = \\frac{1}{\\text{in\\_features}}`\n        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n                If :attr:`bias` is ``True``, the values are initialized from\n                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n                :math:`k = \\frac{1}{\\text{in\\_features}}`\n\n\n    \"\"\"\n\n    cls_to_become = Linear  # type: ignore[assignment]\n    weight: UninitializedParameter\n    bias: UninitializedParameter  # type: ignore[assignment]\n\n    def __init__(self, out_features: int, bias: bool = True,\n                 device=None, dtype=None) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        # bias is hardcoded to False to avoid creating tensor\n        # that will soon be overwritten.\n        super().__init__(0, 0, False)\n        self.weight = UninitializedParameter(**factory_kwargs)\n        self.out_features = out_features\n        if bias:\n            self.bias = UninitializedParameter(**factory_kwargs)\n\n    def reset_parameters(self) -> None:\n        if not self.has_uninitialized_params() and self.in_features != 0:\n            super().reset_parameters()\n\n    def initialize_parameters(self, input) -> None:  # type: ignore[override]\n        if self.has_uninitialized_params():\n            with torch.no_grad():\n                self.in_features = input.shape[-1]\n                self.weight.materialize((self.out_features, self.in_features))\n                if self.bias is not None:\n                    self.bias.materialize((self.out_features,))\n                self.reset_parameters()\n# TODO: PartialLinear - maybe in sparse?\n", 264]}, "functions": {"__new__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:149)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py", 149], "is_scripting (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\_jit_internal.py:1120)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\_jit_internal.py", 1120], "__init__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\autograd\\grad_mode.py:74)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\autograd\\grad_mode.py", 74], "__init__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\autograd\\grad_mode.py:183)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\autograd\\grad_mode.py", 183], "__enter__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\autograd\\grad_mode.py:79)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\autograd\\grad_mode.py", 79], "__getattr__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py", 1675], "_conv_forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:451)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py", 451], "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py", 459], "_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1513)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py", 1513], "_wrapped_call_impl (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py", 1507], "_check_input_dim (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:418)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py", 418], "__get__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py:36)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\backends\\__init__.py", 36], "batch_norm (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2451)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py", 2451], "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:141)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py", 141], "relu (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1462)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py", 1462], "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:100)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py", 100], "_max_pool2d (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:774)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py", 774], "fn (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\_jit_internal.py:489)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\_jit_internal.py", 489], "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:163)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py", 163], "__iter__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:207)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py", 207], "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py", 215], "forward (C:\\Users\\adam/.cache\\torch\\hub\\pytorch_vision_v0.10.0\\torchvision\\models\\resnet.py:121)": ["C:\\Users\\adam/.cache\\torch\\hub\\pytorch_vision_v0.10.0\\torchvision\\models\\resnet.py", 121], "<listcomp> (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\utils.py:41)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\utils.py", 41], "_list_with_default (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\utils.py:33)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\utils.py", 33], "adaptive_avg_pool2d (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1221)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\functional.py", 1221], "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:1189)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py", 1189], "forward (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:115)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py", 115], "_forward_impl (C:\\Users\\adam/.cache\\torch\\hub\\pytorch_vision_v0.10.0\\torchvision\\models\\resnet.py:230)": ["C:\\Users\\adam/.cache\\torch\\hub\\pytorch_vision_v0.10.0\\torchvision\\models\\resnet.py", 230], "forward (C:\\Users\\adam/.cache\\torch\\hub\\pytorch_vision_v0.10.0\\torchvision\\models\\resnet.py:248)": ["C:\\Users\\adam/.cache\\torch\\hub\\pytorch_vision_v0.10.0\\torchvision\\models\\resnet.py", 248], "__exit__ (C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\autograd\\grad_mode.py:83)": ["C:\\workspace\\projects\\talk\\venv\\Lib\\site-packages\\torch\\autograd\\grad_mode.py", 83]}}}